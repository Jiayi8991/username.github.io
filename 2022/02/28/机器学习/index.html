<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiayi8991.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Introduction 介绍定义：  Arthur Samuel (1959).   Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed. 使计算机无需明确编程就能学习的研究领域。   Tom Mitchell (1998)">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="JiayiSpace">
<meta property="og:description" content="Introduction 介绍定义：  Arthur Samuel (1959).   Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed. 使计算机无需明确编程就能学习的研究领域。   Tom Mitchell (1998)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/4.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/5.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/6.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/7.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/8.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/9.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/10.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/11.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/12.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/13.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/14.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/15.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/16.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/17.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/18.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/19.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/20.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/22.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/27.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/24.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/25.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26.png">
<meta property="og:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/28.png">
<meta property="article:published_time" content="2022-02-28T06:34:10.000Z">
<meta property="article:modified_time" content="2022-03-07T05:27:54.080Z">
<meta property="article:author" content="Jiayi Liang">
<meta property="article:tag" content="ML 机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1.png">

<link rel="canonical" href="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习 | JiayiSpace</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">JiayiSpace</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayi8991.github.io/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiayi Liang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiayiSpace">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-28 14:34:10" itemprop="dateCreated datePublished" datetime="2022-02-28T14:34:10+08:00">2022-02-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-07 13:27:54" itemprop="dateModified" datetime="2022-03-07T13:27:54+08:00">2022-03-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Introduction-介绍"><a href="#Introduction-介绍" class="headerlink" title="Introduction 介绍"></a>Introduction 介绍</h1><p><strong>定义：</strong></p>
<ul>
<li><p>Arthur Samuel (1959). </p>
<ul>
<li>Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed. 使计算机无需明确编程就能学习的研究领域。</li>
</ul>
</li>
<li><p>Tom Mitchell (1998) </p>
<ul>
<li>Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. </li>
<li>一个计算机程序能够单独的从一些任务T和表现策略P中进行学习，看是否他在T或者是P中的表现随着经验E所提升</li>
</ul>
</li>
</ul>
<p><strong>机器学习算法：</strong></p>
<ul>
<li><p>Supervised learning       有监督学习</p>
</li>
<li><p>Unsupervised learning   无监督学习</p>
<p>其他的一些算法： Reinforcement learning, recommender systems</p>
</li>
</ul>
<h2 id="Supervised-learning-有监督学习"><a href="#Supervised-learning-有监督学习" class="headerlink" title="Supervised learning 有监督学习"></a>Supervised learning 有监督学习</h2><p><strong>定义：</strong></p>
<blockquote>
<p>In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.</p>
<p>译文：在有监督学习中，我们得到一个数据集，并且已经知道我们正确的输出应该是什么样子，因为我们知道输入和输出之间是有关系的。</p>
</blockquote>
<p>Supervised learning problems are categorized into “regression” and “classification” problems. </p>
<ul>
<li><p><strong>回归问题：</strong></p>
</li>
<li><p>In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function.</p>
<p>在一个回归问题中，我们试图预测连续输出的结果，这意味着我们试图将输入变量映射到某个连续函数。</p>
</li>
<li><p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1.png">  </p>
</li>
<li><p><strong>分类问题：</strong></p>
</li>
<li><p>In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. </p>
<p>在分类问题中，我们试图预测离散输出的结果。换句话说，我们试着将输入变量映射成离散的类别。</p>
</li>
<li><p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2.png">  </p>
</li>
<li><p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.png"> </p>
</li>
<li><p>通过很多的特征来学习寻找不同的分类</p>
</li>
</ul>
<h2 id="Unsupervised-learning-无监督学习"><a href="#Unsupervised-learning-无监督学习" class="headerlink" title="Unsupervised learning 无监督学习"></a>Unsupervised learning 无监督学习</h2><p><strong>定义：</strong></p>
<blockquote>
<p>Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables.</p>
<p>无监督学习是指我们在处理问题时很少或根本不知道我们的结果应该是什么样子。我们可以从不需要知道变量影响的数据中推导出结构。</p>
</blockquote>
<ul>
<li><p>We can derive this structure by clustering the data based on relationships among the variables in the data.</p>
<p>我们可以根据数据中变量之间的关系对数据进行聚类，从而得出这种结构。</p>
</li>
<li><p>With unsupervised learning there is no feedback based on the prediction results.</p>
<p>译文：在无监督学习中，没有基于预测结果的反馈。</p>
</li>
</ul>
<p><strong>EXAMPLE：</strong></p>
<ul>
<li><p><strong>Clustering</strong>: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.</p>
<p>聚类: 将100万个不同的基因集合起来，然后找到一种方法，自动将这些基因分组，这些分组在某种程度上与不同的变量(如寿命、位置、角色等)相似或相关。</p>
</li>
<li><p><strong>Non-clustering</strong>: The “Cocktail Party Algorithm”, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cocktail_party_effect">cocktail party</a>).</p>
</li>
</ul>
<h1 id="Model-and-Cost-Function-模型和代价函数"><a href="#Model-and-Cost-Function-模型和代价函数" class="headerlink" title="Model and Cost Function 模型和代价函数"></a>Model and Cost Function 模型和代价函数</h1><h2 id="回归模型展示"><a href="#回归模型展示" class="headerlink" title="回归模型展示"></a>回归模型展示</h2><p>To establish notation for future use, we’ll use x^(i) to denote the “input” variables (living area in this example), also called input features, and y^(i)to denote the “output” or target variable that we are trying to predict (price). </p>
<p>A pair (x^(i) , y^(i) ) is called a training example, and the dataset that we’ll be using to learn—<strong>a list of m training examples {(x^(i) , y^(i)}; i = 1, . . . , m};<em>i</em>=1,…,<em>m</em></strong>—is called a <strong>training set</strong>(<strong>训练集)</strong>. </p>
<p>Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. We will also use X to denote the space of input values, and Y to denote the space of output values. In this example, X = Y = ℝ. </p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/4.png">  <img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/5.png" style="zoom: 33%;">   </p>
<p>输出结果是连续的，回归问题；</p>
<p>输出结果是少的并且离散的，通过特征得到的，分类问题；</p>
<p><strong>When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem.</strong> </p>
<p><strong>When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem.</strong></p>
<h2 id="Cost-Function-代价函数"><a href="#Cost-Function-代价函数" class="headerlink" title="Cost Function 代价函数"></a>Cost Function 代价函数</h2><p><strong>定义：</strong></p>
<blockquote>
<p>We can measure the accuracy of our hypothesis function by using a <strong>cost function</strong>. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x’s and the actual output y’s.</p>
<p>它取所有假设结果的平均差值(实际上是平均值的一个奇特版本)，输入是x，实际输出是y。</p>
</blockquote>
<p>EX：</p>
<p>线性回归常用的代价函数：</p>
<p>This function is otherwise called the “Squared error function”, or “Mean squared error”. </p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/6.png">  </p>
<p>简化版理解，将常数看做0</p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/7.png">   </p>
<p>变量都不为零的情况下：</p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/8.png"></p>
<p>用螺旋线来表示这个。同一个圈圈上的线表示是一样的J（）的值；</p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/9.png"> </p>
<h1 id="parameter-learning-参数学习"><a href="#parameter-learning-参数学习" class="headerlink" title="parameter learning 参数学习"></a>parameter learning 参数学习</h1><h2 id="Gradient-Descent-梯度下降"><a href="#Gradient-Descent-梯度下降" class="headerlink" title="Gradient Descent  梯度下降"></a>Gradient Descent  梯度下降</h2><ul>
<li><strong>梯度是什么？</strong><ul>
<li><strong>在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是(∂f/∂x, ∂f/∂y)T,简称grad f(x,y)或者▽f(x,y)。对于在点(x0,y0)的具体梯度向量就是(∂f/∂x0, ∂f/∂y0)T.或者▽f(x0,y0)，如果是3个参数的向量梯度，就是(∂f/∂x, ∂f/∂y，∂f/∂z)T,以此类推。</strong></li>
</ul>
</li>
<li><strong>那么这个梯度向量求出来有什么意义呢？</strong><ul>
<li><strong>他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点(x0,y0)，沿着梯度向量的方向就是(∂f/∂x0, ∂f/∂y0)T的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 -(∂f/∂x0, ∂f/∂y0)T的方向，梯度减少最快，也就是更加容易找到函数的最小值。</strong></li>
</ul>
</li>
</ul>
<p>Imagine that we graph our hypothesis function based on its fields <em>θ</em>0 and <em>θ</em>1 (actually we are graphing the cost function as a function of the parameter estimates). We are not graphing x and y itself, but the parameter range of our hypothesis function and the cost resulting from selecting a particular set of parameters.</p>
<p>We put θ0 on the x axis and <em>θ</em>1 on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.</p>
<p>假设我们根据其场θ0和θ1绘制假设函数的图(实际上，我们将代价函数绘制为参数估计的函数)。我们画的不是x和y本身，而是假设函数的参数范围以及选择一组特定参数所产生的代价。</p>
<p>θ*0在x轴上，θ1在y轴上，代价函数在垂直的z轴上。图上的点将是代价函数的结果使用我们的假设和特定的参数。下图描述了这样的设置。</p>
<img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/10.png" style="zoom:67%;">  

<p>The way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. <strong>The size of each step is determined by the parameter α, which is called the learning rate.</strong> </p>
<p>我们的方法是对代价函数求导(一个函数的切线)切线的斜率就是这一点的导数它会给我们一个移动的方向。我们让代价函数沿着下降速度最快的方向逐步下降。<strong>每一步的大小由参数α决定，该参数称为学习率。</strong>（学习率就像下山的步子，lr越大，步子越大，否则反之）</p>
<p>For example, the distance between each ‘star’ in the graph above represents a step determined by our parameter α. A smaller α would result in a smaller step and a larger α results in a larger step. The direction in which the step is taken is determined by the partial derivative of J*(<em>θ</em>0,*θ1). Depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places. </p>
<p>例如，上图中每个“星”之间的距离代表了由参数α决定的一个步长。更小的α会导致更小的步骤，更大的α会导致更大的步骤。步进的方向由J(θ0，θ1) J(θ0，θ1)的偏导数决定。这取决于图的起始点，可能会在不同的点结束。上面的图像显示了两个不同的起点，在两个不同的地方结束。</p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/11.png">  </p>
<p>At each iteration j, one should simultaneously update the parametersθ<em>1,<em>θ</em>2,…,<em>θ</em>n</em>. Updating a specific parameter prior to calculating another one on the j^(th) iteration would yield to a wrong implementation. </p>
<p>在代价函数的每次迭代中，每个参数应该同步更新。</p>
<img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/12.png" style="zoom:67%;">  



<p><strong>简化版（只有一个参数改变）梯度下降算法，示例：</strong></p>
<p> <img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/13.png"> </p>
<p>不管斜率是怎样的，theta总是会趋近于最小值的地方；</p>
<p>On a side note, we should adjust our parameter \alpha<em>α</em> to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.</p>
<p>另一方面，我们应该调整我们的参数alphaα（学习速率），以确保梯度下降算法在合理的时间收敛。不能收敛或花太多时间来获得最小值意味着我们的步长是错误的。</p>
<img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/14.png" style="zoom:80%;">   

<p>当斜率降到0的时候，就到达了局部最小处；</p>
<p>并且斜率会自己变小，所以学习速率是固定的，梯度下降会随着斜率下降步子越来越小。</p>
<img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/15.png" style="zoom:80%;">  

<p><strong>梯度下降算法在线性回归中的示例：</strong></p>
<p>When specifically applied to the case of linear regression, a new form of the gradient descent equation can be derived. We can substitute our actual cost function and our actual hypothesis function and modify the equation to :</p>
<p>当具体应用于线性回归的情况下，可以导出一个新的形式的梯度下降方程。我们可以代入实际的代价函数和实际的假设函数并将方程修改为</p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/16.png"> </p>
<p>The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate.</p>
<p>So, this is simply gradient descent on the original cost function J. <strong>This method looks at every example in the entire training set on every step</strong>, and is called <strong>batch gradient descent</strong>. </p>
<p>Note that, while gradient descent can be susceptible to local minims in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. Indeed, J is a convex quadratic function. Here is an example of gradient descent as it is run to minimize a quadratic function.</p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/17.png">   </p>
<h1 id="Multivariate-Linear-Regression-多元线性回归"><a href="#Multivariate-Linear-Regression-多元线性回归" class="headerlink" title="Multivariate Linear Regression 多元线性回归"></a>Multivariate Linear Regression 多元线性回归</h1><h2 id="Multiple-Features-多个特征"><a href="#Multiple-Features-多个特征" class="headerlink" title="Multiple Features 多个特征"></a>Multiple Features 多个特征</h2><blockquote>
<p>Linear regression with multiple variables is also known as “multivariate linear regression”.</p>
</blockquote>
<p>We now introduce notation for equations where we can have any number of input variables.</p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/18.png">  </p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/19.png"> </p>
<p>在房价的例子中，x1到x4都是影响y的特征</p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/20.png">  </p>
<p>可以用向量的内积来简化运算</p>
<h2 id="Gradient-Descent-For-Multiple-Variables-多元的梯度下降算法"><a href="#Gradient-Descent-For-Multiple-Variables-多元的梯度下降算法" class="headerlink" title="Gradient Descent For Multiple Variables 多元的梯度下降算法"></a>Gradient Descent For Multiple Variables 多元的梯度下降算法</h2><p>The gradient descent equation itself is generally the same form; we just have to repeat it for our ‘n’ features:</p>
<p>梯度下降方程本身通常是相同的形式;我们只需要为我们的“n”特性重复它</p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21.png">  </p>
<p>The following image compares gradient descent with one variable to gradient descent with multiple variables: </p>
<p>下图比较了单变量梯度下降和多变量梯度下降</p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/22.png"> </p>
<h2 id="Gradient-Descent-in-Practice-I-Feature-Scaling-特征缩放"><a href="#Gradient-Descent-in-Practice-I-Feature-Scaling-特征缩放" class="headerlink" title="Gradient Descent in Practice I - Feature Scaling 特征缩放"></a>Gradient Descent in Practice I - Feature Scaling 特征缩放</h2><ul>
<li><strong>特征缩放是什么？</strong><ul>
<li>特征缩放是用来标准化数据特征的范围。</li>
<li>将各个特征的范围缩小到相近可以加速收敛的过程，进而加速学习的速度</li>
</ul>
</li>
<li><strong>为什么要特征缩放？</strong><ul>
<li>We can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</li>
<li>我们可以通过让每个输入值在大致相同的范围内来加速梯度下降。这是因为θ在小范围会快速下降，而在大范围会缓慢下降，因此当变量非常不均匀时，θ会低效地振荡到最优。</li>
</ul>
</li>
</ul>
<p>​        <strong>因为特征的值范围特别大的话，theta的值也会变得偏大或者偏小，就会让螺旋图像下面图一的样子，又扁又长，就会经过更多次迭代才会到达代价最小的点</strong> <img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/27.png">  </p>
<p>一般情况下会将特征的范围缩至如下的情况：</p>
<p>The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:</p>
<p>​            −1 ≤ x*(<em>i) ≤ 1   or     −0.5 ≤x</em>(*i) ≤ 0.5</p>
<p><strong>These aren’t exact requirements</strong>; we are only trying to speed things up. <strong>The goal is to get all input variables into roughly one of these ranges, give or take a few.</strong></p>
<h3 id="mean-normalization-均值标准化"><a href="#mean-normalization-均值标准化" class="headerlink" title="mean normalization 均值标准化"></a>mean normalization 均值标准化</h3><p>Mean normalization involves <strong>subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.</strong> To implement both of these techniques, adjust your input values as shown in this formula:</p>
<p>均值标准化涉及到从一个输入变量的值中减去一个输入变量的平均值，从而得到一个新的输入变量的平均值为零。要实现这两种技术，请按照以下公式调整输入值</p>
<p><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23.png">   </p>
<h2 id="Gradient-Descent-in-Practice-II-Learning-Rate-学习速率"><a href="#Gradient-Descent-in-Practice-II-Learning-Rate-学习速率" class="headerlink" title="Gradient Descent in Practice II - Learning Rate 学习速率"></a>Gradient Descent in Practice II - Learning Rate 学习速率</h2><ul>
<li><p>如何判断梯度下降算法是否正常工作：</p>
<ul>
<li><strong>Debugging gradient descent.</strong> Make a plot with <em>number of iterations</em> on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.</li>
<li><strong>Automatic convergence test.</strong> Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10^{−3}10−3. However in practice it’s difficult to choose this threshold value.</li>
</ul>
</li>
<li><p>学习速率设置对于学习算法的影响：</p>
<ul>
<li><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/24.png"> <ul>
<li>It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration.</li>
</ul>
</li>
<li><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/25.png"> </li>
<li>SUMMARIZE：<ul>
<li><strong>If <em>α</em> is too small: slow convergence.</strong> </li>
<li><strong>If  <em>α</em> is too large: may not decrease on every iteration and thus may not converge.</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Features-and-Polynomial-Regression-特征和多项式回归"><a href="#Features-and-Polynomial-Regression-特征和多项式回归" class="headerlink" title="Features and Polynomial Regression 特征和多项式回归"></a>Features and Polynomial Regression 特征和多项式回归</h2><p>We can improve our features and the form of our hypothesis function in a couple different ways.</p>
<p><strong>我们可以通过改进特征 和 猜想的方程来优化模型，获得更好的结果。</strong></p>
<ul>
<li>We can <strong>combine</strong> multiple features into one. <ul>
<li>For example, we can combine x_1<em>x</em>1 and x_2<em>x</em>2 into a new feature x_3<em>x</em>3 by taking x_1<em>x</em>1⋅x_2<em>x</em>2.</li>
</ul>
</li>
</ul>
<h3 id="Polynomial-Regression-多项式回归"><a href="#Polynomial-Regression-多项式回归" class="headerlink" title="Polynomial Regression 多项式回归"></a><strong>Polynomial Regression</strong> 多项式回归</h3><p>Our hypothesis function need not be linear (a straight line) if that does not fit the data well.</p>
<p>We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</p>
<p>我们可以通过将假设函数变成二次函数、三次函数或平方根函数(或任何其他形式)来改变它的行为或曲线。</p>
<ul>
<li><img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26.png"> </li>
</ul>
<h1 id="Computing-Parameters-Analytically-计算参数分析"><a href="#Computing-Parameters-Analytically-计算参数分析" class="headerlink" title="Computing Parameters Analytically 计算参数分析"></a>Computing Parameters Analytically 计算参数分析</h1><h2 id="Normal-Equation-正规方程"><a href="#Normal-Equation-正规方程" class="headerlink" title="Normal Equation 正规方程"></a>Normal Equation 正规方程</h2><blockquote>
<p>正规方程法和梯度下降算法一样，都是求最小化的一种方法；</p>
<p>不同的地方在于，正规方程法直接通过矩阵直接将最小化的参数分析计算出来，而不用通过迭代的方法去趋近于最小值；并且不用归一化</p>
</blockquote>
<p>This allows us to find the optimum theta without iteration. The normal equation formula is given below: </p>
<p>这使得我们可以在不需要迭代的情况下找到最优的。常规方程公式如下</p>
<blockquote>
<p><em>θ</em>    = ( <em>X<sup>T</sup>X )<sup>−1</sup></em> * X<sup>T</sup> * y</p>
</blockquote>
<p> <img src="/2022/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/28.png"> </p>
<blockquote>
<p>There is <strong>no need</strong> to do feature scaling with the normal equation.</p>
</blockquote>
<p>The following is a comparison of gradient descent and the normal equation:</p>
<table>
<thead>
<tr>
<th align="left">Gradient Descent</th>
<th align="left">Normal Equation</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Need to choose alpha</td>
<td align="left">No need to choose alpha</td>
</tr>
<tr>
<td align="left">Needs many iterations</td>
<td align="left">No need to iterate</td>
</tr>
<tr>
<td align="left">O (kn^2)</td>
<td align="left">O (n^3), need to calculate inverse of X^TX</td>
</tr>
<tr>
<td align="left">Works well when n is large</td>
<td align="left">Slow if n is very large</td>
</tr>
</tbody></table>
<p>当特征变量不那么多的时候可以选择正规方程，因为可以更加快速的找到最优解，但是当特征变量很大的时候，比如超过了10000的时候，就可以考虑使用梯度下降，通过迭代的方法来找最优解。</p>
<h2 id="Normal-Equation-Noninvertibility-不可逆的情况"><a href="#Normal-Equation-Noninvertibility-不可逆的情况" class="headerlink" title="Normal Equation Noninvertibility  不可逆的情况"></a>Normal Equation Noninvertibility  不可逆的情况</h2><p>When implementing the normal equation in octave <strong>we want to use the ‘pinv’ function rather than ‘inv.</strong></p>
<p>‘ The ‘pinv’ function will give you a value of θ even if X<sup>T</sup>X is not invertible. </p>
<p>If X<sup>T</sup>X is <strong>noninvertible,</strong> the common causes might be having :</p>
<ul>
<li>Redundant features, where two features are very closely related (i.e. they are linearly dependent)</li>
<li>Too many features (e.g. m ≤ n). In this case, delete some features or use “regularization” (to be explained in a later lesson).</li>
</ul>
<p>Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ML-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># ML 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/01/22/%E8%84%89%E5%86%B2%E6%98%9F%E8%A1%A5%E4%B9%A0/" rel="prev" title="脉冲星补习">
      <i class="fa fa-chevron-left"></i> 脉冲星补习
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/02/28/docker%E5%AD%A6%E4%B9%A0/" rel="next" title="Docker学习">
      Docker学习 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">Introduction 介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Supervised-learning-%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.1.</span> <span class="nav-text">Supervised learning 有监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Unsupervised-learning-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.</span> <span class="nav-text">Unsupervised learning 无监督学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Model-and-Cost-Function-%E6%A8%A1%E5%9E%8B%E5%92%8C%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">2.</span> <span class="nav-text">Model and Cost Function 模型和代价函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E5%B1%95%E7%A4%BA"><span class="nav-number">2.1.</span> <span class="nav-text">回归模型展示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cost-Function-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.</span> <span class="nav-text">Cost Function 代价函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#parameter-learning-%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.</span> <span class="nav-text">parameter learning 参数学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Descent-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">3.1.</span> <span class="nav-text">Gradient Descent  梯度下降</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multivariate-Linear-Regression-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">4.</span> <span class="nav-text">Multivariate Linear Regression 多元线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Multiple-Features-%E5%A4%9A%E4%B8%AA%E7%89%B9%E5%BE%81"><span class="nav-number">4.1.</span> <span class="nav-text">Multiple Features 多个特征</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Descent-For-Multiple-Variables-%E5%A4%9A%E5%85%83%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="nav-number">4.2.</span> <span class="nav-text">Gradient Descent For Multiple Variables 多元的梯度下降算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Descent-in-Practice-I-Feature-Scaling-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="nav-number">4.3.</span> <span class="nav-text">Gradient Descent in Practice I - Feature Scaling 特征缩放</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mean-normalization-%E5%9D%87%E5%80%BC%E6%A0%87%E5%87%86%E5%8C%96"><span class="nav-number">4.3.1.</span> <span class="nav-text">mean normalization 均值标准化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Descent-in-Practice-II-Learning-Rate-%E5%AD%A6%E4%B9%A0%E9%80%9F%E7%8E%87"><span class="nav-number">4.4.</span> <span class="nav-text">Gradient Descent in Practice II - Learning Rate 学习速率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Features-and-Polynomial-Regression-%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="nav-number">4.5.</span> <span class="nav-text">Features and Polynomial Regression 特征和多项式回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Polynomial-Regression-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="nav-number">4.5.1.</span> <span class="nav-text">Polynomial Regression 多项式回归</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Computing-Parameters-Analytically-%E8%AE%A1%E7%AE%97%E5%8F%82%E6%95%B0%E5%88%86%E6%9E%90"><span class="nav-number">5.</span> <span class="nav-text">Computing Parameters Analytically 计算参数分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Normal-Equation-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="nav-number">5.1.</span> <span class="nav-text">Normal Equation 正规方程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Normal-Equation-Noninvertibility-%E4%B8%8D%E5%8F%AF%E9%80%86%E7%9A%84%E6%83%85%E5%86%B5"><span class="nav-number">5.2.</span> <span class="nav-text">Normal Equation Noninvertibility  不可逆的情况</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jiayi Liang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jiayi Liang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiayi Liang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
