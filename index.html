<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiayi8991.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="JiayiSpace">
<meta property="og:url" content="https://jiayi8991.github.io/index.html">
<meta property="og:site_name" content="JiayiSpace">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Jiayi Liang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://jiayi8991.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>JiayiSpace</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">JiayiSpace</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayi8991.github.io/2022/06/09/OpenStack/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiayi Liang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiayiSpace">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/09/OpenStack/" class="post-title-link" itemprop="url">OpenStack</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-06-09 17:10:22" itemprop="dateCreated datePublished" datetime="2022-06-09T17:10:22+08:00">2022-06-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-06-13 16:10:10" itemprop="dateModified" datetime="2022-06-13T16:10:10+08:00">2022-06-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="OpenStack-云计算平台学习与实践"><a href="#OpenStack-云计算平台学习与实践" class="headerlink" title="OpenStack 云计算平台学习与实践"></a>OpenStack 云计算平台学习与实践</h1><h1 id="云计算-和-OpenStack"><a href="#云计算-和-OpenStack" class="headerlink" title="云计算 和 OpenStack"></a>云计算 和 OpenStack</h1><blockquote>
<p>注：全文摘自CloudMan</p>
</blockquote>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqF6TzeFS35dvTgauaRJ6AYOr7vOIFRNygXlHmjWfiaC9MicXIicZ5d44ibgyp96PPGbpBJ9bc62UMyD0g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>IT系统架构的发展到目前为止大致可以分为3个阶段：</p>
<ol>
<li><p>物理机架构 这一阶段，应用部署和运行在物理机上。 比如企业要上一个ERP系统，如果规模不大，可以找3台物理机，分别部署Web服务器、应用服务器和数据库服务器。 如果规模大一点，各种服务器可以采用集群架构，但每个集群成员也还是直接部署在物理机上。 我见过的客户早期都是这种架构，一套应用一套服务器，通常系统的资源使用率都很低，达到20%的都是好的。</p>
</li>
<li><p>虚拟化架构 摩尔定律决定了物理服务器的计算能力越来越强，虚拟化技术的发展大大提高了物理服务器的资源使用率。 这个阶段，物理机上运行若干虚拟机，应用系统直接部署到虚拟机上。 虚拟化的好处还体现在减少了需要管理的物理机数量，同时节省了维护成本。</p>
</li>
<li><p>云计算架构 虚拟化提高了单台物理机的资源使用率，随着虚拟化技术的应用，IT环境中有越来越多的虚拟机，这时新的需求产生了： 如何对IT环境中的虚拟机进行统一和高效的管理。 有需求就有供给，云计算登上了历史舞台。</p>
</li>
</ol>
<p>计算（CPU/内存）、存储和网络是 IT 系统的三类资源。 通过云计算平台，这三类资源变成了三个池子 当需要虚机的时候，只需要向平台提供虚机的规格。 平台会快速从三个资源池分配相应的资源，部署出这样一个满足规格的虚机。 虚机的使用者不再需要关心虚机运行在哪里，存储空间从哪里来，IP是如何分配，这些云平台都搞定了。</p>
<p>云平台是一个面向服务的架构，按照提供服务的不同分为 IaaS、PaaS 和 SaaS。 请看下图</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqF6TzeFS35dvTgauaRJ6AYOmLnZAQqNfOm6wVVZ8PhDVxK2F0c6G7k91R4JVrT5hDFaCrH8n5OI3Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片">  </p>
<ul>
<li><strong>IaaS</strong>（Infrastructure as a Service）提供的服务是虚拟机。 IaaS 负责管理虚机的生命周期，包括创建、修改、备份、启停、销毁等。 使用者从云平台得到的是一个已经安装好镜像（操作系统+其他预装软件）的虚拟机。 使用者需要关心虚机的类型（OS）和配置（CPU、内存、磁盘），并且自己负责部署上层的中间件和应用。 IaaS 的使用者通常是数据中心的系统管理员。 典型的 IaaS 例子有 AWS、Rackspace、阿里云等。</li>
</ul>
<ul>
<li><strong>PaaS</strong>（Platform as a Service）提供的服务是应用的运行环境和一系列中间件服务（比如数据库、消息队列等）。 使用者只需专注应用的开发，并将自己的应用和数据部署到PaaS环境中。 PaaS负责保证这些服务的可用性和性能。 PaaS的使用者通常是应用的开发人员。 典型的 PaaS 有 Heroku、Google App Engine、IBM BlueMix 等。</li>
</ul>
<ul>
<li><strong>SaaS</strong>（Software as a Service）提供的是应用服务。 使用者只需要登录并使用应用，无需关心应用使用什么技术实现，也不需要关系应用部署在哪里。 SaaS的使用者通常是应用的最终用户。 典型的 SaaS 有 Google Gmail、Salesforce 等。</li>
</ul>
<blockquote>
<p>OpenStack is a cloud operating system that controls large pools of compute, storage, and networking resources throughout a datacenter, all managed through a dashboard that gives administrators control while empowering their users to provision resources through a web interface.</p>
</blockquote>
<h1 id="OpenStack架构"><a href="#OpenStack架构" class="headerlink" title="OpenStack架构"></a>OpenStack架构</h1><p>OpenStack的核心：</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcqgAa9OJl30dqPYe5PbT3Ye6MdB4H0HLABVb2eZqkgLibkxNuxR7z4wgw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片">  </p>
<p>作为 IaaS 层的云操作系统，OpenStack 为虚机提供并管理三大类资源：计算、网络和存储。</p>
<p>这三个就是核心，所以我们的学习重点就是： 搞清楚 OpenStack 是如何对计算、网络和存储资源进行管理的。 在 20+ 模块中，管理这三类资源的核心模块其实不多，这几个模块就是我们的重点了。</p>
<p>要达到这个目的，我们自然需要研究 OpenStack 的整体架构。 架构里哪些核心模块负责管理计算资源、网络资源和存储资源？模块之间如何协调工作？ 同时我们会构建一个实验环境，进到各个模块的内部，通过实际操作真正理解和掌握 OpenStack。</p>
<p>OpenStack的架构：</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcq1snxN7gKt4eV1Licgib1xSfEgQxkHykruS96YuiabakkL4gRwRkWrJYgw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>中间菱形<img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcqe6TUoXLp9hkwWI4zDS7Np17dVv5ofHUK9iacibzAx5CuhVIPntEolJjQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片">是虚拟机，围绕 VM 的那些长方形代表 OpenStack 不同的模块（OpenStack 叫服务，后面都用服务这个术语），下面来分别介绍。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcqOde9JicQFubwIP7qpIuCc3IUONoExsWkOzcviaLIPPwYnkm4ibiaenlSKw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> Nova：管理 VM 的生命周期，是 OpenStack 中最核心的服务。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcqH0wYMryzypMrfAsbPicKYibgtgQW9xvZtD6xniaLicEmbKboYpHyicJTBgQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> Neutron：为 OpenStack 提供网络连接服务，负责创建和管理L2、L3 网络，为 VM 提供虚拟网络和物理网络连接。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcqmbLTpI7E2fnK7Zvdibdt6jR3geZ8099m4cHcVw1gDq9BGCG9U2gIlOA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> Glance：管理 VM 启动镜像，Nova 创建 VM 时将使用 Glance 提供的镜像。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcqRV3kyJ5RosdVianHHP8yQ06elhjJhgnHPmekTOkOjP1wopmUOSdK7Rg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> Cinder：为 VM 提供块存储服务。Cinder 提供的每一个 Volume 在 VM 看来就是一块虚拟硬盘，一般用作数据盘。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcqbia1SvK6ib6gOPpfeNaj6KEiaaw2Q4IpYTNO02RibQ9aYNhdfAQ0xvrM6g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> Swift：提供对象存储服务。VM 可以通过 RESTful API 存放对象数据。作为可选的方案，Glance 可以将镜像存放在 Swift 中；Cinder 也可以将 Volume 备份到 Swift 中。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcqFic1hgb6ibhPR4rqdf1vkjye02icyWnC56yyCGtaONI6UACUIib4LHrvjw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> Keystone：为 OpenStack 的各种服务提供认证和权限管理服务。简单的说，OpenStack 上的每一个操作都必须通过 Keystone 的审核。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcqhHBGEcDMFEraRKSSBJINBIibf1gUy9RAOQfByCz5H3zOI3sKhRbqkWQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> Ceilometer：提供 OpenStac k监控和计量服务，为报警、统计或计费提供数据。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcqsGyPwjQvONfXEWhbsu1x3EqxYvqYicew2YnRqc3CLArjvlcV7uADwUQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> Horizon：为 OpenStack 用户提供一个 Web 的自服务 Portal。</p>
<p>在上面的这些服务中，哪些是 OpenStack 的核心服务呢？ 核心服务就是如果没有它，OpenStack 就跑不起来。 很显然：</p>
<ol>
<li>Nova 管理计算资源，是核心服务。</li>
<li>Neutron 管理网络资源，是核心服务。</li>
<li>Glance 为 VM 提供 OS 镜像，属于存储范畴，是核心服务。</li>
<li>Cinder 提供块存储，VM怎么也得需要数据盘吧，是核心服务。</li>
<li>Swift 提供对象存储，不是必须的，是可选服务。</li>
<li>Keystone 认证服务，没它 OpenStack 转不起来，是核心服务。</li>
<li>Ceilometer 监控服务，不是必须的，可选服务。</li>
<li>Horizon 大家都需要一个操作界面吧。</li>
</ol>
<p>现在核心服务有了，接下来我们将镜头拉近点，看看核心服务内部的组成结构。</p>
<p><strong>Logical Architecture</strong></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcq2Zgu4WuejxFZCJR6zPgspp7ibB81xX7Tic0tfrLpIrcwKLo31KZs8PeA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片">  </p>
<p>在 Logical Architecture 中，可以看到每个服务又由若干组件组成。 以 Neutron 为例，包含</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFy6CxsBRVHeE5iatp5e5vcqA1GBGVtuNMU2yn5QshpDjrtMpCg6bxRS0qUyriaTk5wibul6AIRVibHrw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<ol>
<li>Neutron Server、Neutron plugins 和 Neutron agents</li>
<li>Network provider</li>
<li>消息队列 Queue</li>
<li>数据库 Neutron Database</li>
</ol>
<p>在后面 Neutron 章节会展开学习这些组件。</p>
<p>这里想要强调一点： 上面是 Logical Architecture，描述的是 Neutron 服务各个组成部分以及各组件之间的逻辑关系。 而在实际的部署方案上，各个组件可以部署到不同的物理节点上。</p>
<p>OpenStack 本身是一个分布式系统，不但各个服务可以分布部署，服务中的组件也可以分布部署。 这种分布式特性让 OpenStack 具备极大的灵活性、伸缩性和高可用性。 当然从另一个角度讲，这也使得 OpenStack 比一般系统复杂，学习难度也更大。</p>
<p>![image-20220610142857117](/Users/jiayi/Library/Application Support/typora-user-images/image-20220610142857117.png) </p>
<h1 id="认证Keystone"><a href="#认证Keystone" class="headerlink" title="认证Keystone"></a>认证Keystone</h1><p>作为 OpenStack 的基础支持服务，Keystone 做下面这几件事情：</p>
<ol>
<li>管理用户及其权限</li>
<li>维护 OpenStack Services 的 Endpoint</li>
<li>Authentication（认证）和 Authorization（鉴权）</li>
</ol>
<p>学习 Keystone，得理解下面这些概念：</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWa2KibqTdaff9tvEANsMjp0BCynsgfc70Xwsm27t8UadFCZDzfqVFrBRA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><strong>User</strong></p>
<p>User 指代任何使用 OpenStack 的实体，可以是真正的用户，其他系统或者服务。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWawibNBibEX5aYMXl6FvDhyIqhAQibk4BDd3cYYnWhy2LkTNQc5WQVF1JJQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>当 User 请求访问 OpenStack 时，Keystone 会对其进行验证。Horizon 在 Identity-&gt;Users 管理 User</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWamX925ARyEMyX6RvkBuqsxM7lteia9XY7jPqW5LxCZXF9nzicsicvMFjEw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>除了 admin 和 demo，OpenStack 也为 nova、cinder、glance、neutron 服务创建了相应的 User。 admin 也可以管理这些 User。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWavEdMFN4jPkNTXDK54fAoM4XgI8ZBuItNTjDhuK6Vcr7ibZicZBDE5zOg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><strong>Credentials</strong></p>
<p>Credentials 是 User 用来证明自己身份的信息，可以是：</p>
<p>\1. 用户名/密码<br>\2. Token<br>\3. API Key<br>\4. 其他高级方式</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWaF68soIpKKAxyicqzUU1liacwNeJ4gibtZph0Agw6cibyCdEhJoEunYib2ng/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><strong>Authentication</strong></p>
<p>Authentication 是 Keystone 验证 User 身份的过程。User 访问 OpenStack 时向 Keystone 提交用户名和密码形式的 Credentials，Keystone 验证通过后会给 User 签发一个 Token 作为后续访问的 Credential。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWaNLPLcmEFEBHH7Yvb3hAqBfoeksoQg8Rhc6umRhJZpFsEkP4ExvbgAw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><strong>Token</strong></p>
<p>Token 是由数字和字母组成的字符串，User 成功 Authentication 后 Keystone 生成 Token 并分配给 User。</p>
<ol>
<li><p>Token 用做访问 Service 的 Credential</p>
</li>
<li><p>Service 会通过 Keystone 验证 Token 的有效性</p>
</li>
<li><p>Token 的有效期默认是 24 小时</p>
</li>
</ol>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWaezgaPs2ygAsU0ibNcRHQqGmVMXlhic1QiclHFCNic9WGeibsdh60q1hfbVg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><strong>Project</strong></p>
<p>Project 用于将 OpenStack 的资源（计算、存储和网络）进行分组和隔离。</p>
<p>根据 OpenStack 服务的对象不同，Project 可以是一个客户（公有云，也叫租户）、部门或者项目组（私有云）。</p>
<p>这里请注意：</p>
<ol>
<li>资源的所有权是属于 Project 的，而不是 User。</li>
<li>在 OpenStack 的界面和文档中，Tenant / Project / Account 这几个术语是通用的，但长期看会倾向使用 Project</li>
<li>每个 User（包括 admin）必须挂在 Project 里才能访问该 Project 的资源。 一个User可以属于多个 Project。</li>
<li>admin 相当于 root 用户，具有最高权限</li>
</ol>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWaQVuygKUakA7VnrutH5h0OmdUicZ3uUNVSSAtXQeQCH1yibiaQd0SWz1QQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>Horizon 在 Identity-&gt;Projects 中管理 Project</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWaicNVhtiaHt5Cgg7o2GrrtDVJHQiaGW7iagWCM0OYG1CQ7T17H0XW5YISng/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>通过 Manage Members 将 User 添加到 Project</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWakxB3z8MicuPa8v9vymY5XZDjhZwEPPJsUI3608nMuoGicQjR30qWic3rQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWadKJp3AncCubsUSXTVq5hsKViaErAHeiciaicHjWDyuSfV71ExRnOWia0g1g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><strong>Service</strong></p>
<p>OpenStack 的 Service 包括 Compute (Nova)、Block Storage (Cinder)、Object Storage (Swift)、Image Service (Glance) 、Networking Service (Neutron) 等。每个 Service 都会提供若干个 Endpoint，User 通过 Endpoint 访问资源和执行操作。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWaI7wKjbq1vZCz6VZL9L1PWIHIVHRXHsibK1gMklFg9uibjsJnquWBiaJFg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><strong>Endpoint</strong></p>
<p>Endpoint 是一个网络上可访问的地址，通常是一个 URL。Service 通过 Endpoint 暴露自己的 API。 Keystone 负责管理和维护每个 Service 的 Endpoint。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWaoiabnooGYCurrphFx1H1ibHfVpUTeFY1Jqgh6cxy9xoM1PCz4IWzLAvg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>可以使用下面的命令来查看 Endpoint。</p>
<blockquote>
<p># source devstack/openrc admin admin<br># openstack catalog list</p>
</blockquote>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWa4LOnib0Ev8fXLrGX0vCZJHVrFaLdkib9ibgrvyuKRRbBCsGhiaw4oZIoJQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><strong>Role</strong></p>
<p>安全包含两部分：Authentication（认证）和 Authorization（鉴权）</p>
<p>Authentication 解决的是“你是谁？”的问题<br>Authorization 解决的是“你能干什么？”的问题</p>
<p>Keystone 借助 Role 实现 Authorization：</p>
<ol>
<li><p>Keystone定义Role<img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWa4g1Lo2bT0Qr8icAANReoib7RGstCFT2CBtDeKOIoWRZ4gUO5vOum2w2w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
</li>
<li><p>可以为 User 分配一个或多个 Role，Horizon 的菜单为 Identity-&gt;Project-&gt;Manage Members<img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWaLZFW0PTibkq4N67D3FzvgQZmZTV4UAA19NQejTGvVnHlddpBul6zomw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
</li>
<li><p>Service 决定每个 Role 能做什么事情 Service 通过各自的 policy.json 文件对 Role 进行访问控制。 下面是 Nova 服务 /etc/nova/policy.json 中的示例<img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEd6YBwKj2W1WwdwqkscXWaCA7OcpYVDYSurGCPRtibLrq3xjKmSuicibeQicyDFv1svQp4Ycmmwcx2lQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
</li>
</ol>
<p>上面配置的含义是：对于 create、attach_network 和 attach_volume 操作，任何Role的 User 都可以执行； 但只有 admin 这个 Role 的 User 才能执行 forced_host 操作。</p>
<p>以下是Keystone运作的例子：</p>
<p><strong>第 1 步 登录</strong></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHWhcQFiaKFEmUJcBtQ58fSxleSFk8sxVTUxuibcQyicibaSCTvbjlb0WxiaMBIR4icdhyic5mJ97oc8Ld6Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>当点击<img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHWhcQFiaKFEmUJcBtQ58fSxwjzH7jyFGEYpbaGt3defG7DW9HnuHjhXRUtXw6208wflf6G6S541Vg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片">时，OpenStack 内部发生了哪些事情？请看下面</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHWhcQFiaKFEmUJcBtQ58fSxbibiaxgJU3lPxbllrLobtc0O1unT1zau0hYJEFqkqpI3r4HU13ibP2x4A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>Token 中包含了 User 的 Role 信息</p>
<p><strong>第 2 步 显示操作界面</strong></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHWhcQFiaKFEmUJcBtQ58fSxSibOG7PwlWGToOrxulMXx6XembGB2uKMlQCYOI3oEx88dakWAibBWmVQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>请注意，顶部显示 admin 可访问的  Project 为 “admin” 和 “demo”。 其实在此之前发生了一些事情：</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHWhcQFiaKFEmUJcBtQ58fSxbm2acjkegnZOcZk1r2e27Y2FyBiauRnNz0icV1TnsughYEflBVIBlKOw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>同时，admin 可以访问 Intance, Volume, Image 等服务</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHWhcQFiaKFEmUJcBtQ58fSxdLKxHd2bmhsAl23Yv9T6icoT9qRmRom8kOPUrsw669CU49E2N0FgG4A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>这是因为 admin 已经从 Keystone 拿到了各 Service 的 Endpoints</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHWhcQFiaKFEmUJcBtQ58fSxNjSVFicMrzicW9vT7hG6WZ81ohZxNFrS3OqNDwbVtP4Nd5CvRhTHzS8A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><strong>第 3 步 显示 image 列表</strong></p>
<p>点击 “Images”，会显示 image 列表</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHWhcQFiaKFEmUJcBtQ58fSx9fNGZKjvHqCichIXPN0B1hTZd4rmPsCqPTGNQ0ueOmcA1ic35669Wsrw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>背后发生了这些事：</p>
<p>首先 admin 将请求发送到 Glance 的 Endpoint</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHWhcQFiaKFEmUJcBtQ58fSxAtRkENfMOcEccZuKVEM9x5zZ8Zu9jvWeGJcBicPVNSQTOBmRFRHS5Ng/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>Glance 向 Keystone 询问 admin 身份是否有效</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHWhcQFiaKFEmUJcBtQ58fSxnVgRLtf0KjqeWQ1BeOR00kddTNgHJTYPFrbAqyxmiapHFotz4qr5Fdg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>接下来 Glance 会查看 /etc/glance/policy.json 判断 admin 是否有查看 image 的权限</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHWhcQFiaKFEmUJcBtQ58fSxDwkYzcnY04u4XQ7XPvAbXxc2sHjpjsEOlPS0oSic7MIXwdCmQjoIsLA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>权限判定通过，Glance 将 image 列表发给 admin。</p>
<h1 id="镜像Glance"><a href="#镜像Glance" class="headerlink" title="镜像Glance"></a>镜像Glance</h1><p>OpenStack 由 Glance 提供 Image 服务。</p>
<blockquote>
<p> <strong>理解 Image</strong></p>
</blockquote>
<p>要理解 Image Service，先得搞清楚什么是 Image 以及为什么要用 Image？</p>
<p>在传统 IT 环境下，安装一个系统要么从安装 CD 从头安装，要么用 Ghost 等克隆工具恢复。这两种方式有如下几个问题：</p>
<ol>
<li>如果要安装的系统多了效率就很低</li>
<li>时间长，工作量大</li>
<li>安装完还要进行手工配置，比如安装其他的软件，设置 IP 等</li>
<li>备份和恢复系统不灵活</li>
</ol>
<p>云环境下需要更高效的方案，这就是 Image。 Image 是一个模板，里面包含了基本的操作系统和其他的软件。</p>
<p>举例来说，有家公司需要为每位员工配置一套办公用的系统，一般需要一个 Win7 系统再加 MS office 软件。 OpenStack 是这么玩的：</p>
<ol>
<li><p>先手工安装好这么一个虚机</p>
</li>
<li><p>然后对虚机执行 snapshot，这样就得到了一个 image</p>
</li>
<li><p>当有新员工入职需要办公环境时，立马启动一个或多个该 image 的 instance（虚机）就可以了</p>
</li>
</ol>
<p>在这个过程中，第 1 步跟传统方式类似，需要手工操作和一定时间，但第 2、3 步非常快，全自动化，一般都是秒级别。而且 2、3 步可以循环做。 比如公司新上了一套 OA 系统，每个员工的 PC 上都得有客户端软件。 那么可以在某个现有虚机中先手工安装好 OA 客户端，然后执行 snapshot 操作，得到新的 image，以后可以就直接使用新 image 创建虚机了。另外，snapshot 还有备份的作用，能够非常方便的恢复系统。</p>
<blockquote>
<p><strong>理解Image Service</strong></p>
</blockquote>
<p>Image Service 的功能是管理 Image，让用户能够发现、获取和保存 Image。在 OpenStack 中，提供 Image Service 的是 Glance，其具体功能如下：</p>
<ol>
<li>提供 REST API 让用户能够查询和获取 image 的元数据和 image 本身</li>
<li>支持多种方式存储 image，包括普通的文件系统、Swift、Amazon S3 等</li>
<li>对 Instance 执行 Snapshot 创建新的 image</li>
</ol>
<h3 id="Glance架构"><a href="#Glance架构" class="headerlink" title="Glance架构"></a><strong>Glance架构</strong></h3><p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGf6K2mibNIzz9NGgW1sAfPNTNNp6sY0DicaymCAuHMsAjvUrxuB9DdYiaFAPuVIHKYDxibUrXabCzUjQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片">  </p>
<p>上面是 Glance 的架构图</p>
<h4 id="glance-api"><a href="#glance-api" class="headerlink" title="glance-api"></a><strong>glance-api</strong></h4><p>glance-api 是系统后台运行的服务进程。 对外提供 REST API，响应 image 查询、获取和存储的调用。</p>
<p>glance-api 不会真正处理请求。 如果操作是与 image metadata（元数据）相关，glance-api 会把请求转发给 glance-registry； 如果操作是与 image 自身存取相关，glance-api 会把请求转发给该 image 的 store backend。</p>
<p>在控制节点上可以查看 glance-api 进程</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGf6K2mibNIzz9NGgW1sAfPNytPPuKCkITlL30ac6LlgFGJMoqRCOEfdBCqbB5ffj6sFwWRibXXc4Rg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<h4 id="glance-registry"><a href="#glance-registry" class="headerlink" title="glance-registry"></a><strong>glance-registry</strong></h4><p>glance-registry 是系统后台运行的服务进程。 负责处理和存取 image 的 metadata，例如 image 的大小和类型。在控制节点上可以查看 glance-registry 进程</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGf6K2mibNIzz9NGgW1sAfPNkRicyFx3ALNxOKlicibuwxR01XWbJicajicF1yAAnCKVDIQMx4XrOuZACFA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>Glance 支持多种格式的 image，包括</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGf6K2mibNIzz9NGgW1sAfPNykY5eXxtnQiaUnHhMR7x2sAQ3sNd9VgptlVjMZib6GOZPkw21fnUuZOQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<h4 id="Database"><a href="#Database" class="headerlink" title="Database"></a><strong>Database</strong></h4><p>Image 的 metadata 会保持到 database 中，默认是 MySQL。 在控制节点上可以查看 glance 的 database 信息</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGf6K2mibNIzz9NGgW1sAfPNB38ZPb2w8xMVUs8XZZZ3aJ4TpIy7PTsv68Sg1WhhXQPeZJStAciaNKw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<h4 id="Store-backend"><a href="#Store-backend" class="headerlink" title="Store backend"></a><strong>Store backend</strong></h4><p>Glance 自己并不存储 image。 真正的 image 是存放在 backend 中的。 Glance 支持多种 backend，包括：</p>
<ol>
<li>A directory on a local file system（这是默认配置）</li>
<li>GridFS</li>
<li>Ceph RBD</li>
<li>Amazon S3</li>
<li>Sheepdog</li>
<li>OpenStack Block Storage (Cinder)</li>
<li>OpenStack Object Storage (Swift)</li>
<li>VMware ESX</li>
</ol>
<p>具体使用哪种 backend，是在 /etc/glance/glance-api.conf 中配置的<br>在我们的 devstack 环境中，image 存放在控制节点本地目录 /opt/stack/data/glance/images/ 中</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGf6K2mibNIzz9NGgW1sAfPNiaU5wkNHXPNHlLNzLAyz8VricN8XxG7B6n0T8UupITiboz0oAWpeFJhaw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>其他 backend 的配置可参考<a target="_blank" rel="noopener" href="http://docs.openstack.org/liberty/config-reference/content/configuring-image-service-backends.html">http://docs.openstack.org/liberty/config-reference/content/configuring-image-service-backends.html</a></p>
<p>查看目前已经存在的 image</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGf6K2mibNIzz9NGgW1sAfPNYywoRibibobzDHUkibR1y1wexueDmQnDcelsa2jgv0Nnyt812kuVCFM5g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>查看保存目录</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGf6K2mibNIzz9NGgW1sAfPN51bK22FuiaUicc4ES3WneFxvIfkj69rtCKtSzzvWDqlur3yL1QQsFiaFg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>每个 image 在目录下都对应有一个文件，文件以 image 的 ID 命名。</p>
<h3 id="CLI-创建-image"><a href="#CLI-创建-image" class="headerlink" title="CLI 创建 image"></a>CLI 创建 image</h3><p>cirros 这个 linux 镜像很小，通过 Web UI 上传很快，操作会很顺畅。但如果我们要上传的镜像比较大（比如好几个 G ），那么操作会长时间停留在上传的 Web 界面，我们也不知道目前到底处于什么状态。 对于这样的操作，CLI 是更好的选择。</p>
<ol>
<li><p>将 image 上传到控制节点的文件系统中，例如 /tmp/cirros-0.3.4-x86_64-disk.img</p>
</li>
<li><p>设置环境变量<br><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFw4ICM9axnNmF0hriaVdzGFkw04rhxv4kibV9jcBbhxOayLOVeNudtblkrJMG8sZuDXEibDS1JkqLzw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"><br>Devstack 的安装目录下有个 openrc 文件。source 该文件就可以配置 CLI 的环境变量。这里我们传入了两个参数，第一个参数是 OpenStack 用户名 admin；第二个参数是 Project 名 admin</p>
</li>
<li><p>执行 image 创建命令</p>
</li>
</ol>
<blockquote>
<p>glance image-create –name cirros –file /tmp/cirros-0.3.4-x86_64-disk.img –disk-format qcow2 –container-format bare –progress</p>
</blockquote>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFw4ICM9axnNmF0hriaVdzGF77o6825q5KpNs4oLOWecxXOYCgAT23sxJ8TSgwKKyMsskibAmhxeibtA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>在创建 image 的 CLI 参数中我们用 –progress 让其显示文件上传的百分比 %，是不是比 Web UI更直观呢？</p>
<p>在 /opt/stack/data/glance/images/ 下查看新的 Image<br><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFw4ICM9axnNmF0hriaVdzGFSCgUvvRBK3EuJuoOFNbYavs1Jppa12wdtel2mblzHH4tgt682kqp4g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<h1 id="计算Nova"><a href="#计算Nova" class="headerlink" title="计算Nova"></a>计算Nova</h1><p>Compute Service Nova 是 OpenStack 最核心的服务，负责维护和管理云环境的计算资源。OpenStack 作为 IaaS 的云操作系统，虚拟机生命周期管理也就是通过 Nova 来实现的。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGJPebq3Rz7Ng3ibj5FlecmqKgwxsIhShOn3DcoSGyyCKDnQINagzSrmpUHdBNDz5pROg4b8t2c9Bw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>在上图中可以看到，Nova 处于 Openstak 架构的中心，其他组件都为 Nova 提供支持： Glance 为 VM 提供 image ；Cinder 和 Swift 分别为 VM 提供块存储和对象存储； Neutron 为 VM 提供网络连接。Nova 架构如下：</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGJPebq3Rz7Ng3ibj5FlecmqHNp317xLg3wSRZdbM0zMbKzXmplOrWpVEvqNZvCm37LmcbbCPdLMFw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>Nova 的架构比较复杂，包含很多组件。 这些组件以子服务（后台 deamon 进程）的形式运行，可以分为以下几类：</p>
<ol>
<li><strong>API</strong></li>
</ol>
<p><strong>nova-api</strong></p>
<p>接收和响应客户的 API 调用。 除了提供 OpenStack 自己的API，nova-api 还支持 Amazon EC2 API。 也就是说，如果客户以前使用 Amazon EC2，并且用 EC2 的 API 开发了些工具来管理虚机，那么如果现在要换成 OpenStack，这些工具可以无缝迁移到 OpenStack，因为 nova-api 兼容 EC2 API，无需做任何修改。</p>
<ol start="2">
<li><strong>Compute Core</strong></li>
</ol>
<p>**nova-scheduler<br>**</p>
<p>虚机调度服务，负责决定在哪个计算节点上运行虚机</p>
<p>**nova-compute<br>**</p>
<p>管理虚机的核心服务，通过调用 Hypervisor API 实现虚机生命周期管理</p>
<p><strong>Hypervisor</strong><br>计算节点上跑的虚拟化管理程序，虚机管理最底层的程序。 不同虚拟化技术提供自己的 Hypervisor。 常用的 Hypervisor 有 KVM，Xen， VMWare 等</p>
<p>**nova-conductor<br>**nova-compute 经常需要更新数据库，比如更新虚机的状态。 出于安全性和伸缩性的考虑，nova-compute 并不会直接访问数据库，而是将这个任务委托给 nova-conductor，这个我们后面详细讨论。</p>
<ol start="3">
<li><strong>Console Interface</strong></li>
</ol>
<p>**nova-console<br>**用户可以通过多种方式访问虚机的控制台：<br>nova-novncproxy，基于 Web 浏览器的 VNC 访问<br>nova-spicehtml5proxy，基于 HTML5 浏览器的 SPICE 访问<br>nova-xvpnvncproxy，基于 Java 客户端的 VNC 访问</p>
<p>**nova-consoleauth<br>**负责对访问虚机控制台请求提供 Token 认证</p>
<p>**nova-cert<br>**</p>
<p>提供 x509 证书支持</p>
<p><strong>Database</strong></p>
<p>Nova 会有一些数据需要存放到数据库中，一般使用 MySQL。数据库安装在控制节点上。 Nova 使用命名为 “nova” 的数据库。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGJPebq3Rz7Ng3ibj5FlecmqXXgZK8iasQUn19266AU5OjrBEBia04XWiayibdSEt9HuyicicfWSMmiaphaVA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<ol start="4">
<li><strong>Message Queue</strong></li>
</ol>
<p>在前面我们了解到 Nova 包含众多的子服务，这些子服务之间需要相互协调和通信。为解耦各个子服务，Nova 通过 Message Queue 作为子服务的信息中转站。 所以在架构图上我们看到了子服务之间没有直接的连线，是通过 Message Queue 联系的。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGJPebq3Rz7Ng3ibj5Flecmq1MnOd7sk1MQ9qaqGnblVDIO3AWldVC272H8moOq1jWOrXc9vKNEtLQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>OpenStack 默认是用 RabbitMQ 作为 Message Queue。 MQ 是 OpenStack 的核心基础组件，我们后面也会详细介绍。</p>
<h3 id="Nova物理部署方案"><a href="#Nova物理部署方案" class="headerlink" title="Nova物理部署方案"></a>Nova物理部署方案</h3><p>前面大家已经看到 Nova 由很多子服务组成，我们也知道 OpenStack 是一个分布式系统，可以部署到若干节点上，那么接下来大家可能就会问：Nova 的这些服务在物理上应该如何部署呢？</p>
<p>对于 Nova，这些服务会部署在两类节点上：计算节点和控制节点。</p>
<p>计算节点上安装了 Hypervisor，上面运行虚拟机。 由此可知：</p>
<ol>
<li>只有 nova-compute 需要放在计算节点上。</li>
<li>其他子服务则是放在控制节点上的。</li>
</ol>
<p>下面我们可以看看实验环境的具体部署情况。 通过在计算节点和控制节点上运行 ps -elf|grep nova 来查看运行的 nova 子服务</p>
<p><strong>计算节点</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFibI6JwibQfktJ1WaTjzzf4T4YbLP6QicpCdZVb7E9rMQV42cQM5icDonQ52Ria5Wc9Pla3YBWqHdqeDg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>计算节点 devstack-compute1 上只运行了 nova-compute 子服务</p>
<p><strong>控制节点</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFibI6JwibQfktJ1WaTjzzf4TGzZZyOaLQ0rKQ4z32uSOoZal6VPLVKMVAdhSS2hAurER56ctOjT30A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>控制节点 devstack-controller 上运行了若干 nova-* 子服务</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFibI6JwibQfktJ1WaTjzzf4TDwBbic4NWnDw7lWXWuAsH5ibS5JP9dWR7q0oaVxCzKM1yD0IXNmJOicWA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>RabbitMQ 和 MySQL 也是放在控制节点上的。可能细心的同学已经发现我们的控制节点上也运行了 nova-compute。 这实际上也就意味着 devstack-controller 既是一个控制节点，同时也是一个计算节点，也可以在上面运行虚机。</p>
<p>这也向我们展示了 OpenStack 这种分布式架构部署上的灵活性： 可以将所有服务都放在一台物理机上，作为一个 All-in-One 的测试环境； 也可以将服务部署在多台物理机上，获得更好的性能和高可用。</p>
<p>另外，也可以用 nova service-list 查看 nova-* 子服务都分布在哪些节点上</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFibI6JwibQfktJ1WaTjzzf4TnljsTN8FSxnMLQkHNM7azuDgZZQLoIGgSy4D8HxlLlCjKQuib7BBsiaQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<h3 id="从虚拟机创建流程看-nova-子服务如何协同工作"><a href="#从虚拟机创建流程看-nova-子服务如何协同工作" class="headerlink" title="从虚拟机创建流程看 nova-* 子服务如何协同工作"></a>从虚拟机创建流程看 nova-* 子服务如何协同工作</h3><p>从学习 Nova 的角度看，虚机创建是一个非常好的场景，涉及的 nova-* 子服务很全，下面是流程图。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFibI6JwibQfktJ1WaTjzzf4TRWKlXWqV4VregAMebcImDmjicJBaMskoSOz7Qxvn0tQUGpovezyusibA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<ol>
<li><p>客户（可以是 OpenStack 最终用户，也可以是其他程序）向 API（nova-api）发送请求：“帮我创建一个虚机”</p>
</li>
<li><p>API 对请求做一些必要处理后，向 Messaging（RabbitMQ）发送了一条消息：“让 Scheduler 创建一个虚机”</p>
</li>
<li><p>Scheduler（nova-scheduler）从 Messaging 获取到 API 发给它的消息，然后执行调度算法，从若干计算节点中选出节点 A</p>
</li>
<li><p>Scheduler 向 Messaging 发送了一条消息：“在计算节点 A 上创建这个虚机”</p>
</li>
<li><p>计算节点 A 的 Compute（nova-compute）从 Messaging 中获取到 Scheduler 发给它的消息，然后在本节点的 Hypervisor 上启动虚机。</p>
</li>
<li><p>在虚机创建的过程中，Compute 如果需要查询或更新数据库信息，会通过 Messaging 向 Conductor（nova-conductor）发送消息，Conductor 负责数据库访问。</p>
</li>
</ol>
<p>以上是创建虚机最核心的步骤，当然省略了很多细节，我们会在后面的章节详细讨论。 这几个步骤向我们展示了 nova-* 子服务之间的协作的方式，也体现了 OpenStack 整个系统的分布式设计思想，掌握这种思想对我们深入理解 OpenStack 会非常有帮助。</p>
<h3 id="OpenStack通用设计思路"><a href="#OpenStack通用设计思路" class="headerlink" title="OpenStack通用设计思路"></a>OpenStack通用设计思路</h3><p><strong>API 前端服务</strong></p>
<p>每个 OpenStack 组件可能包含若干子服务，其中必定有一个 API 服务负责接收客户请求。</p>
<p>以 Nova 为例，nova-api 作为 Nova 组件对外的唯一窗口，向客户暴露 Nova 能够提供的功能。 当客户需要执行虚机相关的操作，能且只能向 nova-api 发送 REST 请求。 这里的客户包括终端用户、命令行和 OpenStack 其他组件。</p>
<p>设计 API 前端服务的好处在于：</p>
<ol>
<li>对外提供统一接口，隐藏实现细节</li>
<li>API 提供 REST 标准调用服务，便于与第三方系统集成</li>
<li>可以通过运行多个 API 服务实例轻松实现 API 的高可用，比如运行多个 nova-api 进程</li>
</ol>
<p><strong>Scheduler 调度服务</strong></p>
<p>对于某项操作，如果有多个实体都能够完成任务，那么通常会有一个 scheduler 负责从这些实体中挑选出一个最合适的来执行操作。</p>
<p>在前面的例子中，Nova 有多个计算节点。 当需要创建虚机时，nova-scheduler 会根据计算节点当时的资源使用情况选择一个最合适的计算节点来运行虚机。</p>
<p>调度服务好比是开发团队中的项目经理，当接到新的开发任务后，项目经理会评估任务的难度，考察团队成员目前的工作负荷和技能水平，然后将任务分配给最合适的开发人员。除了 Nova，块服务组件 Cinder 也有 scheduler 子服务，后面我们会详细讨论。</p>
<p><strong>Worker 工作服务</strong></p>
<p>调度服务只管分配任务，真正执行任务的是 Worker 工作服务。在 Nova 中，这个 Worker 就是 nova-compute 了。 将 Scheduler 和 Worker 从职能上进行划分使得 OpenStack 非常容易扩展：</p>
<ol>
<li><p>当计算资源不够了无法创建虚机时，可以增加计算节点（增加 Worker）</p>
</li>
<li><p>当客户的请求量太大调度不过来时，可以增加 Scheduler</p>
</li>
</ol>
<p><strong>Driver 框架</strong></p>
<p>OpenStack 作为开放的 Infrastracture as a Service 云操作系统，支持业界各种优秀的技术。这些技术可能是开源免费的，也可能是商业收费的。 这种开放的架构使得 OpenStack 能够在技术上保持先进性，具有很强的竞争力，同时又不会造成厂商锁定（Lock-in）。</p>
<p>那 OpenStack 的这种开放性体现在哪里呢？</p>
<p>一个重要的方面就是采用基于 Driver 的框架。以 Nova 为例，OpenStack 的计算节点支持多种 Hypervisor。 包括 KVM, Hyper-V, VMWare, Xen, Docker, LXC 等。Nova-compute 为这些 Hypervisor 定义了统一的接口，hypervisor 只需要实现这些接口，就可以 driver 的形式即插即用到 OpenStack 中。 下面是 nova driver 的架构示意图</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFp0CQbZfhvePDaiaLHApfXweGlUiaBQz6jNXOAe1QnB3V4OGqsVQkPDUJQHicvTlx1lMNNeMxXWuasQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>在 nova-compute 的配置文件 /etc/nova/nova.conf 中由 compute_driver 配置项指定该计算节点使用哪种 Hypervisor 的 driver</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFp0CQbZfhvePDaiaLHApfXwY03evSaDhos36slFHU9jNC3ibs5jhgF8PKcCpvC3juHeqhyh29Wiaoibg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片">   </p>
<p>在我们的环境中因为是 KVM，所以配置的是 Libvirt 的 driver。</p>
<p>不知大家是否记得我们在学习 Glance 时谈到： OpenStack 支持多种 backend 来存放 image。 可以是本地文件系统，Cinder，Ceph，Swift 等。其实这也是一个 driver 架构。 只要符合 Glance 定义的规范，新的存储方式可以很方便的加入到 backend 支持列表中。在后面 Cinder 和 Neutron 中我们还会看到 driver 框架的应用。</p>
<p><strong>Messaging 服务</strong></p>
<p>在前面创建虚机的流程示意图中，我们看到 nova-* 子服务之间的调用严重依赖 Messaging。Messaging 是 nova-* 子服务交互的中枢。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFp0CQbZfhvePDaiaLHApfXwZ1HkUPq0ctJUDhLOG6jibliaH2xXFdmdPUpFrE8PFjgzsfmFA9t7ECJQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>以前没接触过分布式系统的同学可能会不太理解为什么不让 API 直接调用Scheduler，或是让Scheuler 直接调用 Compute，而是非要通过 Messaging 进行中转。 这里做一些解释。程序之间的调用通常分两种：同步调用和异步调用。</p>
<p><strong>同步调用</strong></p>
<p>API 直接调用 Scheduler 的接口是同步调用。 其特点是 API 发出请求后需要一直等待，直到 Scheduler 完成对 Compute 的调度，将结果返回给 API 后 API 才能够继续做后面的工作。</p>
<p><strong>异步调用</strong></p>
<p>API 通过 Messaging 间接调用 Scheduler 就是异步调用。 其特点是 API 发出请求后不需要等待，直接返回，继续做后面的工作。 Scheduler 从 Messaging 接收到请求后执行调度操作，完成后将结果也通过 Messaging 发送给 API。在 OpenStack 这类分布式系统中，通常采用异步调用的方式，其好处是：</p>
<ol>
<li><p>解耦各子服务。 子服务不需要知道其他服务在哪里运行，只需要发送消息给 Messaging 就能完成调用。</p>
</li>
<li><p>提高性能 异步调用使得调用者无需等待结果返回。这样可以继续执行更多的工作，提高系统总的吞吐量。</p>
</li>
<li><p>提高伸缩性 子服务可以根据需要进行扩展，启动更多的实例处理更多的请求，在提高可用性的同时也提高了整个系统的伸缩性。而且这种变化不会影响到其他子服务，也就是说变化对别人是透明的。</p>
</li>
</ol>
<p>在后面各章节，我们都能看到 Messaging 的应用。</p>
<p><strong>Database</strong></p>
<p>OpenStack 各组件需要维护自己的状态信息。比如 Nova 中有虚机的规格、状态，这些信息都是在数据库中维护的。 每个 OpenStack 组件在 MySQL 中有自己的数据库。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFp0CQbZfhvePDaiaLHApfXwxQp6seHuX8ibGeqqCnu0EKCqiacKNvmlJ149MrdbdPOe1GmwEC3yEQPQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p><strong>小结</strong></p>
<p>Nova 是 OpenStack 中最重要的组件，也是很典型的组件。Nova 充分体现了 OpenStack 的设计思路。 理解了这种思路，再来学习 OpenStack 的其他组件就能够举一反三，清晰容易很多。</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/UxQAxoSIhr3SOv0ISGTtYw">https://mp.weixin.qq.com/s/UxQAxoSIhr3SOv0ISGTtYw</a></p>
<p>我们在后面 Cinder 和 Neutron 的学习中还会回顾这些设计思路。</p>
<h1 id="存储Cinder"><a href="#存储Cinder" class="headerlink" title="存储Cinder"></a>存储Cinder</h1><h3 id="Cinder框架"><a href="#Cinder框架" class="headerlink" title="Cinder框架"></a>Cinder框架</h3><p>从本节开始我们学习 OpenStack 的 Block Storage Service，Cinder。</p>
<p><strong>理解 Block Storage</strong></p>
<p>操作系统获得存储空间的方式一般有两种：</p>
<ol>
<li><p>通过某种协议（SAS,SCSI,SAN,iSCSI 等）挂接裸硬盘，然后分区、格式化、创建文件系统；或者直接使用裸硬盘存储数据（数据库）</p>
</li>
<li><p>通过 NFS、CIFS 等 协议，mount 远程的文件系统</p>
</li>
</ol>
<p>第一种裸硬盘的方式叫做 Block Storage（块存储），每个裸硬盘通常也称作 Volume（卷） 第二种叫做文件系统存储。NAS 和 NFS 服务器，以及各种分布式文件系统提供的都是这种存储。</p>
<p><strong>理解 Block Storage Service</strong></p>
<p>Block Storage Servicet 提供对 volume 从创建到删除整个生命周期的管理。从 instance 的角度看，挂载的每一个 Volume 都是一块硬盘。OpenStack 提供 Block Storage Service 的是 Cinder，其具体功能是：</p>
<ol>
<li><p>提供 REST API 使用户能够查询和管理 volume、volume snapshot 以及 volume type</p>
</li>
<li><p>提供 scheduler 调度 volume 创建请求，合理优化存储资源的分配</p>
</li>
<li><p>通过 driver 架构支持多种 back-end（后端）存储方式，包括 LVM，NFS，Ceph 和其他诸如 EMC、IBM 等商业存储产品和方案</p>
</li>
</ol>
<p><strong>Cinder 架构</strong></p>
<p>下图是 cinder 的逻辑架构图</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHLd9PmsqE1ZNftI1TE2l7CokOFtGNmp736wqAWfJwx8AnC714u6LtvHucmGWDZgNmFwe4G8G1Mww/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>Cinder 包含如下几个组件：</p>
<p>**cinder-api<br>**接收 API 请求，调用 cinder-volume 。</p>
<p>**cinder-volume<br>**管理 volume 的服务，与 volume provider 协调工作，管理 volume 的生命周期。运行 cinder-volume 服务的节点被称作为存储节点。</p>
<p>**cinder-scheduler<br>**scheduler 通过调度算法选择最合适的存储节点创建 volume。</p>
<p>**volume provider<br>**数据的存储设备，为 volume 提供物理存储空间。 cinder-volume 支持多种 volume provider，每种 volume provider 通过自己的 driver 与cinder-volume 协调工作。</p>
<p>**Message Queue<br>**Cinder 各个子服务通过消息队列实现进程间通信和相互协作。因为有了消息队列，子服务之间实现了解耦，这种松散的结构也是分布式系统的重要特征。</p>
<p><strong>Database Cinder</strong> </p>
<p>有一些数据需要存放到数据库中，一般使用 MySQL。数据库是安装在控制节点上的，比如在我们的实验环境中，可以访问名称为“cinder”的数据库。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHLd9PmsqE1ZNftI1TE2l7CxMKsjeAZqnGWvOs48icEiam9KM9ISoHT9uTOFeLPkFKsZY9wErc96JCw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p><strong>物理部署方案</strong></p>
<p>Cinder 的服务会部署在两类节点上，控制节点和存储节点。我们来看看控制节点 devstack-controller 上都运行了哪些 cinder-* 子服务。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHLd9PmsqE1ZNftI1TE2l7Cu6zMeoesOEYtxhAWLeNicpqd2C8KYqnrtpv7aCz8P0M6MtichO7n7Txg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>cinder-api 和 cinder-scheduler 部署在控制节点上，这个很合理。</p>
<p>至于 cinder-volume 也在控制节点上可能有些同学就会迷糊了：cinder-volume 不是应该部署在存储节点上吗？</p>
<p>要回答这个问题，首先要搞清楚一个事实： OpenStack 是分布式系统，其每个子服务都可以部署在任何地方，只要网络能够连通。无论是哪个节点，只要上面运行了 cinder-volume，它就是一个存储节点，当然，该节点上也可以运行其他 OpenStack服务。</p>
<p>cinder-volume 是一顶存储节点帽子，cinder-api 是一顶控制节点帽子。在我们的环境中，devstack-controller 同时戴上了这两顶帽子，所以它既是控制节点，又是存储节点。当然，我们也可以用一个专门的节点来运行 cinder-volume。</p>
<p>这再一次展示了 OpenStack 分布式架构部署上的灵活性： 可以将所有服务都放在一台物理机上，用作一个 All-in-One 的测试环境；而在生产环境中可以将服务部署在多台物理机上，获得更好的性能和高可用。</p>
<p>RabbitMQ 和 MySQL 通常放在控制节点上。另外，也可以用 cinder service list 查看 cinder-* 子服务都分布在哪些节点上</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHLd9PmsqE1ZNftI1TE2l7CtOib4cKjOdhrG6PZdicOGic55SywOK57tWwpBB3dVEcmS0L6neVIpKsicw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>还有一个问题：volume provider 放在那里？</p>
<p>一般来讲，volume provider 是独立的。cinder-volume 使用 driver 与 volume provider 通信并协调工作。所以只需要将 driver 与 cinder-volume 放到一起就可以了。在 cinder-volume 的源代码目录下有很多 driver，支持不同的 volume provider。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqHLd9PmsqE1ZNftI1TE2l7CLK2oSbyLhJKjzJwxibb4MCicP4tQZzcn7rNKzMK3Z6lLtlFWSSYQTWjg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>后面我们会以 LVM 和 NFS 这两种 volume provider 为例讨论 cinder-volume 的使用，其他 volume provider 可以查看 OpenStack 的 configuration 文档。</p>
<p>下一节我们将讨论 <a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653587767&idx=1&sn=e3535a2ccf793e5591d633cbd9d5879c&chksm=8d30812eba470838464262dc60bed1b50c4ad63342a7dae3a7b1af08ab4effcd463efbbe7209&scene=21#wechat_redirect">Cinder 的这些组件如何协调工作</a>。</p>
<h3 id="Cinder-设计思想"><a href="#Cinder-设计思想" class="headerlink" title="Cinder 设计思想"></a>Cinder 设计思想</h3><p>上一节介绍了<a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653587138&idx=1&sn=eedfae7ccae28a5081342b29035cb578&scene=21#wechat_redirect"> Cinder 的架构</a>，这节讨论 Cinder 个组件如何协同工作及其设计思想。</p>
<p><strong>从 volume 创建流程看 cinder-* 子服务如何协同工作</strong></p>
<p>对于 Cinder 学习来说，Volume 创建是一个非常好的场景，涉及各个 cinder-* 子服务，下面是流程图。<img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFDVXiaL6zITRP8Ol2r6gWEVQp6lA7mLyiaWPV9ORJicCAQOQcrNXvV1rWF5MbjkcgyrUO15eG7fxxKQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<ol>
<li><p>客户（可以是 OpenStack 最终用户，也可以是其他程序）向 API（cinder-api）发送请求：“帮我创建一个 volume”</p>
</li>
<li><p>API 对请求做一些必要处理后，向 Messaging（RabbitMQ）发送了一条消息：“让 Scheduler 创建一个 volume”</p>
</li>
<li><p>Scheduler（cinder-scheduler）从 Messaging 获取到 API 发给它的消息，然后执行调度算法，从若干计存储点中选出节点 A</p>
</li>
<li><p>Scheduler 向 Messaging 发送了一条消息：“让存储节点 A 创建这个 volume”</p>
</li>
<li><p>存储节点 A 的 Volume（cinder-volume）从 Messaging 中获取到 Scheduler 发给它的消息，然后通过 driver 在 volume provider 上创建 volume。</p>
</li>
</ol>
<p>上面是创建虚机最核心的几个步骤，当然省略了很多细节，我们会在后面的章节详细讨论。</p>
<p><strong>Cinder 的设计思想</strong></p>
<p>Cinder 延续了 Nova 的以及其他组件的设计思想。</p>
<p><strong>API 前端服</strong>务</p>
<p>cinder-api 作为 Cinder 组件对外的唯一窗口，向客户暴露 Cinder 能够提供的功能，当客户需要执行 volume 相关的操作，能且只能向 cinder-api 发送 REST 请求。这里的客户包括终端用户、命令行和 OpenStack 其他组件。</p>
<p>设计 API 前端服务的好处在于：</p>
<ol>
<li>对外提供统一接口，隐藏实现细节</li>
<li>API 提供 REST 标准调用服务，便于与第三方系统集成</li>
<li>可以通过运行多个 API 服务实例轻松实现 API 的高可用，比如运行多个 cinder-api 进程</li>
</ol>
<p><strong>Scheduler 调度服务</strong></p>
<p>Cinder 可以有多个存储节点，当需要创建 volume 时，cinder-scheduler 会根据存储节点的属性和资源使用情况选择一个最合适的节点来创建 volume。</p>
<p>调度服务就好比是一个开发团队中的项目经理，当接到新的开发任务时，项目经理会根据任务的难度，每个团队成员目前的工作负荷和技能水平，将任务分配给最合适的开发人员。</p>
<p><strong>Worker 工作服务</strong></p>
<p>调度服务只管分配任务，真正执行任务的是 Worker 工作服务。</p>
<p>在 Cinder 中，这个 Worker 就是 cinder-volume 了。这种 Scheduler 和 Worker 之间职能上的划分使得 OpenStack 非常容易扩展：当存储资源不够时可以增加存储节点（增加 Worker）。 当客户的请求量太大调度不过来时，可以增加 Scheduler。</p>
<p><strong>Driver 框架</strong></p>
<p>OpenStack 作为开放的 Infrastracture as a Service 云操作系统，支持业界各种优秀的技术，这些技术可能是开源免费的，也可能是商业收费的。</p>
<p>这种开放的架构使得 OpenStack 保持技术上的先进性，具有很强的竞争力，同时又不会造成厂商锁定（Lock-in）。 那 OpenStack 的这种开放性体现在哪里呢？一个重要的方面就是采用基于 Driver 的框架。</p>
<p>以 Cinder 为例，存储节点支持多种 volume provider，包括 LVM, NFS, Ceph, GlusterFS，以及 EMC, IBM 等商业存储系统。 cinder-volume 为这些 volume provider 定义了统一的 driver 接口，volume provider 只需要实现这些接口，就可以 driver 的形式即插即用到 OpenStack 中。下面是 cinder driver 的架构示意图：</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFDVXiaL6zITRP8Ol2r6gWEVO8ia9mZVXLQvqohgOhQZgb2eZLIOMrgmibzAlR8PPfxmAl9w8DAj813Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>在 cinder-volume 的配置文件 /etc/cinder/cinder.conf 中 volume_driver 配置项设置该存储节点使用哪种 volume provider 的 driver，下面的示例表示使用的是 LVM。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFDVXiaL6zITRP8Ol2r6gWEVvA8S0REdgtpFn7eH9QmZuAHjX8ibYDmgzicY4OuAIdXTVJGvc1Zaw2Mg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>下一节我们将详细讨论 <a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653587763&idx=1&sn=1374e3e2bcc453b782f0eb8a78f46ad6&chksm=8d30812aba47083cbe1f0aa93b3eff408590397eb2c88fe040eafd15b04888d0d1d116405d18&scene=21#wechat_redirect">Cinder 的每一个组件</a>。</p>
<h3 id="Cinder-的组件"><a href="#Cinder-的组件" class="headerlink" title="Cinder 的组件"></a>Cinder 的组件</h3><p>从本节开始，我们将详细讲解 Cinder 的各个子服务。</p>
<h4 id="cinder-api"><a href="#cinder-api" class="headerlink" title="cinder-api"></a>cinder-api</h4><p><strong>cinder-api</strong></p>
<p>cinder-api 是整个 Cinder 组件的门户，所有 cinder 的请求都首先由 cinder-api 处理。cinder-api 向外界暴露若干 HTTP REST API 接口。在 keystone 中我们可以查询 cinder-api 的 endponits。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGTic0yDaDlEt7iaao8xIeJdWNgTGP6APlAdTUic8iaUfJicUTmC2wxlgbI4U5YE0oeqlGn5HWRgo2k9Mw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>客户端可以将请求发送到 endponits 指定的地址，向 cinder-api 请求操作。 当然，作为最终用户的我们不会直接发送 Rest API 请求。OpenStack CLI，Dashboard 和其他需要跟 Cinder 交换的组件会使用这些 API。</p>
<p>cinder-api 对接收到的 HTTP API 请求会做如下处理：</p>
<ol>
<li>检查客户端传人的参数是否合法有效</li>
<li>调用 cinder 其他子服务的处理客户端请求</li>
<li>将 cinder 其他子服务返回的结果序列号并返回给客户端</li>
</ol>
<p>cinder-api 接受哪些请求呢？简单的说，只要是 Volume 生命周期相关的操作，cinder-api 都可以响应。大部分操作都可以在 Dashboard 上看到。</p>
<p>打开 Volume 管理界面</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGTic0yDaDlEt7iaao8xIeJdWcA3ibeM5ibYRUtyDHUnMPDYdp6GBCiaeMJyS5U28G00yrVPibC6JlapZLQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>点击下拉箭头，列表中就是 cinder-api 可执行的操作。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGTic0yDaDlEt7iaao8xIeJdWsWcjBTZ10HDTQLCrJa9oaKG519iaibkxiazIdxPnDb8F6YXI1u28vvKlQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p><strong>cinder-scheduler</strong></p>
<p>创建 Volume 时，cinder-scheduler 会基于容量、Volume Type 等条件选择出最合适的存储节点，然后让其创建 Volume。这个部分比较多，我们下一次单独讨论。</p>
<p><strong>cinder-volume</strong></p>
<p>cinder-volume 在存储节点上运行，OpenStack 对 Volume 的操作，最后都是交给 cinder-volume 来完成的。cinder-volume 自身并不管理真正的存储设备，存储设备是由 volume provider 管理的。cinder-volume 与 volume provider 一起实现 volume 生命周期的管理。</p>
<p><strong>通过 Driver 架构支持多种 Volume Provider</strong></p>
<p>接着的问题是：现在市面上有这么多块存储产品和方案（volume provider），cinder-volume 如何与它们配合呢？</p>
<p>这就是我们之前讨论过的 Driver 架构。 cinder-volume 为这些 volume provider 定义了统一的接口，volume provider 只需要实现这些接口，就可以 Driver 的形式即插即用到 OpenStack 系统中。下面是 Cinder Driver 的架构示意图：</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGTic0yDaDlEt7iaao8xIeJdWMdvTzYEk3Wng4wGlD0jXkCM5ultVo687vFCEW5YHCQtAT9aJTJhgQw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>我们可以在 /opt/stack/cinder/cinder/volume/drivers/ 目录下查看到 OpenStack 源代码中已经自带了很多 volume provider 的 Driver：</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGTic0yDaDlEt7iaao8xIeJdW4AcbwbM5qKA2GPmohXPecWhicS4azKTp0piachMicCfOibotFogmyFjmnA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>存储节点在配置文件 /etc/cinder/cinder.conf 中用 volume_driver 选项配置使用的driver：</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGTic0yDaDlEt7iaao8xIeJdWhtdibD5yEvCJq5oIs5AYbOKPOwpplIyibpQJKDZ0SQYSy4CXsVJgu8SQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>这里 LVM 是我们使用的 volume provider。</p>
<p><strong>定期向 OpenStack 报告计算节点的状态</strong></p>
<p>在前面 cinder-scheduler 会用到 CapacityFilter 和 CapacityWeigher，它们都是通过存储节点的空闲容量来做筛选。 那这里有个问题：Cinder 是如何得知每个存储节点的空闲容量信息的呢？</p>
<p>答案就是：<strong>cinder-volume 会定期向 Cinder 报告</strong>。</p>
<p>从 cinder-volume 的日志 /opt/stack/logs/c-vol.log 可以发现每隔一段时间，cinder-volume 就会报告当前存储节点的资源使用情况。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqGTic0yDaDlEt7iaao8xIeJdWIvR1gI2IuO4vUajEQdXodwLMGGuzYN0FibZuXVL1Hvc3eQSTriaiazrQA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>因为在我们的实验环境中存储节点使用的是 LVM，所以在上面的日志看到存储节点通过“vgs”和”lvs”这两个命令获取 LVM 的容量使用信息。</p>
<p><strong>实现 volume 生命周期管理</strong></p>
<p>Cinder 对 volume 的生命周期的管理最终都是通过 cinder-volume 完成的，包括 volume 的 create、extend、attach、snapshot、delete 等，后面我们会详细讨论。下一节我们将详细讨论 cinder-scheduler 如何筛选 cinder-volume。</p>
<h4 id="cinder-schduler"><a href="#cinder-schduler" class="headerlink" title="cinder-schduler"></a>cinder-schduler</h4><p>上一节我们详细讨论了 cinder-api 和 cinder-volume，今天讨论另一个重要的 Cinder 组件 cinder-scheduler。</p>
<p>创建 Volume 时，cinder-scheduler 会基于容量、Volume Type 等条件选择出最合适的存储节点，然后让其创建 Volume。下面介绍 cinder-scheduler 是如何实现这个调度工作的。</p>
<p>在 /etc/cinder/cinder.conf 中，cinder 通过 scheduler_driver， scheduler_default_filters 和 scheduler_default_weighers 这三个参数来配置 cinder-scheduler。</p>
<p><strong>Filter scheduler</strong></p>
<p>Filter scheduler 是 cinder-scheduler 默认的调度器。</p>
<blockquote>
<p>scheduler_driver=cinder.scheduler.filter_scheduler.FilterScheduler</p>
</blockquote>
<p>与 Nova 一样，Cinder 也允许使用第三方 scheduler，配置 scheduler_driver 即可。</p>
<p>scheduler 调度过程如下：</p>
<ol>
<li>通过过滤器（filter）选择满足条件的存储节点（运行 cinder-volume）</li>
<li>通过权重计算（weighting）选择最优（权重值最大）的存储节点。</li>
</ol>
<p>可见，cinder-scheduler 的运行机制与 nova-scheduler 完全一样。</p>
<p><strong>Filter</strong></p>
<p>当 Filter scheduler 需要执行调度操作时，会让 filter 对存储节点进行判断，filter 返回 True 或者 False。cinder.conf 中 scheduler_default_filters 选项指定 filter scheduler 使用的 filter，默认值为：</p>
<blockquote>
<p>scheduler_default_filters = AvailabilityZoneFilter, CapacityFilter, CapabilitiesFilter</p>
</blockquote>
<p>Filter scheduler 将按照上面的顺序依次过滤：</p>
<p>AvailabilityZoneFilter</p>
<p>为提高容灾性和提供隔离服务，可以将存储节点和计算节点划分到不同的 Availability Zone 中。例如把一个机架上的机器划分在一个 Availability Zone 中。OpenStack 默认有一个命名为“Nova”的 Availability Zone，所有的节点初始都是放在“Nova”中。用户可以根据需要创建自己的 Availability Zone。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFq5oB4FnarfMMyOP87ibqrQ843WbfXuLQ6DRgZs2Q1exiahGWI7VxXLxwbAJZZdtAT32CEcBDqVIXA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>创建 Volume 时，需要指定 Volume 所属的 Availability Zone。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFq5oB4FnarfMMyOP87ibqrQw12TOdvw9kZoh3pXicxQgTU14hqBOlJBEW0eUeYT49DeCFjcqzFgFag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>cinder-scheduler 在做 filtering 时，会使用 AvailabilityZoneFilter 将不属于指定 Availability Zone 的存储节点过滤掉。</p>
<p>CapacityFilter</p>
<p>创建 Volume 时，用户会指定 Volume 的大小。CapacityFilter 的作用是将存储空间不能满足 Volume 创建需求的存储节点过滤掉。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFq5oB4FnarfMMyOP87ibqrQYaztxrefL4qKpgtTVVgB8ia2EKg3ic1hf6n6sIHNgxbIo3LqEGIn88Lw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>CapabilitiesFilter</p>
<p>不同的 Volume Provider 有自己的特性（Capabilities），比如是否支持 thin provision 等。Cinder 允许用户创建 Volume 时通过 Volume Type 指定需要的 Capabilities。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFq5oB4FnarfMMyOP87ibqrQV1bHaphZrryjz3qf68GepQ1dic0F2FVzhVWVFAibp0iavC7SCLsFNWO3w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>Volume Type 可以根据需要定义若干 Capabilities，详细描述 Volume 的属性。VolumeVolume Type 的作用与 Nova 的 flavor 类似。</p>
<p>Volume Type 在 Admin -&gt; System -&gt; Volume 菜单里管理</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFq5oB4FnarfMMyOP87ibqrQ0icjtiat2Tu6qj31O4ibZuYqQsrgSYH6G6bLeZvjfmgPG5XmRw3GiaGW0A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>通过 Volume Type 的 Extra Specs 定义 Capabilities</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFq5oB4FnarfMMyOP87ibqrQl1yS1ZUIUKfMhBq0hQf4icezugxpVbZDj4DrTRLngahRYtMlibb6iaq8A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>Extra Specs 是用 Key-Value 的形式定义。 不同的 Volume Provider 支持的 Extra Specs 不同，需要参考 Volume Provider 的文档。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFq5oB4FnarfMMyOP87ibqrQn0Bttt8QrfiaibhlExOQic9mtjwSVibWqWwujMv7qDhFm6ics2EpztZwDcQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>上图所示的 Volume Type 只有一个 Extra Specs “volume_backend_name”，这是最重要也是必须的 Extra Specs。</p>
<p>cinder-volume 会在自己的配置文件 /etc/cinder/cinder.conf 中设置“volume_backend_name”这个参数，其作用是为存储节点的 Volume Provider 命名。这样，CapabilitiesFilter 就可以通过 Volume Type 的“volume_backend_name”筛选出指定的 Volume Provider。</p>
<p>不同的存储节点可以在各自的 cinder.conf 中配置相同的 volume_backend_name，这是允许的。因为虽然存储节点不同，但它们可能使用的是一种 Volume Provider。</p>
<p>如果在第一步 filtering 环节选出了多个存储节点，那么接下来的 weighting 环节会挑选出最合适的一个节点。</p>
<p><strong>Weighter</strong></p>
<p>Filter Scheduler 通过 scheduler_default_weighers 指定计算权重的 weigher，默认为 CapacityWeigher。</p>
<blockquote>
<p>scheduler_default_weighers = CapacityWeigher</p>
</blockquote>
<p>如命名所示，CapacityWeigher 基于存储节点的空闲容量计算权重值，空闲最多的胜出。下一节我们将开始通过各种场景学习 Cinder。</p>
<h1 id="网络Neutron"><a href="#网络Neutron" class="headerlink" title="网络Neutron"></a>网络Neutron</h1><h3 id="Neutron概述"><a href="#Neutron概述" class="headerlink" title="Neutron概述"></a>Neutron概述</h3><p>从今天开始，我们将学习 OpenStack 的 Networking Service，Neutron。</p>
<p>Neutron 的难度会比前面所有模块都大一些，内容也多一些。为了帮助大家更好的掌握 Neutorn，CloudMan 也会分析地更详细一些。</p>
<p><strong>Neutron 概述</strong></p>
<p>传统的网络管理方式很大程度上依赖于管理员手工配置和维护各种网络硬件设备；而云环境下的网络已经变得非常复杂，特别是在多租户场景里，用户随时都可能需要创建、修改和删除网络，网络的连通性和隔离不已经太可能通过手工配置来保证了。</p>
<p>如何快速响应业务的需求对网络管理提出了更高的要求。传统的网络管理方式已经很难胜任这项工作，而“软件定义网络（software-defined networking, SDN）”所具有的灵活性和自动化优势使其成为云时代网络管理的主流。</p>
<p>Neutron 的设计目标是实现“网络即服务（Networking as a Service）”。为了达到这一目标，在设计上遵循了基于 SDN 实现网络虚拟化的原则，在实现上充分利用了 Linux 系统上的各种网络相关的技术。</p>
<p>在这一章，我们将讨论 Neutron 的功能和它的各个组件，学习部署和配置 OpenStack 网络的不同方法，会涉及软件和硬件设备多个层面。</p>
<p><strong>Neutron 功能</strong></p>
<p>Neutron 为整个 OpenStack 环境提供网络支持，包括二层交换，三层路由，负载均衡，防火墙和 VPN 等。Neutron 提供了一个灵活的框架，通过配置，无论是开源还是商业软件都可以被用来实现这些功能。</p>
<p>二层交换 Switching</p>
<p>Nova 的 Instance 是通过虚拟交换机连接到虚拟二层网络的。Neutron 支持多种虚拟交换机，包括 Linux 原生的 Linux Bridge 和 Open vSwitch。 Open vSwitch（OVS）是一个开源的虚拟交换机，它支持标准的管理接口和协议。</p>
<p>利用 Linux Bridge 和 OVS，Neutron 除了可以创建传统的 VLAN 网络，还可以创建基于隧道技术的 Overlay 网络，比如 VxLAN 和 GRE（Linux Bridge 目前只支持 VxLAN）。在后面章节我们会学习如何使用和配置 Linux Bridge 和 Open vSwitch。</p>
<p>三层路由 Routing</p>
<p>Instance 可以配置不同网段的 IP，Neutron 的 router（虚拟路由器）实现 instance 跨网段通信。router 通过 IP forwarding，iptables 等技术来实现路由和 NAT。我们将在后面章节讨论如何在 Neutron 中配置 router 来实现 instance 之间，以及与外部网络的通信。</p>
<p>负载均衡 Load Balancing</p>
<p>Openstack 在 Grizzly 版本第一次引入了 Load-Balancing-as-a-Service（LBaaS），提供了将负载分发到多个 instance 的能力。LBaaS 支持多种负载均衡产品和方案，不同的实现以 Plugin 的形式集成到 Neutron，目前默认的 Plugin 是 HAProxy。我们会在后面章节学习 LBaaS 的使用和配置。</p>
<p>防火墙 Firewalling</p>
<p>Neutron 通过下面两种方式来保障 instance 和网络的安全性。</p>
<p>Security Group </p>
<p>通过 iptables 限制进出 instance 的网络包。</p>
<p>Firewall-as-a-Service<br>WaaS，限制进出虚拟路由器的网络包，也是通过 iptables 实现。</p>
<p>后面章节会详细讨论 Security 和 FWaaS。下一节我们会讨论 <a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653587694&idx=1&sn=7d844a30662b9af11de26d7f4322e9c9&chksm=8d3080f7ba4709e12ea3c32c314cb5b93c3e23d751cda63114234821f9946ef5c4d3a4880f25&scene=21#wechat_redirect">Neutron 网络涉及的一些基本概念</a>，便于后面深入学习。</p>
<h3 id="Neutron网络基本概念"><a href="#Neutron网络基本概念" class="headerlink" title="Neutron网络基本概念"></a>Neutron网络基本概念</h3><p>上次我们讨论了 <a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653587201&idx=1&sn=d35b7e5932adf85fdcb744e9d91d6f8d&scene=21#wechat_redirect">Neutron 提供的功能</a>，今天我们学习 Neutron 模块几个重要的概念。Neutron 管理的网络资源包括 Network，subnet 和 port，下面依次介绍。</p>
<p><strong>network</strong></p>
<p>network 是一个隔离的二层广播域。Neutron 支持多种类型的 network，包括 local, flat, VLAN, VxLAN 和 GRE。</p>
<p><strong>local</strong><br>local 网络与其他网络和节点隔离。local 网络中的 instance 只能与位于同一节点上同一网络的 instance 通信，local 网络主要用于单机测试。</p>
<p><strong>flat</strong><br>flat 网络是无 vlan tagging 的网络。flat 网络中的 instance 能与位于同一网络的 instance 通信，并且可以跨多个节点。</p>
<p><strong>vlan</strong><br>vlan 网络是具有 802.1q tagging 的网络。vlan 是一个二层的广播域，同一 vlan 中的 instance 可以通信，不同 vlan 只能通过 router 通信。vlan 网络可跨节点，是应用最广泛的网络类型。</p>
<p><strong>vxlan</strong><br>vxlan 是基于隧道技术的 overlay 网络。vxlan 网络通过唯一的 segmentation ID（也叫 VNI）与其他 vxlan 网络区分。vxlan 中数据包会通过 VNI 封装成 UDP 包进行传输。因为二层的包通过封装在三层传输，能够克服 vlan 和物理网络基础设施的限制。</p>
<p><strong>gre</strong><br>gre 是与 vxlan 类似的一种 overlay 网络。主要区别在于使用 IP 包而非 UDP 进行封装。</p>
<p>不同 network 之间在二层上是隔离的。</p>
<p>以 vlan 网络为例，network A 和 network B 会分配不同的 VLAN ID，这样就保证了 network A 中的广播包不会跑到 network B 中。当然，这里的隔离是指二层上的隔离，借助路由器不同 network 是可能在三层上通信的。</p>
<p>network 必须属于某个 Project（ Tenant 租户），Project 中可以创建多个 network。 network 与 Project 之间是 1对多 关系。</p>
<p><strong>subnet</strong></p>
<p>subnet 是一个 IPv4 或者 IPv6 地址段。instance 的 IP 从 subnet 中分配。每个 subnet 需要定义 IP 地址的范围和掩码。</p>
<p>network 与 subnet 是 1对多 关系。一个 subnet 只能属于某个 network；一个 network 可以有多个 subnet，这些 subnet 可以是不同的 IP 段，但不能重叠。下面的配置是有效的：</p>
<p>network A  subnet A-a: 10.10.1.0/24  {“start”: “10.10.1.1”, “end”: “10.10.1.50”}</p>
<p>​          subnet A-b: 10.10.2.0/24  {“start”: “10.10.2.1”, “end”: “10.10.2.50”}</p>
<p>但下面的配置则无效，因为 subnet 有重叠</p>
<p>networkA   subnet A-a: 10.10.1.0/24  {“start”: “10.10.1.1”, “end”: “10.10.1.50”}</p>
<p>​          subnet A-b: 10.10.1.0/24  {“start”: “10.10.1.51”, “end”: “10.10.1.100”}</p>
<p>这里不是判断 IP 是否有重叠，而是 subnet 的 CIDR 重叠（都是 10.10.1.0/24）。但是，如果 subnet 在不同的 network 中，CIDR 和 IP 都是可以重叠的，比如</p>
<p>network A  subnet A-a: 10.10.1.0/24  {“start”: “10.10.1.1”, “end”: “10.10.1.50”}</p>
<p>networkB   subnet B-a: 10.10.1.0/24  {“start”: “10.10.1.1”, “end”: “10.10.1.50”}</p>
<p>这里大家不免会疑惑： 如果上面的IP地址是可以重叠的，那么就可能存在具有相同 IP 的两个 instance，这样会不会冲突？ 简单的回答是：不会！</p>
<p>具体原因： 因为 Neutron 的 router 是通过 Linux network namespace 实现的。network namespace 是一种网络的隔离机制。通过它，每个 router 有自己独立的路由表。上面的配置有两种结果：</p>
<ol>
<li><p>如果两个 subnet 是通过同一个 router 路由，根据 router 的配置，只有指定的一个 subnet 可被路由。</p>
</li>
<li><p>如果上面的两个 subnet 是通过不同 router 路由，因为 router 的路由表是独立的，所以两个 subnet 都可以被路由。</p>
</li>
</ol>
<p>这里只是先简单做个说明，我们会在后面三层路由的章节详细分析这种场景。</p>
<p><strong>port</strong></p>
<p>port 可以看做虚拟交换机上的一个端口。port 上定义了 MAC 地址和 IP 地址，当 instance 的虚拟网卡 VIF（Virtual Interface） 绑定到 port 时，port 会将 MAC 和 IP 分配给 VIF。</p>
<p>subnet 与 port 是 1对多 关系。一个 port 必须属于某个 subnet；一个 subnet 可以有多个 port。</p>
<p><strong>小节</strong></p>
<p>下面总结了 Project，Network，Subnet，Port 和 VIF 之间关系。</p>
<p>Project 1 : m Network 1 : m Subnet 1 : m Port 1 : 1 VIF m : 1 Instance</p>
<p>下一节我们讨论 <a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653587691&idx=1&sn=c71b110dade71c3e120ec6b2389b3e33&chksm=8d3080f2ba4709e44eb08c55223e141f7ed0e069ebd6f770b7770665e00ff72d48ce41596a0b&scene=21#wechat_redirect">Neutron 的架构</a>。</p>
<h3 id="Neutron的架构"><a href="#Neutron的架构" class="headerlink" title="Neutron的架构"></a>Neutron的架构</h3><p>前面我们讨论了 <a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653587204&idx=1&sn=9d4b36188bfd91a896d68062dad4ee83&scene=21#wechat_redirect">Neutron 的基本概念</a>，今天我们开始分析 Neutron 的架构。</p>
<p><strong>Neutron 架构</strong></p>
<p>与 OpenStack 的其他服务的设计思路一样，Neutron 也是采用分布式架构，由多个组件（子服务）共同对外提供网络服务。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEZ5ovWJOuB1iaq0vRiaplgJtus2jUWMVcnHbgjteEHz4rJoCsotcrR0C3hUoWiaC7DfpOqleG8lRVibA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>Neutron 由如下组件构成：</p>
<p>**Neutron Server<br>**对外提供 OpenStack 网络 API，接收请求，并调用 Plugin 处理请求。</p>
<p>**Plugin<br>**处理 Neutron Server 发来的请求，维护 OpenStack 逻辑网络状态， 并调用 Agent 处理请求。</p>
<p>**Agent<br>**处理 Plugin 的请求，负责在 network provider 上真正实现各种网络功能。</p>
<p>**network provider<br>**提供网络服务的虚拟或物理网络设备，例如 Linux Bridge，Open vSwitch 或者其他支持 Neutron 的物理交换机。</p>
<p>**Queue<br>**Neutron Server，Plugin 和 Agent 之间通过 Messaging Queue 通信和调用。</p>
<p>**Database<br>**存放 OpenStack 的网络状态信息，包括 Network, Subnet, Port, Router 等。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqEZ5ovWJOuB1iaq0vRiaplgJtEbDQkCyia1o8QZwqWOJgmmDlrbN454daWqqvaRjbKZqu117AjDscfZQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>Neutron 架构非常灵活，层次较多，目的是：</p>
<ol>
<li><p>为了支持各种现有或者将来会出现的优秀网络技术。</p>
</li>
<li><p>支持分布式部署，获得足够的扩展性。</p>
</li>
</ol>
<p>通常鱼和熊掌不能兼得，虽然获得了这些优势，但这样使得 Neutron 更加复杂，更不容易理解。 后面我们会详细讨论 Neutron 的各个组件，但在这之前，非常有必要先通过一个例子了解这些组件各自的职责以及是如何协同工作。</p>
<p>以创建一个 VLAN100 的 network 为例，假设 network provider 是 linux bridge， 流程如下：</p>
<blockquote>
<ol>
<li><p>Neutron Server 接收到创建 network 的请求，通过 Message Queue（RabbitMQ）通知已注册的 Linux Bridge Plugin。</p>
</li>
<li><p>Plugin 将要创建的 network 的信息（例如名称、VLAN ID等）保存到数据库中，并通过 Message Queue 通知运行在各节点上的 Agent。</p>
</li>
<li><p>Agent 收到消息后会在节点上的物理网卡（比如 eth2）上创建 VLAN 设备（比如 eth2.100），并创建 bridge （比如 brqXXX） 桥接 VLAN 设备。</p>
</li>
</ol>
</blockquote>
<p>关于 linux bridge 如何实现 VLAN 大家可以参考本教程“预备知识-&gt;网络虚拟化”的相关章节。这里进行几点说明：</p>
<ol>
<li><p>plugin 解决的是 What 的问题，即网络要配置成什么样子？而至于如何配置 How 的工作则交由 agent 完成。</p>
</li>
<li><p>plugin，agent 和 network provider 是配套使用的，比如上例中 network provider 是 linux bridge，那么就得使用 linux bridge 的 plungin 和 agent；如果 network provider 换成了 OVS 或者物理交换机，plugin 和 agent 也得替换。</p>
</li>
<li><p>plugin 的一个主要的职责是在数据库中维护 Neutron 网络的状态信息，这就造成一个问题：所有 network provider 的 plugin 都要编写一套非常类似的数据库访问代码。为了解决这个问题，Neutron 在 Havana 版本实现了一个 ML2（Modular Layer 2）plugin，对 plgin 的功能进行抽象和封装。有了 ML2 plugin，各种 network provider 无需开发自己的 plugin，只需要针对 ML2 开发相应的 driver 就可以了，工作量和难度都大大减少。ML2 会在后面详细讨论。</p>
</li>
<li><p>plugin 按照功能分为两类： core plugin 和 service plugin。core plugin 维护 Neutron 的 netowrk, subnet 和 port 相关资源的信息，与 core plugin 对应的 agent 包括 linux bridge, OVS 等； service plugin 提供 routing, firewall, load balance 等服务，也有相应的 agent。后面也会分别详细讨论。</p>
</li>
</ol>
<p>以上是Neutron的逻辑架构，下一节我们讨论 <a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653587687&idx=1&sn=adf2078fa91f552a23dd406bf9c0d30d&chksm=8d3080feba4709e8fca066ec1371c51f9b3a89229cfeb81f886e6895f0c2509a29e840ab6d19&scene=21#wechat_redirect">Neutron 的物理部署方案</a>。</p>
<h3 id="Neutron-物理部署方案"><a href="#Neutron-物理部署方案" class="headerlink" title="Neutron 物理部署方案"></a>Neutron 物理部署方案</h3><p>前面我们讨论了 <a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653587207&idx=1&sn=2ad3cff789378312144d41ca2e9c1dc9&scene=21#wechat_redirect">Neutron 的架构</a>，本节讨论 Neutron 的物理部署方案：不同节点部署不同的 Neutron 服务组件。</p>
<p><strong>方案1：控制节点 + 计算节点</strong></p>
<p>在这个部署方案中，OpenStack 由控制节点和计算节点组成。</p>
<p><strong>控制节点</strong><br>部署的服务包括：neutron server, core plugin 的 agent 和 service plugin 的 agent。</p>
<p><strong>计算节点</strong><br>部署 core plugin 的agent，负责提供二层网络功能。</p>
<p>这里有几点需要说明： </p>
<p>\1. core plugin 和 service plugin 已经集成到 neutron server，不需要运行独立的 plugin 服务。</p>
<p>\2. 控制节点和计算节点都需要部署 core plugin 的 agent，因为通过该 agent 控制节点与计算节点才能建立二层连接。</p>
<p>\3. 可以部署多个控制节点和计算节点。</p>
<p><img src="/2022/06/09/OpenStack/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p>
<p><strong>方案2：控制节点 + 网络节点 + 计算节点</strong></p>
<p>在这个部署方案中，OpenStack 由控制节点，网络节点和计算节点组成。</p>
<p>**控制节点<br>**</p>
<p>部署 neutron server 服务。</p>
<p>**网络节点<br>**部署的服务包括：core plugin 的 agent 和 service plugin 的 agent。</p>
<p>**计算节点<br>**部署 core plugin 的agent，负责提供二层网络功能。</p>
<p>这个方案的要点是将所有的 agent 从控制节点分离出来，部署到独立的网络节点上。</p>
<ol>
<li><p>控制节点只负责通过 neutron server 响应 API 请求。</p>
</li>
<li><p>由独立的网络节点实现数据的交换，路由以及 load balance等高级网络服务。</p>
</li>
<li><p>可以通过增加网络节点承担更大的负载。</p>
</li>
<li><p>可以部署多个控制节点、网络节点和计算节点。</p>
</li>
</ol>
<p>该方案特别适合规模较大的 OpenStack 环境。</p>
<p><img src="https://mmbiz.qlogo.cn/mmbiz/Hia4HVYXRicqHuicQalCOiaMzJkIfDGxicQibEt6Yiat5ia1Nae2zkDn7AnFDflibI8bHgibtV4EmGD8eXhycKbVU4FApZvg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1&retryload=2" alt="图片"></p>
<p>以上就是 Neutron 两种典型的部署方案，下一节我们开始讨论 Neutron 的各个服务组件。首先学习 <a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653587683&idx=1&sn=3c5cadd759831abf596d53859cb64b9d&chksm=8d3080faba4709ec386d03890f7ccec74a9adfb69a943466626428b237d4a3929e372496e8db&scene=21#wechat_redirect">Neutron Server</a> 。</p>
<h3 id="Neutron-Server-分层模型"><a href="#Neutron-Server-分层模型" class="headerlink" title="Neutron Server 分层模型"></a>Neutron Server 分层模型</h3><p>本节开始讨论 Neutron 的各个服务组件，首先学习 Neutron Server 。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqG2kaGXyYgYWVaMZBrLruk6LYphLicwibym5k79oEPDPAlrVCY9aQ9bDiahZQ2YTNZ8Ee6XgXqnhSGKQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"></p>
<p>上图是 Neutron Server 的分层结构，至上而下依次为：</p>
<p>**Core API<br>**对外提供管理 network, subnet 和 port 的 RESTful API。</p>
<p>**Extension API<br>**对外提供管理 router, load balance, firewall 等资源 的 RESTful API。</p>
<p>**Commnon Service<br>**认证和校验 API 请求。</p>
<p>**Neutron Core<br>**Neutron server 的核心处理程序，通过调用相应的 Plugin 处理请求。</p>
<p>**Core Plugin API<br>**定义了 Core Plgin 的抽象功能集合，Neutron Core 通过该 API 调用相应的 Core Plgin。</p>
<p>**Extension Plugin API<br>**定义了 Service Plgin 的抽象功能集合，Neutron Core 通过该 API 调用相应的 Service Plgin。</p>
<p>**Core Plugin<br>**实现了 Core Plugin API，在数据库中维护 network, subnet 和 port 的状态，并负责调用相应的 agent 在 network provider 上执行相关操作，比如创建 network。</p>
<p>**Service Plugin<br>**实现了 Extension Plugin API，在数据库中维护 router, load balance, security group 等资源的状态，并负责调用相应的 agent 在 network provider 上执行相关操作，比如创建 router。</p>
<p>归纳起来，Neutron Server 包括两部分：<br>\1. 提供 API 服务。<br>\2. 运行 Plugin。</p>
<p>即 <strong>Neutron Server = API + Plugins</strong></p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqG2kaGXyYgYWVaMZBrLruk6fRic3xJzD7tJjhr2g0OXKJaOz6DNopZv0JiaibdzBBNcZ4GJsOHh8JpQQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"><br>明白了 Neutron Server 的分层模型，我们就更容易理解 <a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzIwMTM5MjUwMg==&mid=2653587682&idx=1&sn=bd57acc0560ce9bcdcc7781ed24067df&chksm=8d3080fbba4709ed1dd5ba820daa2c845d1aff1d4cd5670b74337df386f3fbf2544ab5fb65c0&scene=21#wechat_redirect">Neutron 是如何支持多种 network provider</a>。这一点我们放到下节详细讨论。</p>
<h1 id="如何使用-OpenStack-CLI"><a href="#如何使用-OpenStack-CLI" class="headerlink" title="如何使用 OpenStack CLI"></a>如何使用 OpenStack CLI</h1><p>OpenStack 服务都有自己的 CLI。</p>
<p>命令很好记，就是服务的名字，比如 Glance 就是 glance，Nova 就是 nova。</p>
<p>但 Keystone 比较特殊，现在是用 openstack 来代替老版的 keystone 命令。 比如查询用户列表，如果用 keystone user-list</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFnjmCfCyrnKejYc9C8sHxoZ0O7zQTVVic7H0DDiccziaL8ZD3yc8yicn8D7j02Rr9I1G3MxZ2tIGJkvw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>会提示 keystone 已经 deprecated 了。 用 openstack 命令代替</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFnjmCfCyrnKejYc9C8sHxoY8lTica2iayAOzjcLnl7hcTKZMlXkg281SgwOxTFLvFAg4icLkJlREGQw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>不同服务用的命令虽然不同，但这些命令使用方式却非常类似，可以举一反三。</p>
<p>\1. 执行命令之前，需要设置环境变量。</p>
<p>这些变量包含用户名、Project、密码等； 如果不设置，每次执行命令都必须设置相关的命令行参数</p>
<p>\2. 各个服务的命令都有增、删、改、查的操作</p>
<p>其格式是</p>
<blockquote>
<p>CMD <obj>-create [parm1] [parm2]…<br>CMD <obj>-delete [parm]<br>CMD <obj>-update [parm1] [parm2]…<br>CMD <obj>-list CMD <obj>-show [parm]</obj></obj></obj></obj></obj></p>
</blockquote>
<p>例如 glance 管理的是 image，那么:<br>CMD 就是 glance，obj 就是 image，对应的命令就有</p>
<blockquote>
<p>glance image-create<br>glance image-delete<br>glance image-update<br>glance image-list<br>glance image-show</p>
</blockquote>
<p>再比如 neutron 负责管理网络和子网，那么：<br>CMD 就是 neutron；obj 就是 net 和 subnet 对应的命令就有</p>
<p>网络相关操作</p>
<blockquote>
<p>neutron net-create<br>neutron net -delete<br>neutron net -update<br>neutron net -list<br>neutron net –show</p>
</blockquote>
<p>子网相关操作</p>
<blockquote>
<p>neutron subnet-create<br>neutron subnet -delete<br>neutron subnet -update<br>neutron subnet -list<br>neutron subnet–show</p>
</blockquote>
<p>有的命令 <obj> 可以省略，比如 nova 下面的操作都是针对 instance</obj></p>
<blockquote>
<p>nova boot<br>nova delete<br>nova list nova show</p>
</blockquote>
<p>\3. 每个对象都有 ID</p>
<p>delete，show 等操作都以 ID 为参数，例如</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFnjmCfCyrnKejYc9C8sHxosDD7iaX5dVUBApKyE8J5Y6wzrJdoL8LRaicZUOibNGEsT4nwex8wtPia7g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>\4. 可用 help 查看命令的用法</p>
<p>除了 delete，show 等操作只需要 ID 一个参数，其他操作可能需要更多的参数，用 help 查看所需的参数，格式是</p>
<blockquote>
<p>CMD help [SUB-CMD]</p>
</blockquote>
<p>例如查看 glance 都有哪些 SUB-CMD</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFnjmCfCyrnKejYc9C8sHxoiaouoAy9x9HeoGbAlbxeZV1icICxlOggtu8TQDjYngjbQaCbDUwkrMMg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>查看 glance image-update 的用法</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFnjmCfCyrnKejYc9C8sHxoC6wkrs2JreTTXg8acclN6D0QWgo1PBKibG9mXe2VkiaAJD0tUcibFsn4g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p><strong>如何 Troubleshooting</strong></p>
<p>OpenStack 排查问题的方法主要是通过日志，Service 都有自己单独的日志。Glance 主要有两个日志，glance_api.log 和 glance_registry.log，保存在 /opt/stack/logs 目录里。devstack 的 screen 窗口已经帮我们打开了这两个日志，可以直接查看</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFnjmCfCyrnKejYc9C8sHxoHB1eic8Am009h7Yic2gice9UAZwjYickt0LGDBLPnfiawMPjyVQWzR24l6Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>g-api 窗口显示 glance-api 日志，记录 REST API 调用情况。<br>g-reg 窗口显示 glance-registry 日志，记录 Glance 服务处理请求的过程以及数据库操作。</p>
<p>如果需要得到最详细的日志信息，可以在 /etc/glance/*.conf 中打开 debug 选项。 devstack 默认已经打开了 debug。</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz/Hia4HVYXRicqFnjmCfCyrnKejYc9C8sHxo4z8oFPdchEt1zJian8icia6mzphEE8FtzvicMflwHwh9PKLicpvsWprVFQA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片"> </p>
<p>在非 devstack 安装中，日志在 /var/log/glance/ 目录里。</p>
<p>packstack –allinone –os-neutron-l2-agent=openvswitch –os-neutron-ml2-mechanism-drivers=openvswitch –os-neutron-ml2-tenant-network-types=vxlan –os-neutron-ml2-type-drivers=vxlan,flat –provision-demo=n –os-neutron-ovs-bridge-mappings=extnet:br-ex –os-neutron-ovs-bridge-interfaces=br-ex:eno1 –keystone-admin-passwd=’cnlab.net’ –provision-demo=n  –nova-libvirt-virt-type=kvm –os-swift-storage-size=100G –cinder-volumes-size=500G</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayi8991.github.io/2022/04/21/New-Computer-s-Test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiayi Liang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiayiSpace">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/21/New-Computer-s-Test/" class="post-title-link" itemprop="url">New Computer's Test</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-04-21 10:42:06" itemprop="dateCreated datePublished" datetime="2022-04-21T10:42:06+08:00">2022-04-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayi8991.github.io/2022/03/02/Octave%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiayi Liang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiayiSpace">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/02/Octave%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">Octave基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-03-02 14:52:47" itemprop="dateCreated datePublished" datetime="2022-03-02T14:52:47+08:00">2022-03-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-23 09:31:02" itemprop="dateModified" datetime="2022-03-23T09:31:02+08:00">2022-03-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Basic-operation-基础操作"><a href="#Basic-operation-基础操作" class="headerlink" title="Basic operation 基础操作"></a>Basic operation 基础操作</h1><p>算数运算：加减乘除； ==；~=；</p>
<p>逻辑运算：&amp;&amp;   -AND；  ||   -OR； </p>
<p>字符串： EX： a = ‘Hello world!’</p>
<p>disp函数： EX： disp( sprintf(‘ %0.2f’ ),variable )  %保留variable小数点后两位</p>
<p>矩阵： A = [1 2；2 3；4 5]</p>
<p>V = 1: 0.2: 2     %会生成一个增量为0.2的从1到2的 1行_列的矩阵; 如果中间不填，默认增量是1；</p>
<p>ones函数：ones（3，2）  %生成一个3行两列的 值都是1的矩阵； 同理zeros（）也是</p>
<p>rand（m，n）函数：随机在0-1范围内生成m*n矩阵的值</p>
<p>randn（m，n）函数：随机生成的数符合高斯分布/正态分布</p>
<p>hist（）：绘制直方图</p>
<p>eye( n )  : 生成n阶单位矩阵</p>
<p>size( ) : 返回矩阵的大小，返回的也是一个1行2列的矩阵；</p>
<p>size（A，1 or 2）：返回矩阵的行或列</p>
<p>help命令： 如 help eye；就可以获得eye函数的用法</p>
<h1 id="Moving-Data-Around-移动数据过来"><a href="#Moving-Data-Around-移动数据过来" class="headerlink" title="Moving Data Around 移动数据过来"></a>Moving Data Around 移动数据过来</h1><p><strong>load file.dat</strong></p>
<p><strong>load ( ‘file.dat ‘ )</strong></p>
<p>who函数： 展示目前工作空间中的所有变量</p>
<p>whos函数：更详细的展示变量的size,bytes</p>
<p>clear(variable): 删除某个变量</p>
<p>save  filename.mat v; : 将v保存为filename.mat的名字</p>
<p>or save filename.txt v -ascii;</p>
<p>查看矩阵元素的值：</p>
<p>如 A=[1 2；3 4；5 6]</p>
<p>A（2，2） ans = 4;   A(2,:) ans= 3, 4      % “:”代表着某行或某列所有的元素</p>
<p>A([1 3] , :) ans = 1 2; 5 6        % 1,3行的所有列元素</p>
<p>也可利用这个单独给某列赋值：</p>
<p>A（：，2） = [12 ；11； 10]</p>
<p>A = [A, [11;22;33]]                   %Add  a new column vector</p>
<p>A（：） 可以把所有元素变成一个变量</p>
<p>[ A B] 将两个行相等的矩阵放一起</p>
<p>[A ; B] 将列相等的矩阵竖置</p>
<h1 id="Plotting-Data-绘制数据"><a href="#Plotting-Data-绘制数据" class="headerlink" title="Plotting Data 绘制数据"></a>Plotting Data 绘制数据</h1><p><strong>常用命令：</strong></p>
<p>plot（x,y(x)）; </p>
<p>xlabel ( ‘ ‘ );   ylabel ( ‘ ‘ ) ;</p>
<p>legend( ‘ ‘ , ‘ ‘ );</p>
<p>title( ‘ ‘ )</p>
<p>print -dpng ‘filename.png’;</p>
<p>figure(1): plot(t ,y1)</p>
<p>figure(2): plot(t,y2)</p>
<p>subplot(1,2,1);</p>
<p>polar(thera , r): 绘制极坐标</p>
<p>cart2pol: 将直角坐标系转化为极坐标系</p>
<p>pol2cart: 极坐标转化为直角坐标系          ex： [x，y] = pol2cart（theta，r）</p>
<blockquote>
<p>误差棒图： errorbar（x，y，error）</p>
</blockquote>
<p><strong>向量图形：</strong></p>
<ol>
<li>箭头图</li>
</ol>
<p>quiver(U,V)   U,V 都是二维矩阵；</p>
<ol start="2">
<li>罗盘图</li>
</ol>
<p>compass（x，y）   ； </p>
<p><strong>三维图形：</strong></p>
<p>plot3（x，y，z）；</p>
<p>mesh();</p>
<p>peaks();</p>
<p>meshgrid( ) 将一维变成二维</p>
<h3 id="图形修饰处理"><a href="#图形修饰处理" class="headerlink" title="图形修饰处理"></a>图形修饰处理</h3><ol>
<li>坐标轴处理   —-    daspect</li>
<li>mesh();   hidden on / off</li>
<li>视角处理： view(az/方位角, el/仰角)</li>
</ol>
<h3 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h3><p>F =  getframe( )</p>
<p>movie(F, num_)</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayi8991.github.io/2022/02/28/docker%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiayi Liang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiayiSpace">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/28/docker%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">Docker学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-28 20:12:35" itemprop="dateCreated datePublished" datetime="2022-02-28T20:12:35+08:00">2022-02-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-03-28 15:05:51" itemprop="dateModified" datetime="2022-03-28T15:05:51+08:00">2022-03-28</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Docker教程：<a target="_blank" rel="noopener" href="https://docker-curriculum.com/">https://docker-curriculum.com/</a></p>
<p>K8S教程：<a target="_blank" rel="noopener" href="https://k8s.easydoc.net/docs/dRiQjyTY/28366845/6GiNOzyZ/9EX8Cp45">https://k8s.easydoc.net/docs/dRiQjyTY/28366845/6GiNOzyZ/9EX8Cp45</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayi8991.github.io/2022/01/22/%E8%84%89%E5%86%B2%E6%98%9F%E8%A1%A5%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiayi Liang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiayiSpace">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/22/%E8%84%89%E5%86%B2%E6%98%9F%E8%A1%A5%E4%B9%A0/" class="post-title-link" itemprop="url">脉冲星补习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-01-22 14:13:52 / 修改时间：14:15:46" itemprop="dateCreated datePublished" datetime="2022-01-22T14:13:52+08:00">2022-01-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Pulsar"><a href="#Pulsar" class="headerlink" title="Pulsar"></a>Pulsar</h1><h2 id><a href="#" class="headerlink" title></a></h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayi8991.github.io/2021/12/22/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiayi Liang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiayiSpace">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/22/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" class="post-title-link" itemprop="url">数据处理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-22 13:54:16 / 修改时间：20:42:15" itemprop="dateCreated datePublished" datetime="2021-12-22T13:54:16+08:00">2021-12-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="/2021/12/22/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/2021-12-2214-00-09.png"> </p>
<blockquote>
<p>数据处理的主要任务：</p>
<ol>
<li><p>Data cleaning</p>
<pre><code>Handle missing data, smooth noisy data, identify or remove outliers, and resolve inconsistencies
</code></pre>
</li>
<li><p>Data integration</p>
<p>​    Integration of multiple databases, data cubes, or files</p>
</li>
<li><p>Data reduction</p>
<p>​    Dimensionality reduction<br>​    Numerosity reduction<br>​    Data compression</p>
</li>
<li><p>Data transformation and data discretization</p>
<p>​    Normalization</p>
<p>​    Concept hierarchy generation</p>
</li>
</ol>
</blockquote>
<h1 id="Data-Cleaning"><a href="#Data-Cleaning" class="headerlink" title="Data Cleaning"></a>Data Cleaning</h1><ul>
<li> 生活中的数据通常“Dirty”，比如格式的错误，人和电脑的错误或是什么</li>
<li>Incomplete: lacking attribute values, lacking certain attributes of interest, or containing only aggregate data<ul>
<li>e.g., <em>Occupation</em> = “ ” (missing data)</li>
</ul>
</li>
<li>Noisy: containing noise, errors, or outliers<ul>
<li>e.g., <em>Salary</em> = “−10” (an error)</li>
</ul>
</li>
<li>Inconsistent: containing discrepancies in codes or names, <ul>
<li>e.g.,</li>
<li><em>Age</em> = “42”, <em>Birthday</em> = “03/07/2010”</li>
<li>Was rating “1, 2, 3”, now rating “A, B, C”</li>
<li>discrepancy between duplicate records</li>
</ul>
</li>
<li>Intentional (e.g., <em>disguised missing</em> data)<ul>
<li>Jan. 1 as everyone’s birthday?用户故意的输入不正确值（比如选用默认的生日）</li>
</ul>
</li>
</ul>
<h2 id="不完整数据（Incomplete-Data）"><a href="#不完整数据（Incomplete-Data）" class="headerlink" title="不完整数据（Incomplete Data）"></a>不完整数据（Incomplete Data）</h2><ul>
<li>Data is not always available<ul>
<li>E.g., many tuples have no recorded value for several attributes, such as customer income in sales data</li>
</ul>
</li>
<li>Missing data may be due to <ul>
<li>Equipment malfunction</li>
<li>Inconsistent with other recorded data and thus deleted</li>
<li>Data were not entered due to misunderstanding</li>
<li>Certain data may not be considered important at the time of entry</li>
<li>Did not register history or changes of the data</li>
</ul>
</li>
<li>Missing data may need to be inferred</li>
</ul>
<blockquote>
<p>How to deal with them ? 怎样处理缺失数据</p>
</blockquote>
<ul>
<li>Ignore the tuple: usually done when class label is missing (when doing classification)—not effective when the % of missing values per attribute varies considerably</li>
<li>Fill in the missing value manually: tedious + infeasible?</li>
<li>Fill in it automatically with<ul>
<li>a global constant : e.g., “unknown”, a new class?! </li>
<li>the attribute mean</li>
<li>the attribute mean for all samples belonging to the same class: smarter</li>
<li><strong>the most probable value: inference-based such as Bayesian formula or decision tree</strong>最可能值:基于推理的，如贝叶斯公式或决策树</li>
</ul>
</li>
</ul>
<h2 id="Noisy-Data"><a href="#Noisy-Data" class="headerlink" title="Noisy Data"></a>Noisy Data</h2><ul>
<li><strong>Noise:</strong> random error or variance in a measured variable (被测变量的随机误差或方差)</li>
<li><strong>Incorrect attribute values</strong> may be due to<ul>
<li>Faulty data collection instruments</li>
<li>Data entry problems</li>
<li>Data transmission problems</li>
<li>Technology limitation</li>
<li>Inconsistency in naming convention </li>
</ul>
</li>
<li><strong>Other data</strong> <strong>problems</strong><ul>
<li>Duplicate records</li>
<li>Incomplete data</li>
<li>Inconsistent data</li>
</ul>
</li>
</ul>
<blockquote>
<p>How to Handle Noisy Data? </p>
</blockquote>
<ul>
<li>Binning 分箱：按大小分箱，以各箱均值或中位值或边界值等代替箱内数据<ul>
<li>First sort data and partition into (equal-frequency) bins</li>
<li>Then one can <strong>smooth by bin means, smooth by bin median, smooth by bin boundaries</strong>, etc.</li>
</ul>
</li>
<li>Regression 回归<ul>
<li>Smooth by fitting the data into regression functions</li>
</ul>
</li>
<li>Clustering 聚类<ul>
<li>Detect and remove outliers</li>
</ul>
</li>
<li>Semi-supervised: Combined computer and human inspection  半监督<ul>
<li>Detect suspicious values and check by human (e.g., deal with possible outliers)</li>
</ul>
</li>
</ul>
<h2 id="数据清洗的过程"><a href="#数据清洗的过程" class="headerlink" title="数据清洗的过程"></a>数据清洗的过程</h2><ul>
<li><strong>Data discrepancy</strong> <strong>detection</strong> <strong>数据偏差检测</strong><ul>
<li>Use metadata (e.g., domain, range, dependency, distribution)</li>
<li>Check field overloading </li>
<li>Check uniqueness rule, consecutive rule and null rule</li>
<li>Use commercial tools<ul>
<li>Data scrubbing: use simple domain knowledge (e.g., postal code, spell-check) to detect errors and make corrections  数据清洗</li>
<li>Data auditing: by analyzing data to discover rules and relationship to detect violators (e.g., correlation and clustering to find outliers) 数据审计</li>
</ul>
</li>
</ul>
</li>
<li><strong>Data migration and</strong> <strong>integration</strong> <strong>数据迁移和集成</strong><ul>
<li>Data migration tools: allow transformations to be specified</li>
<li>ETL (Extraction/Transformation/Loading) tools: allow users to specify transformations through a graphical user interface</li>
</ul>
</li>
<li>Integration of the two processes （合并数据偏差和数据变换）<ul>
<li>Iterative and interactive (e.g., Potter’s Wheels)</li>
</ul>
</li>
</ul>
<h1 id="Data-Integration"><a href="#Data-Integration" class="headerlink" title="Data Integration"></a>Data Integration</h1><ul>
<li>Data integration 数据集成<ul>
<li>Combining data from multiple sources into a coherent store（将来自多个来源的数据合并到一个连贯的存储中）</li>
</ul>
</li>
<li>Schema integration: e.g., A.cust-id  B.cust-# 模式集成<ul>
<li>Integrate metadata from different sources</li>
</ul>
</li>
<li><strong>Entity</strong> <strong>identification:</strong>  <strong>实体识别</strong><ul>
<li>Identify real world entities from multiple data sources, e.g., Bill Clinton = William Clinton</li>
</ul>
</li>
<li>Detecting and resolving data value conflicts 检测和解决数值冲突<ul>
<li>For the same real world entity, attribute values from different sources are different</li>
<li>Possible reasons: different representations, different scales, e.g., metric vs. British units</li>
</ul>
</li>
</ul>
<h2 id="Handling-Redundancy-in-Integration"><a href="#Handling-Redundancy-in-Integration" class="headerlink" title="Handling Redundancy in Integration"></a>Handling Redundancy in Integration</h2><ul>
<li>Redundant data occur often when integration of multiple databases<ul>
<li><em>Object identification</em>: The same attribute or object may have different names in different databases<ul>
<li>对象标识:相同的属性或对象在不同的数据库中可能有不同的名称</li>
</ul>
</li>
<li><em>Derivable data:</em> One attribute may be a “derived” attribute in another table, e.g., annual revenue<ul>
<li>派生数据:一个属性可能是另一个表中的派生属性，例如年收入</li>
</ul>
</li>
</ul>
</li>
<li><strong>Redundant attributes may be able to be detected by</strong> <em><strong>correlation analysis</strong></em> <strong>and</strong> <em><strong>covariance analysis</strong></em>（通过相关分析和协方差分析可以发现冗余属性）</li>
<li>Careful integration of the data from multiple sources may help reduce/avoid redundancies and inconsistencies and improve mining speed and quality</li>
</ul>
<h2 id="相关性分析Correlation-Analysis-for-Categorical-Data"><a href="#相关性分析Correlation-Analysis-for-Categorical-Data" class="headerlink" title="相关性分析Correlation Analysis (for Categorical Data)"></a>相关性分析Correlation Analysis (for Categorical Data)</h2><ul>
<li>Null hypothesis: The two distributions are independent<ul>
<li>零假设:两个分布是独立的</li>
</ul>
</li>
<li>The cells that contribute the most to the Χ<sup>2</sup> value are those whose actual count is very different from the expected count<ul>
<li>The larger the Χ<sup>2</sup> value, the more likely the variables are related</li>
</ul>
</li>
<li>Note: Correlation does not imply causality 注:相关性并不意味着因果关系<ul>
<li># of hospitals and # of car-theft in a city are correlated</li>
<li>Both are causally linked to the third variable: population</li>
</ul>
</li>
</ul>
<h1 id="Data-Reduction-and-Transformation"><a href="#Data-Reduction-and-Transformation" class="headerlink" title="Data Reduction and Transformation"></a>Data Reduction and Transformation</h1><h2 id="Data-Reduction"><a href="#Data-Reduction" class="headerlink" title="Data Reduction"></a>Data Reduction</h2><ul>
<li><strong>Data reduction</strong>: <ul>
<li>Obtain a reduced representation of the data set <ul>
<li>much smaller in volume but yet produces <em>almost</em> the same analytical results</li>
</ul>
</li>
</ul>
</li>
<li>Why data reduction?—A database/data warehouse may store terabytes of data<ul>
<li>Complex analysis may take a very long time to run on the complete data set</li>
</ul>
</li>
<li><strong>Methods for data</strong> <strong>reduction</strong> (also <em>data size reduction</em> or <em>numerosity</em> <em>reduction</em>) <ul>
<li>Regression and Log-Linear Models</li>
<li>Histograms, clustering, sampling</li>
<li>Data cube aggregation</li>
<li>Data compression</li>
</ul>
</li>
</ul>
<blockquote>
<p>Data Reduction: <strong>Parametric vs. Non-Parametric Methods</strong></p>
</blockquote>
<ul>
<li>Reduce data volume by choosing alternative, <em>smaller forms</em> of data representation</li>
<li><strong>Parametric methods</strong> (e.g., regression)<ul>
<li>Assume the data fits some model, estimate model parameters, store only the parameters, and discard the data (except possible outliers)假设数据符合某个模型，估计模型参数，只存储参数，并丢弃数据(可能的离群值除外)</li>
<li>Ex.: Log-linear models—obtain value at a point in <em>m</em>-D space as the product on appropriate marginal subspaces 例如:对数线性模型获得m-D空间中某一点的值作为相应边际子空间上的乘积</li>
</ul>
</li>
<li><strong>Non-parametric</strong> methods <ul>
<li>Do not assume models</li>
<li>Major families: histograms, clustering, sampling, … </li>
</ul>
</li>
</ul>
<h3 id="参数化方法"><a href="#参数化方法" class="headerlink" title="参数化方法"></a>参数化方法</h3><h3 id="非参数化方法"><a href="#非参数化方法" class="headerlink" title="非参数化方法"></a>非参数化方法</h3><h2 id="Data-Transformation"><a href="#Data-Transformation" class="headerlink" title="Data Transformation"></a>Data Transformation</h2><ul>
<li>A function that maps the entire set of values of a given attribute to a new set of replacement values s.t. each old value can be identified with one of the new values  将给定属性的整个值集映射到一组新的替换值s.t的函数。每个旧值可以用一个新值来标识</li>
<li>Methods<ul>
<li>Smoothing: Remove noise from data</li>
<li>Attribute/feature construction<ul>
<li>New attributes constructed from the given ones</li>
</ul>
</li>
</ul>
</li>
<li>Aggregation: Summarization, data cube construction</li>
<li>Normalization: Scaled to fall within a smaller, specified range<ul>
<li>min-max normalization</li>
<li>z-score normalization</li>
<li>normalization by decimal scaling</li>
</ul>
</li>
<li>Discretization: Concept hierarchy climbing</li>
</ul>
<h3 id="Discretization（离散化）"><a href="#Discretization（离散化）" class="headerlink" title="Discretization（离散化）"></a>Discretization（离散化）</h3><ul>
<li>Three types of attributes<ul>
<li>Nominal—values from an unordered set, e.g., color, profession</li>
<li>Ordinal—values from an ordered set, e.g., military or academic rank </li>
<li>Numeric—real numbers, e.g., integer or real numbers</li>
</ul>
</li>
<li>Discretization: Divide the range of a continuous attribute into intervals<ul>
<li>Interval labels can then be used to replace actual data values </li>
<li>Reduce data size by discretization</li>
<li>Supervised vs. unsupervised</li>
<li>Split (top-down) vs. merge (bottom-up)</li>
<li>Discretization can be performed recursively on an attribute</li>
<li>Prepare for further analysis, e.g., classification</li>
</ul>
</li>
</ul>
<blockquote>
<p>离散化的方式</p>
</blockquote>
<ul>
<li>Binning <ul>
<li>Top-down split, unsupervised</li>
</ul>
</li>
<li>Histogram analysis<ul>
<li>Top-down split, unsupervised</li>
</ul>
</li>
<li>Clustering analysis <ul>
<li>Unsupervised, top-down split or bottom-up merge</li>
</ul>
</li>
<li>Decision-tree analysis<ul>
<li>Supervised, top-down split</li>
</ul>
</li>
<li>Correlation (e.g., 2) analysis <ul>
<li>Unsupervised, bottom-up merge</li>
</ul>
</li>
<li>Note: All the methods can be applied recursively</li>
</ul>
<h3 id="Concept-Hierarchy-Generation-概念分层"><a href="#Concept-Hierarchy-Generation-概念分层" class="headerlink" title="Concept Hierarchy Generation  概念分层"></a>Concept Hierarchy Generation  概念分层</h3><ul>
<li><strong>Concept hierarchy</strong> organizes concepts (i.e., attribute values) hierarchically and is usually associated with each dimension in a data warehouse概念层次按层次组织概念(即属性值)，通常与数据仓库中的每个维度相关联</li>
<li>Concept hierarchies facilitate drilling and rolling in data warehouses to view data in multiple granularity 概念层次结构便于在数据仓库中钻取和滚动，以便以多种粒度查看数据</li>
<li>Concept hierarchy formation: Recursively reduce the data by collecting and replacing low level concepts (such as numeric values for <em>age</em>) by higher level concepts (such as <em>youth, adult</em>, or <em>senior</em>) 概念层次结构的形成:通过收集和替换低级概念(如年龄的数值)，递归地减少数据(如青年、成人或老年人)</li>
<li>Concept hierarchies can be explicitly specified by domain experts and/or data warehouse designers  概念层次结构可以由领域专家和/或数据仓库设计者显式指定</li>
<li>Concept hierarchy can be automatically formed for both numeric and nominal data—For numeric data, use discretization methods shown  对于数值数据，可以使用所示的离散化方法</li>
</ul>
<h1 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h1><ul>
<li><strong>Curse of dimensionality</strong><ul>
<li>When dimensionality increases, data becomes increasingly sparse</li>
<li>Density and distance between points, which is critical to clustering, outlier analysis, becomes less meaningful</li>
<li>The possible combinations of subspaces will grow exponentially</li>
</ul>
</li>
<li><strong>Dimensionality reduction</strong><ul>
<li>Reducing the number of random variables under consideration, via obtaining a set of principal variables 通过获取一组主变量，减少考虑的随机变量的数量</li>
</ul>
</li>
<li><strong>Advantages of dimensionality reduction</strong><ul>
<li>Avoid the curse of dimensionality</li>
<li>Help eliminate irrelevant features and reduce noise</li>
<li>Reduce time and space required in data mining</li>
<li>Allow easier visualization</li>
</ul>
</li>
</ul>
<blockquote>
<p>Dimensionality Reduction Techniques</p>
</blockquote>
<ul>
<li>Dimensionality reduction methodologies<ul>
<li><strong>Feature selection</strong>: Find a subset of the original variables (or features, attributes)</li>
<li><strong>Feature extraction</strong>: Transform the data in the high-dimensional space to a space of fewer dimensions</li>
</ul>
</li>
<li>Some typical dimensionality methods<ul>
<li>Principal Component Analysis</li>
<li>Supervised and nonlinear techniques <ul>
<li>Feature subset selection</li>
<li>Feature creation</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Principal-Component-Analysis-PCA"><a href="#Principal-Component-Analysis-PCA" class="headerlink" title="Principal Component Analysis (PCA)"></a>Principal Component Analysis (PCA)</h2><ul>
<li>PCA: A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called <em><strong>principal</strong></em> <em><strong>components</strong></em><ul>
<li>主成分分析(PCA):一种统计方法，它利用正交变换将一组可能相关变量的观测值转换为一组线性不相关变量的值，称为主成分</li>
</ul>
</li>
<li>The original data are projected onto a much smaller space, resulting in dimensionality reduction将原始数据投影到一个小得多的空间，从而进行降维</li>
<li>Method: Find the eigenvectors of the covariance matrix, and these eigenvectors define the new space  方法:求协方差矩阵的特征向量，用这些特征向量定义新空间</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayi8991.github.io/2021/12/21/%E5%9F%BA%E4%BA%8E%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E7%9A%84%E9%9A%8F%E6%9C%BA%E4%BC%98%E5%8C%96%E6%90%9C%E7%B4%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiayi Liang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiayiSpace">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/21/%E5%9F%BA%E4%BA%8E%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E7%9A%84%E9%9A%8F%E6%9C%BA%E4%BC%98%E5%8C%96%E6%90%9C%E7%B4%A2/" class="post-title-link" itemprop="url">基于遗传算法的随机优化搜索</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-21 22:25:00" itemprop="dateCreated datePublished" datetime="2021-12-21T22:25:00+08:00">2021-12-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-04-18 17:48:41" itemprop="dateModified" datetime="2022-04-18T17:48:41+08:00">2022-04-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><h1 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h1><h2 id="个体与种群"><a href="#个体与种群" class="headerlink" title="个体与种群"></a>个体与种群</h2><ul>
<li>个体(individual/candidate) 就是模拟生物个体而对问题中的对象（一般就是问题的解）的一种称呼，一个个体也就是搜索空间中的一个点</li>
<li>种群(population) 就是模拟生物种群而由若干个体组成的群体, 它一般是整个搜索空间的一个很小的子集。<ul>
<li>生活例子：杂交水稻。</li>
</ul>
</li>
</ul>
<h2 id="适应度和适应度函数"><a href="#适应度和适应度函数" class="headerlink" title="适应度和适应度函数"></a>适应度和适应度函数</h2><ul>
<li>   适应度(fitness)就是借鉴生物个体对环境的适应程度,而对问题中的个体对象所设计的表征其优劣的一种测度。</li>
<li>   适应度函数(fitness function)就是问题中的 全体个体与其适应度之间的一个对应关系。它一般是一个实值函数。该函数就是遗传算法中<strong>指导搜索的评价函数（函数值越大越好）</strong>。 </li>
</ul>
<h2 id="染色体和基因"><a href="#染色体和基因" class="headerlink" title="染色体和基因"></a>染色体和基因</h2><ul>
<li><p>染色体（chromosome）就是问题中个体的某种字符串形式的编码表示。字符串中的字符也就称为基因（gene）。</p>
</li>
<li><p>  例如：</p>
</li>
</ul>
<p>  ​      个体     染色体</p>
<p>  ​       9  —-  1001</p>
<p>  ​      （2，5，6）—- 010 101 110</p>
<h2 id="遗传操作"><a href="#遗传操作" class="headerlink" title="遗传操作"></a>遗传操作</h2><ul>
<li>亦称遗传算子(genetic operator)，就是关于染色体的运算。遗传算法中有三种遗传操作: </li>
<li>   选择-复制(selection-reproduction)</li>
<li>   交叉(crossover，亦称交换、交配或杂交)</li>
<li>   变异(mutation，亦称突变)</li>
</ul>
<h3 id="选择-复制"><a href="#选择-复制" class="headerlink" title="选择-复制"></a>选择-复制</h3><ul>
<li>通常做法是：对于一个规模为<em>N</em>的种群<em>S</em>,按每个染色体x<sub>i</sub>∈<em>S</em>的选择概率<em>P</em>(x<sub>i</sub>)所决定的选中机会, 分<em>N</em>次从<em>S</em>中随机选定<em>N</em>个染色体, 并进行复制。    </li>
</ul>
<p><img src="/2021/12/21/%E5%9F%BA%E4%BA%8E%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E7%9A%84%E9%9A%8F%E6%9C%BA%E4%BC%98%E5%8C%96%E6%90%9C%E7%B4%A2/2021-12-2717-14-14.png"> </p>
<h3 id="交叉"><a href="#交叉" class="headerlink" title="交叉"></a>交叉</h3><ul>
<li><p> 交叉 就是互换两个染色体某些位上的基因。</p>
</li>
<li><p>​    例如, 设染色体 <em>s</em>1=01001011, <em>s</em>2=10010101, 交换其后4位基因, 即</p>
<ul>
<li><img src="/2021/12/21/%E5%9F%BA%E4%BA%8E%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E7%9A%84%E9%9A%8F%E6%9C%BA%E4%BC%98%E5%8C%96%E6%90%9C%E7%B4%A2/2021-12-2717-10-45.png"></li>
<li><em>s</em>1′=01000101,  <em>s</em>2′=10011011</li>
<li>可以看做是原染色体<em>s</em>1和<em>s</em>2的子代染色体</li>
</ul>
</li>
</ul>
<h3 id="变异"><a href="#变异" class="headerlink" title="变异"></a>变异</h3><ul>
<li><p><strong>变异</strong> 就是改变染色体某个(些)位上的基因。</p>
</li>
<li><p>​    例如,  设染色体 <em>s</em>=11001101将其第三位上的0变为1, 即</p>
<p>​       <em>s</em>=11001101 <strong>→</strong>11101101= <em>s</em>′</p>
</li>
<li><p>​    <em>s</em>′也可以看做是原染色体<em>s</em>的子代染色体</p>
</li>
</ul>
<h1 id="基本遗传算法"><a href="#基本遗传算法" class="headerlink" title="基本遗传算法"></a>基本遗传算法</h1><p><img src="/2021/12/21/%E5%9F%BA%E4%BA%8E%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E7%9A%84%E9%9A%8F%E6%9C%BA%E4%BC%98%E5%8C%96%E6%90%9C%E7%B4%A2/2021-12-2717-15-13.png"> </p>
<ul>
<li><p>算法中的一些控制参数：</p>
<ul>
<li>  <strong>种群规模</strong>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </li>
<li>  <strong>最大换代数</strong></li>
<li>  <strong>交叉率</strong>(crossover rate)就是参加交叉运算的染色体个数占全体染色体总数的比例，记为<em>P</em>c,取值范围一般为0.4～0.99</li>
<li>  <strong>变异率</strong>(mutation rate)是指发生变异的基因位数所占全体染色体的基因总位数的比例，记为<em>P</em>m，取值范围一般为0.0001～0.1</li>
</ul>
</li>
<li><blockquote>
<p>基本遗传算法</p>
</blockquote>
</li>
<li><p> 步1 在搜索空间<em>U</em>上定义一个适应度函数<em>f</em>(<em>x</em>)，给定种群规模<em>N</em>，交叉率<em>P</em>c和变异率<em>P</em>m，代数<em>T</em>；</p>
</li>
<li><p> 步2 随机产生<em>U</em>中的<em>N</em>个个体<em>s</em>1, <em>s</em>2, …, <em>s</em>N，组成初始种群<em>S</em>={<em>s</em>1, <em>s</em>2, …, <em>s</em>N}，置代数计数器<em>t</em>=1；</p>
</li>
<li><p> 步3 计算<em>S</em>中每个个体的适应度f() ；</p>
</li>
<li><p> 步4 若终止条件满足，则取<em>S</em>中适应度最大的个体作为所求结果，算法结束。</p>
</li>
<li><p> 步5 按选择概率<em>P</em>(<em>x**i</em>)所决定的选中机会，每次从<em>S</em>中随机选定1个个体并将其染色体复制，共做<em>N</em>次，然后将复制所得的<em>N</em>个染色体组成群体<em>S</em>1；</p>
</li>
<li><p> 步6 按交叉率<em>P</em>c所决定的参加交叉的染色体数<em>c</em>，从<em>S</em>1中随机确定<em>c</em>个染色体，配对进行交叉操作，并用产生的新染色体代替原染色体，得群体<em>S</em>2；</p>
</li>
<li><p> 步7 按变异率<em>P</em>m所决定的变异次数<em>m</em>，从<em>S</em>2中随机确定<em>m</em>个染色体，分别进行变异操作，并用产生的新染色体代替原染色体，得群体<em>S</em>3；</p>
</li>
<li><p>步8 将群体<em>S</em>3作为新一代种群，即用<em>S</em>3代替<em>S</em>，<em>t</em> = <em>t</em>+1，转步3；</p>
</li>
</ul>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>如：利用遗传算法求解区间［0,31］上的二次函数<em>y</em>=x<sup>2</sup>的最大值。</p>
<p>分析：</p>
<ul>
<li>原问题可转化为在区间［0, 31］中搜索能使y取最大值的点<em>a</em>的问题。那么，［0, 31］ 中的点<em>x</em>就是个体, 函数值<em>f</em>(<em>x</em>)恰好就可以作为<em>x</em>的适应度，区间［0, 31］就是一个(解)空间 。这样, 只要能给出个体<em>x</em>的适当染色体编码, 该问题就可以用遗传算法来解决。</li>
</ul>
<p>解：</p>
<ul>
<li><p>(1) 设定种群规模,编码染色体，产生初始种群。</p>
</li>
<li><p>  将种群规模设定为4；用5位二进制数编码染色体；取下列个体组成初始种群<em>S</em>1:</p>
</li>
<li><p>​           <em>s</em>1= 13 (01101), <em>s</em>2= 24 (11000)</p>
</li>
<li><p>​           <em>s</em>3= 8 (01000),  <em>s</em>4= 19 (10011) </p>
</li>
<li><p>(2) 定义适应度函数,</p>
</li>
<li><p>​        取适应度函数：<em>f</em> (<em>x</em>)=<em>x</em><sup>2</sup></p>
</li>
<li><p>(3) 计算各代种群中的各个体的适应度, 并对其染色体进行遗传操作,直到适应度最高的个体(即31（11111）)出现为止。</p>
<p>首先计算种群<em>S</em>1中各个体</p>
<p>​        <em>s</em>1= 13(01101),  <em>s</em>2= 24(11000)           </p>
<p>​        <em>s</em>3= 8(01000),   <em>s</em>4= 19(10011)</p>
<p>的适应度<em>f</em> (<em>s<sub>i</sub></em>) 。</p>
<p>   容易求得</p>
<p>​           <em>f</em> (<em>s</em>1) = <em>f</em>(13) = 132 = 169</p>
<p>​            <em>f</em> (<em>s</em>2) = <em>f</em>(24) = 242 = 576</p>
<p>​            <em>f</em> (<em>s</em>3) = <em>f</em>(8) = 82 = 64</p>
<p>​            <em>f</em> (<em>s</em>4) = <em>f</em>(19) = 192 = 361</p>
<ul>
<li><p>再计算种群<em>S</em>1中各个体的选择概率</p>
<ul>
<li><p>选择概率的计算公式为<br>$$<br>P（xi）= f(xi)/ f（x）的和<br>$$</p>
<ul>
<li>由此可求得</li>
<li>​            <em>P</em>(<em>s</em>1) = <em>P</em>(13) = 0.14</li>
<li>​            <em>P</em>(<em>s</em>2) = <em>P</em>(24) = 0.49 </li>
<li>​            <em>P</em>(<em>s</em>3) = <em>P</em>(8) = 0.06</li>
<li>​            <em>P</em>(<em>s</em>4) = <em>P</em>(19) = 0.31</li>
</ul>
</li>
</ul>
</li>
<li><p><img src="/2021/12/21/%E5%9F%BA%E4%BA%8E%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E7%9A%84%E9%9A%8F%E6%9C%BA%E4%BC%98%E5%8C%96%E6%90%9C%E7%B4%A2/2021-12-2719-07-55.png"> </p>
</li>
<li><p>在算法中赌轮选择法可用下面的子过程来模拟: ① 在［0, 1］区间内产生一个均匀分布的随机数<em>r</em>。</p>
</li>
<li><p> ② 若<em>r</em>≤<em>q</em>1,则染色体<em>x</em>1被选中。</p>
</li>
<li><p>③ 若<em>q<sub>k-1</sub></em>&lt; r <em>≤*q<sub>k</sub> (2≤</em>k<em>≤</em>N*), 则染色体<em>x<sub>k</sub>被选中。 其中的</em>q<sub>i</sub>称为染色体<em>x<sub>i</sub> (<em>i</em>=1, 2, …, <em>n</em>)的*<em>积累概率</em></em>, 其计算公式为<br>$$<br>q i = P(x)到i的和<br>$$</p>
</li>
</ul>
<blockquote>
<p>第一轮</p>
</blockquote>
<p>  设从区间［0, 1］中产生4个随机数如下: </p>
<p>  ​        <em>r</em>1 = 0.450126,  <em>r</em>2 = 0.110347 </p>
<p>  ​        <em>r</em>3 = 0.572496,  <em>r</em>4 = 0.98503</p>
<table>
<thead>
<tr>
<th>染色体</th>
<th>适应度</th>
<th>选择概率</th>
<th>积累概率</th>
<th>选中次数</th>
</tr>
</thead>
<tbody><tr>
<td><em>s</em>1=01101(13)</td>
<td>169</td>
<td>0.14</td>
<td>0.14</td>
<td>1</td>
</tr>
<tr>
<td><em>s</em>2=11000(24)</td>
<td>576</td>
<td>0.49</td>
<td>0.63</td>
<td>2</td>
</tr>
<tr>
<td><em>s</em>3=01000(8)</td>
<td>64</td>
<td>0.06</td>
<td>0.69</td>
<td>0</td>
</tr>
<tr>
<td><em>s</em>4=10011(19)</td>
<td>361</td>
<td>0.31</td>
<td>1.00</td>
<td>1</td>
</tr>
</tbody></table>
<blockquote>
<ul>
<li>适应度函数 y = x^2，选择概率：赌轮选择法</li>
<li>积累概率：积累选择概率，根据积累概率区间选择染色体</li>
</ul>
</blockquote>
<ul>
<li>于是，经复制得群体：</li>
<li><em>s</em>1<em>’</em> =11000（24）, <em>s</em>2<em>’</em> =01101（13） </li>
<li><em>s</em>3<em>’</em> =11000（24）, <em>s</em>4<em>’</em> =10011（19）</li>
</ul>
</li>
<li><p>交叉</p>
</li>
<li><p> 设交叉率<em>p**c</em>=100%，即<em>S</em>1中的全体染色体都参加交叉运算。</p>
</li>
<li><p>s1’ =11000（24）, s2’ =01101（13） </p>
</li>
<li><p>s3’ =11000（24）, s4’ =10011（19）</p>
</li>
<li><p>​    设<em>s</em>1<em>’</em>与<em>s</em>2<em>’</em>配对，<em>s</em>3<em>’</em>与<em>s</em>4<em>’</em>配对。分别交换后两位基因，得新染色体：</p>
</li>
<li><p>  <em>s</em>1<em>’’</em>=11001（25）, <em>s</em>2<em>’’</em>=01100（12）</p>
</li>
<li><p><em>s</em>3<em>’’</em>=11011（27）, <em>s</em>4<em>’’</em>=10000（16）</p>
</li>
<li><p>变异</p>
</li>
<li><p>​    设变异率<em>p**m</em>=0.001。</p>
</li>
<li><p>​    这样，群体<em>S</em>1中共有</p>
</li>
<li><p>​                5×4×0.001=0.02    位基因可以变异。</p>
</li>
<li><p>​     0.02位显然不足1位，所以本轮遗传操作不做变异。</p>
<blockquote>
<p>第二轮</p>
</blockquote>
</li>
<li><p>于是，得到第二代种群<em>S</em>2：</p>
</li>
<li><p>​    <em>s</em>1=11001（25）, <em>s</em>2=01100（12）</p>
</li>
<li><p>​     <em>s</em>3=11011（27）, <em>s</em>4=10000（16）</p>
</li>
<li><p>如果从区间［0, 1］中产生4个随机数如下: </p>
</li>
<li><p>​        <em>r</em>1 = 0.450126,  <em>r</em>2 = 0.110347 </p>
</li>
<li><p>​        <em>r</em>3 = 0.572496,  <em>r</em>4 = 0.98503 </p>
</li>
<li><table>
<thead>
<tr>
<th>染色体</th>
<th>适应度</th>
<th>选择概率</th>
<th>积累概率</th>
<th>选中次数</th>
</tr>
</thead>
<tbody><tr>
<td><em>s</em>1=11001(25)</td>
<td>625</td>
<td>0.36</td>
<td>0.36</td>
<td>1</td>
</tr>
<tr>
<td><em>s</em>2=01100(12)</td>
<td>144</td>
<td>0.08</td>
<td>0.44</td>
<td>0</td>
</tr>
<tr>
<td><em>s</em>3=11011(27)</td>
<td>729</td>
<td>0.41</td>
<td>0.85</td>
<td>2</td>
</tr>
<tr>
<td><em>s</em>4=10000(16)</td>
<td>256</td>
<td>0.15</td>
<td>1.00</td>
<td>1</td>
</tr>
</tbody></table>
<ul>
<li>假设这一轮选择-复制操作中，种群<em>S</em>2中的</li>
<li><strong>4个染色体都被选中（小概率事件发生了）</strong>，则得到群体： </li>
<li> <em>s</em>1<em>’</em>=11001（25）, <em>s</em>2<em>’</em>= 01100（12）</li>
<li> <em>s</em>3<em>’</em>=11011（27）, <em>s</em>4<em>’</em>= 10000（16）</li>
<li>做交叉运算，让<em>s</em>1<em>’</em>与<em>s</em>2<em>’</em>，<em>s</em>3<em>’</em>与<em>s</em>4<em>’</em> 分别交换后三位基因，得 </li>
<li>   <em>s</em>1<em>’’</em> =11100（28）, <em>s</em>2<em>’’</em> = 01001（9）</li>
<li>   <em>s</em>3<em>’’</em> =11000（24）, <em>s</em>4<em>’’</em> = 10011（19）</li>
<li>这一轮仍然不会发生变异。</li>
</ul>
<blockquote>
<p>第三轮</p>
</blockquote>
<ul>
<li><p>于是，得第三代种群<em>S</em>3：</p>
</li>
<li><p> <em>s</em>1=11100（28）, <em>s</em>2=01001（9）</p>
</li>
<li><p> <em>s</em>3=11000（24）, <em>s</em>4=10011（19）</p>
</li>
<li><table>
<thead>
<tr>
<th>染色体</th>
<th>适应度</th>
<th>选择概率</th>
<th>积累概率</th>
<th>估计的选中次数</th>
</tr>
</thead>
<tbody><tr>
<td><em>s</em>1=11100(28)</td>
<td>784</td>
<td>0.44</td>
<td>0.44</td>
<td>2</td>
</tr>
<tr>
<td><em>s</em>2=01001(9)</td>
<td>81</td>
<td>0.04</td>
<td>0.48</td>
<td>0</td>
</tr>
<tr>
<td><em>s</em>3=11000(24)</td>
<td>576</td>
<td>0.32</td>
<td>0.80</td>
<td>1</td>
</tr>
<tr>
<td><em>s</em>4=10011(19)</td>
<td>361</td>
<td>0.20</td>
<td>1.00</td>
<td>1</td>
</tr>
</tbody></table>
<ul>
<li>设这一轮的选择-复制结果为：</li>
<li>​       <em>s</em>1<em>’</em>=11100（28）, <em>s</em>2<em>’</em>=11100（28）</li>
<li>​       <em>s</em>3<em>’</em>=11000（24）, <em>s</em>4<em>’</em>=10011（19） </li>
<li>做交叉运算，让<em>s</em>1<em>’</em>与<em>s</em>4<em>’</em>，<em>s</em>2<em>’</em>与<em>s</em>3<em>’</em> 分别交换后两位基因，得 </li>
<li><em>s</em>1<em>’’</em>=11111（31）, <em>s</em>2<em>’’</em>=11100（28）</li>
<li><em>s</em>3<em>’’</em>=11000（24）, <em>s</em>4<em>’’</em>=10000（16） </li>
</ul>
</li>
</ul>
<blockquote>
<p>第四轮</p>
</blockquote>
<ul>
<li>于是，得第四代种群<em>S</em>4： </li>
<li>​     <em>s</em>1=11111（31）, <em>s</em>2=11100（28）</li>
<li>​     <em>s</em>3=11000（24）, <em>s</em>4=10000（16） </li>
<li>显然，在这一代种群中已经出现了适应度最高的染色体<em>s</em>1=11111。于是，遗传操作终止，将染色体“11111”作为最终结果输出。</li>
<li> 然后，将染色体“11111”解码为表现型，即得所求的最优解：31。</li>
<li> 将31代入函数<em>y</em>=<em>x</em>2中，即得原问题的解，即函数<em>y</em>=<em>x</em>2的最大值为961。</li>
</ul>
</li>
</ul>
<h1 id="遗传算法的特点和优势"><a href="#遗传算法的特点和优势" class="headerlink" title="遗传算法的特点和优势"></a>遗传算法的特点和优势</h1><h2 id="遗传算法的主要特点"><a href="#遗传算法的主要特点" class="headerlink" title="遗传算法的主要特点"></a>遗传算法的主要特点</h2><ul>
<li> 遗传算法一般是直接在解空间搜索, 而不像图搜索那样一般是在问题空间搜索, 最后才找到解。 </li>
<li> 遗传算法的搜索随机地始于搜索空间的一个点集, 而不像图搜索那样固定地始于搜索空间的初始节点或终止节点, 所以遗传算法是一种随机搜索算法。</li>
<li>遗传算法总是在寻找优解, 而不像图搜索那样并非总是要求优解, 而一般是设法尽快找到解, 所以遗传算法又是一种优化搜索算法。</li>
<li> 遗传算法的搜索过程是从空间的一个点集(种群)到另一个点集(种群)的搜索,而不像图搜索那样一般是从空间的一个点到另一个点地搜索。 因而它实际是一种并行搜索, 适合大规模并行计算,而且这种种群到种群的搜索有能力跳出局部最优解。 </li>
<li>遗传算法的适应性强, 除需知适应度函数外, 几乎不需要其他的先验知识。 </li>
<li> 遗传算法长于全局搜索, 它不受搜索空间的限制性假设的约束,不要求连续性, 能以很大的概率从离散的、多极值的、 含有噪声的高维问题中找到全局最优解（但是不能保证一定找到最优解）</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayi8991.github.io/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiayi Liang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiayiSpace">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">机器学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-21 22:25:00" itemprop="dateCreated datePublished" datetime="2021-12-21T22:25:00+08:00">2021-12-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-04-18 17:49:06" itemprop="dateModified" datetime="2022-04-18T17:49:06+08:00">2022-04-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction-介绍"><a href="#Introduction-介绍" class="headerlink" title="Introduction 介绍"></a>Introduction 介绍</h1><p><strong>定义：</strong></p>
<ul>
<li><p>Arthur Samuel (1959). </p>
<ul>
<li>Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed. 使计算机无需明确编程就能学习的研究领域。</li>
</ul>
</li>
<li><p>Tom Mitchell (1998) </p>
<ul>
<li>Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. </li>
<li>一个计算机程序能够单独的从一些任务T和表现策略P中进行学习，看是否他在T或者是P中的表现随着经验E所提升</li>
</ul>
</li>
</ul>
<p><strong>机器学习算法：</strong></p>
<ul>
<li><p>Supervised learning       有监督学习</p>
</li>
<li><p>Unsupervised learning   无监督学习</p>
<p>其他的一些算法： Reinforcement learning, recommender systems</p>
</li>
</ul>
<h2 id="Supervised-learning-有监督学习"><a href="#Supervised-learning-有监督学习" class="headerlink" title="Supervised learning 有监督学习"></a>Supervised learning 有监督学习</h2><p><strong>定义：</strong></p>
<blockquote>
<p>In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.</p>
<p>译文：在有监督学习中，我们得到一个数据集，并且已经知道我们正确的输出应该是什么样子，因为我们知道输入和输出之间是有关系的。</p>
</blockquote>
<p>Supervised learning problems are categorized into “regression” and “classification” problems. </p>
<ul>
<li><p><strong>回归问题：</strong></p>
</li>
<li><p>In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function.</p>
<p>在一个回归问题中，我们试图预测连续输出的结果，这意味着我们试图将输入变量映射到某个连续函数。</p>
</li>
<li><p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1.png">  </p>
</li>
<li><p><strong>分类问题：</strong></p>
</li>
<li><p>In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. </p>
<p>在分类问题中，我们试图预测离散输出的结果。换句话说，我们试着将输入变量映射成离散的类别。</p>
</li>
<li><p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2.png">  </p>
</li>
<li><p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3.png"> </p>
</li>
<li><p>通过很多的特征来学习寻找不同的分类</p>
</li>
</ul>
<h2 id="Unsupervised-learning-无监督学习"><a href="#Unsupervised-learning-无监督学习" class="headerlink" title="Unsupervised learning 无监督学习"></a>Unsupervised learning 无监督学习</h2><p><strong>定义：</strong></p>
<blockquote>
<p>Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables.</p>
<p>无监督学习是指我们在处理问题时很少或根本不知道我们的结果应该是什么样子。我们可以从不需要知道变量影响的数据中推导出结构。</p>
</blockquote>
<ul>
<li><p>We can derive this structure by clustering the data based on relationships among the variables in the data.</p>
<p>我们可以根据数据中变量之间的关系对数据进行聚类，从而得出这种结构。</p>
</li>
<li><p>With unsupervised learning there is no feedback based on the prediction results.</p>
<p>译文：在无监督学习中，没有基于预测结果的反馈。</p>
</li>
</ul>
<p><strong>EXAMPLE：</strong></p>
<ul>
<li><p><strong>Clustering</strong>: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.</p>
<p>聚类: 将100万个不同的基因集合起来，然后找到一种方法，自动将这些基因分组，这些分组在某种程度上与不同的变量(如寿命、位置、角色等)相似或相关。</p>
</li>
<li><p><strong>Non-clustering</strong>: The “Cocktail Party Algorithm”, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cocktail_party_effect">cocktail party</a>).</p>
</li>
</ul>
<h1 id="Model-and-Cost-Function-模型和代价函数"><a href="#Model-and-Cost-Function-模型和代价函数" class="headerlink" title="Model and Cost Function 模型和代价函数"></a>Model and Cost Function 模型和代价函数</h1><h2 id="回归模型展示"><a href="#回归模型展示" class="headerlink" title="回归模型展示"></a>回归模型展示</h2><p>To establish notation for future use, we’ll use x^(i) to denote the “input” variables (living area in this example), also called input features, and y^(i)to denote the “output” or target variable that we are trying to predict (price).<br>Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. We will also use X to denote the space of input values, and Y to denote the space of output values. In this example, X = Y = ℝ. </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/4.png">  <img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/5.png" style="zoom: 33%;">   </p>
<p>输出结果是连续的，回归问题；</p>
<p>输出结果是少的并且离散的，通过特征得到的，分类问题；</p>
<p><strong>When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem.</strong> </p>
<p><strong>When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem.</strong></p>
<h2 id="Cost-Function-代价函数"><a href="#Cost-Function-代价函数" class="headerlink" title="Cost Function 代价函数"></a>Cost Function 代价函数</h2><p><strong>定义：</strong></p>
<blockquote>
<p>We can measure the accuracy of our hypothesis function by using a <strong>cost function</strong>. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x’s and the actual output y’s.</p>
<p>它取所有假设结果的平均差值(实际上是平均值的一个奇特版本)，输入是x，实际输出是y。</p>
</blockquote>
<p>EX：</p>
<p>线性回归常用的代价函数：</p>
<p>This function is otherwise called the “Squared error function”, or “Mean squared error”. </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/6.png">  </p>
<p>简化版理解，将常数看做0</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/7.png">   </p>
<p>theta 都不为零的情况下：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/8.png"></p>
<p>用螺旋线来表示这个。同一个圈圈上的线表示是一样的J（theta）的值；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/9.png"> </p>
<h1 id="parameter-learning-参数学习"><a href="#parameter-learning-参数学习" class="headerlink" title="parameter learning 参数学习"></a>parameter learning 参数学习</h1><h2 id="Gradient-Descent-梯度下降"><a href="#Gradient-Descent-梯度下降" class="headerlink" title="Gradient Descent  梯度下降"></a>Gradient Descent  梯度下降</h2><ul>
<li><strong>梯度是什么？</strong><ul>
<li><strong>在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是(∂f/∂x, ∂f/∂y)T,简称grad f(x,y)或者▽f(x,y)。对于在点(x0,y0)的具体梯度向量就是(∂f/∂x0, ∂f/∂y0)T.或者▽f(x0,y0)，如果是3个参数的向量梯度，就是(∂f/∂x, ∂f/∂y，∂f/∂z)T,以此类推。</strong></li>
</ul>
</li>
<li><strong>那么这个梯度向量求出来有什么意义呢？</strong><ul>
<li><strong>他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点(x0,y0)，沿着梯度向量的方向就是(∂f/∂x0, ∂f/∂y0)T的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 -(∂f/∂x0, ∂f/∂y0)T的方向，梯度减少最快，也就是更加容易找到函数的最小值。</strong></li>
</ul>
</li>
</ul>
<p>Imagine that we graph our hypothesis function based on its fields <em>θ</em>0 and <em>θ</em>1 (actually we are graphing the cost function as a function of the parameter estimates). We are not graphing x and y itself, but the parameter range of our hypothesis function and the cost resulting from selecting a particular set of parameters.</p>
<p>We put θ0 on the x axis and <em>θ</em>1 on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.</p>
<p>假设我们根据其场θ0和θ1绘制假设函数的图(实际上，我们将代价函数绘制为参数估计的函数)。我们画的不是x和y本身，而是假设函数的参数范围以及选择一组特定参数所产生的代价。</p>
<p>θ*0在x轴上，θ1在y轴上，代价函数在垂直的z轴上。图上的点将是代价函数的结果使用我们的假设和特定的参数。下图描述了这样的设置。</p>
<img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/10.png" style="zoom:67%;">  

<p>The way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. <strong>The size of each step is determined by the parameter α, which is called the learning rate.</strong> </p>
<p>我们的方法是对代价函数求导(一个函数的切线)切线的斜率就是这一点的导数它会给我们一个移动的方向。我们让代价函数沿着下降速度最快的方向逐步下降。<strong>每一步的大小由参数α决定，该参数称为学习率。</strong>（学习率就像下山的步子，lr越大，步子越大，否则反之）</p>
<p>For example, the distance between each ‘star’ in the graph above represents a step determined by our parameter α. A smaller α would result in a smaller step and a larger α results in a larger step. The direction in which the step is taken is determined by the partial derivative of J*(<em>θ</em>0,*θ1). Depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places. </p>
<p>例如，上图中每个“星”之间的距离代表了由参数α决定的一个步长。更小的α会导致更小的步骤，更大的α会导致更大的步骤。步进的方向由J(θ0，θ1) J(θ0，θ1)的偏导数决定。这取决于图的起始点，可能会在不同的点结束。上面的图像显示了两个不同的起点，在两个不同的地方结束。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/11.png">  </p>
<p>At each iteration j, one should simultaneously update the parametersθ<em>1,<em>θ</em>2,…,<em>θ</em>n</em>. Updating a specific parameter prior to calculating another one on the j^(th) iteration would yield to a wrong implementation. </p>
<p>在代价函数的每次迭代中，每个参数应该同步更新。</p>
<img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/12.png" style="zoom:67%;">  



<p><strong>简化版（只有一个参数改变）梯度下降算法，示例：</strong></p>
<p> <img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/13.png"> </p>
<p>不管斜率是怎样的，theta总是会趋近于最小值的地方；</p>
<p>On a side note, we should adjust our parameter \alpha<em>α</em> to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.</p>
<p>另一方面，我们应该调整我们的参数alphaα（学习速率），以确保梯度下降算法在合理的时间收敛。不能收敛或花太多时间来获得最小值意味着我们的步长是错误的。</p>
<img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/14.png" style="zoom:80%;">   

<p>当斜率降到0的时候，就到达了局部最小处；</p>
<p>并且斜率会自己变小，所以学习速率是固定的，梯度下降会随着斜率下降步子越来越小。</p>
<img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/15.png" style="zoom:80%;">  

<p><strong>梯度下降算法在线性回归中的示例：</strong></p>
<p>When specifically applied to the case of linear regression, a new form of the gradient descent equation can be derived. We can substitute our actual cost function and our actual hypothesis function and modify the equation to :</p>
<p>当具体应用于线性回归的情况下，可以导出一个新的形式的梯度下降方程。我们可以代入实际的代价函数和实际的假设函数并将方程修改为</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/16.png"> </p>
<p>The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate.</p>
<p>So, this is simply gradient descent on the original cost function J. <strong>This method looks at every example in the entire training set on every step</strong>, and is called <strong>batch gradient descent</strong>. </p>
<p>Note that, while gradient descent can be susceptible to local minims in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. Indeed, J is a convex quadratic function. Here is an example of gradient descent as it is run to minimize a quadratic function.</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/17.png">   </p>
<h1 id="Multivariate-Linear-Regression-多元线性回归"><a href="#Multivariate-Linear-Regression-多元线性回归" class="headerlink" title="Multivariate Linear Regression 多元线性回归"></a>Multivariate Linear Regression 多元线性回归</h1><h2 id="Multiple-Features-多个特征"><a href="#Multiple-Features-多个特征" class="headerlink" title="Multiple Features 多个特征"></a>Multiple Features 多个特征</h2><blockquote>
<p>Linear regression with multiple variables is also known as “multivariate linear regression”.</p>
</blockquote>
<p>We now introduce notation for equations where we can have any number of input variables.</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/18.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/19.png"> </p>
<p>在房价的例子中，x1到x4都是影响y的特征</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/20.png">  </p>
<p>可以用向量的内积来简化运算</p>
<h2 id="Gradient-Descent-For-Multiple-Variables-多元的梯度下降算法"><a href="#Gradient-Descent-For-Multiple-Variables-多元的梯度下降算法" class="headerlink" title="Gradient Descent For Multiple Variables 多元的梯度下降算法"></a>Gradient Descent For Multiple Variables 多元的梯度下降算法</h2><p>The gradient descent equation itself is generally the same form; we just have to repeat it for our ‘n’ features:</p>
<p>梯度下降方程本身通常是相同的形式;我们只需要为我们的“n”特性重复它</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21.png">  </p>
<p>The following image compares gradient descent with one variable to gradient descent with multiple variables: </p>
<p>下图比较了单变量梯度下降和多变量梯度下降</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/22.png"> </p>
<h2 id="Gradient-Descent-in-Practice-I-Feature-Scaling-特征缩放"><a href="#Gradient-Descent-in-Practice-I-Feature-Scaling-特征缩放" class="headerlink" title="Gradient Descent in Practice I - Feature Scaling 特征缩放"></a>Gradient Descent in Practice I - Feature Scaling 特征缩放</h2><ul>
<li><strong>特征缩放是什么？</strong><ul>
<li>特征缩放是用来标准化数据特征的范围。</li>
<li>将各个特征的范围缩小到相近可以加速收敛的过程，进而加速学习的速度</li>
</ul>
</li>
<li><strong>为什么要特征缩放？</strong><ul>
<li>We can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</li>
<li>我们可以通过让每个输入值在大致相同的范围内来加速梯度下降。这是因为θ在小范围会快速下降，而在大范围会缓慢下降，因此当变量非常不均匀时，θ会低效地振荡到最优。</li>
</ul>
</li>
</ul>
<p>​        <strong>因为特征的值范围特别大的话，theta的值也会变得偏大或者偏小，就会让螺旋图像下面图一的样子，又扁又长，就会经过更多次迭代才会到达代价最小的点</strong> <img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/27.png">  </p>
<p>一般情况下会将特征的范围缩至如下的情况：</p>
<p>The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:</p>
<p>​            −1 ≤ x*(<em>i) ≤ 1   or     −0.5 ≤x</em>(*i) ≤ 0.5</p>
<p><strong>These aren’t exact requirements</strong>; we are only trying to speed things up. <strong>The goal is to get all input variables into roughly one of these ranges, give or take a few.</strong></p>
<h3 id="mean-normalization-均值标准化"><a href="#mean-normalization-均值标准化" class="headerlink" title="mean normalization 均值标准化"></a>mean normalization 均值标准化</h3><p>Mean normalization involves <strong>subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.</strong> To implement both of these techniques, adjust your input values as shown in this formula:</p>
<p>均值标准化涉及到从一个输入变量的值中减去一个输入变量的平均值，从而得到一个新的输入变量的平均值为零。要实现这两种技术，请按照以下公式调整输入值</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23.png">   </p>
<h2 id="Gradient-Descent-in-Practice-II-Learning-Rate-学习速率"><a href="#Gradient-Descent-in-Practice-II-Learning-Rate-学习速率" class="headerlink" title="Gradient Descent in Practice II - Learning Rate 学习速率"></a>Gradient Descent in Practice II - Learning Rate 学习速率</h2><ul>
<li><p>如何判断梯度下降算法是否正常工作：</p>
<ul>
<li><strong>Debugging gradient descent.</strong> Make a plot with <em>number of iterations</em> on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.</li>
<li><strong>Automatic convergence test.</strong> Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10^{−3}10−3. However in practice it’s difficult to choose this threshold value.</li>
</ul>
</li>
<li><p>学习速率设置对于学习算法的影响：</p>
<ul>
<li><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/24.png"> <ul>
<li>It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration.</li>
</ul>
</li>
<li><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/25.png"> </li>
<li>SUMMARIZE：<ul>
<li><strong>If <em>α</em> is too small: slow convergence.</strong> </li>
<li><strong>If  <em>α</em> is too large: may not decrease on every iteration and thus may not converge.</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Features-and-Polynomial-Regression-特征和多项式回归"><a href="#Features-and-Polynomial-Regression-特征和多项式回归" class="headerlink" title="Features and Polynomial Regression 特征和多项式回归"></a>Features and Polynomial Regression 特征和多项式回归</h2><p>We can improve our features and the form of our hypothesis function in a couple different ways.</p>
<p><strong>我们可以通过改进特征 和 猜想的方程来优化模型，获得更好的结果。</strong></p>
<ul>
<li>We can <strong>combine</strong> multiple features into one. <ul>
<li>For example, we can combine x_1<em>x</em>1 and x_2<em>x</em>2 into a new feature x_3<em>x</em>3 by taking x_1<em>x</em>1⋅x_2<em>x</em>2.</li>
</ul>
</li>
</ul>
<h3 id="Polynomial-Regression-多项式回归"><a href="#Polynomial-Regression-多项式回归" class="headerlink" title="Polynomial Regression 多项式回归"></a><strong>Polynomial Regression</strong> 多项式回归</h3><p>Our hypothesis function need not be linear (a straight line) if that does not fit the data well.</p>
<p>We can <strong>change the behavior or curve</strong> of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).</p>
<p>我们可以通过将假设函数变成二次函数、三次函数或平方根函数(或任何其他形式)来改变它的行为或曲线。</p>
<ul>
<li><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26.png"> </li>
</ul>
<h1 id="Computing-Parameters-Analytically-计算参数分析"><a href="#Computing-Parameters-Analytically-计算参数分析" class="headerlink" title="Computing Parameters Analytically 计算参数分析"></a>Computing Parameters Analytically 计算参数分析</h1><h2 id="Normal-Equation-正规方程"><a href="#Normal-Equation-正规方程" class="headerlink" title="Normal Equation 正规方程"></a>Normal Equation 正规方程</h2><blockquote>
<p>正规方程法和梯度下降算法一样，都是求最小化的一种方法；</p>
<p>不同的地方在于，正规方程法直接通过矩阵直接将最小化的参数分析计算出来，而不用通过迭代的方法去趋近于最小值；并且不用归一化</p>
</blockquote>
<p>This allows us to find the optimum theta without iteration. The normal equation formula is given below: </p>
<p>这使得我们可以在不需要迭代的情况下找到最优的。常规方程公式如下</p>
<blockquote>
<p><em>θ</em>    = ( <em>X<sup>T</sup>X )<sup>−1</sup></em> * X<sup>T</sup> * y</p>
</blockquote>
<p> <img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/28.png"> </p>
<blockquote>
<p>There is <strong>no need</strong> to do feature scaling with the normal equation.</p>
</blockquote>
<p>The following is a comparison of gradient descent and the normal equation:</p>
<table>
<thead>
<tr>
<th align="left">Gradient Descent</th>
<th align="left">Normal Equation</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Need to choose alpha</td>
<td align="left">No need to choose alpha</td>
</tr>
<tr>
<td align="left">Needs many iterations</td>
<td align="left">No need to iterate</td>
</tr>
<tr>
<td align="left">O (kn^2)</td>
<td align="left">O (n^3), need to calculate inverse of X^TX</td>
</tr>
<tr>
<td align="left">Works well when n is large</td>
<td align="left">Slow if n is very large</td>
</tr>
</tbody></table>
<p>当特征变量不那么多的时候可以选择正规方程，因为可以更加快速的找到最优解，但是当特征变量很大的时候，比如超过了10000的时候，就可以考虑使用梯度下降，通过迭代的方法来找最优解。</p>
<h2 id="Normal-Equation-Noninvertibility-不可逆的情况"><a href="#Normal-Equation-Noninvertibility-不可逆的情况" class="headerlink" title="Normal Equation Noninvertibility  不可逆的情况"></a>Normal Equation Noninvertibility  不可逆的情况</h2><p>When implementing the normal equation in octave <strong>we want to use the ‘pinv’ function rather than ‘inv.</strong></p>
<p>‘ The ‘pinv’ function will give you a value of θ even if X<sup>T</sup>X is not invertible. </p>
<p>If X<sup>T</sup>X is <strong>noninvertible,</strong> the common causes might be having :</p>
<ul>
<li>Redundant features, where two features are very closely related (i.e. they are linearly dependent)</li>
<li>Too many features (e.g. m ≤ n). In this case, delete some features or use “regularization” (to be explained in a later lesson).</li>
</ul>
<p>Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.</p>
<h1 id="Classification-and-Representation-分类和表示"><a href="#Classification-and-Representation-分类和表示" class="headerlink" title="Classification and Representation  分类和表示"></a>Classification and Representation  分类和表示</h1><h2 id="Classification-分类"><a href="#Classification-分类" class="headerlink" title="Classification 分类"></a>Classification 分类</h2><p>To attempt classification, one method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. However, this method doesn’t work well because classification is not actually a linear function.</p>
<p>要尝试分类，一种方法是使用线性回归，将所有大于0.5的预测都映射为1，所有小于0.5的预测都映射为0。然而，这种方法并不能很好地工作，因为分类实际上不是一个线性函数。如下图所展示：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/29.png">   </p>
<p>the <strong>binary classification</strong> <strong>problem</strong> in which y can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) </p>
<p>二分类问题，其中y只能取两个值，0和1。(我们在这里所说的大部分内容也适用于多类情况。)</p>
<h2 id="Hypothesis-Representation-假设函数表达式"><a href="#Hypothesis-Representation-假设函数表达式" class="headerlink" title="Hypothesis Representation 假设函数表达式"></a>Hypothesis Representation 假设函数表达式</h2><p> Intuitively, it also doesn’t make sense for hθ*(<em>x) to take values larger than 1 or smaller than 0 when we know that y ∈ {0, 1}. To fix this, let’s change the form for our hypotheses <em>hθ</em>(<em>x</em>) to satisfy 0≤</em>hθ*(<em>x</em>)≤1. </p>
<p>直观地说，当我们知道y{0,1}时，hθ(x)取大于1或小于0的值也没有意义。为了解决这个问题，我们改变假设hθ(x)的形式来满足0 hθ(x) 1。</p>
<p>This is accomplished by plugging  θ<sup>T</sup>x into the Logistic Function.</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/30.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/31.png">  </p>
<p>The function g(z), shown here, maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.</p>
<p>函数g(z)，如图所示，将任意实数映射到(0,1)区间，这使得它可以将任意值函数转换为更适合分类的函数。</p>
<p><strong>常常用于二分类问题；</strong></p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/32.png">  </p>
<h2 id="Decision-Boundary-决策边界"><a href="#Decision-Boundary-决策边界" class="headerlink" title="Decision Boundary 决策边界"></a>Decision Boundary 决策边界</h2><p>为了在二分类的时候可以得到 0 或者是 1，我们可以将假设函数 H（x） &gt;=0.5 的 看作是 y = 1；</p>
<p>将H（x）&lt; 0.5 的 看作为 y = 0；</p>
<p>The way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5:</p>
<blockquote>
<p>g ( z ) &gt;= 0.5</p>
<p>when z &gt;= 0 </p>
</blockquote>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/33.png">  </p>
<blockquote>
<p>The <strong>decision boundary</strong> is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function.</p>
<p>决策边界是 y = 0 和 y = 1 区域的分界线。它是由假设函数创建的。</p>
</blockquote>
<p>非线性决策边界 的例子：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/34.png">  </p>
<h1 id="Logistic-Regression-Model-逻辑回归模型"><a href="#Logistic-Regression-Model-逻辑回归模型" class="headerlink" title="Logistic Regression Model 逻辑回归模型"></a>Logistic Regression Model 逻辑回归模型</h1><h2 id="Cost-Function-代价函数-1"><a href="#Cost-Function-代价函数-1" class="headerlink" title="Cost Function 代价函数"></a>Cost Function 代价函数</h2><p>We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.我们不能使用与线性回归相同的成本函数，因为Logistic函数会导致输出波动，导致许多局部最优值。换句话说，它不是一个凸函数。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/35.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/36.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/37.png"> </p>
<p>If our correct answer ‘y’ is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.</p>
<ul>
<li>如果我们的正确答案y是0，那么代价函数就是0如果我们的假设函数 输出也是0。如果我们的假设趋于1，那么代价函数将趋于无穷。</li>
</ul>
<p>If our correct answer ‘y’ is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.</p>
<ul>
<li>如果正确答案y是1，那么如果假设函数输出1代价函数就是0。如果我们的假设趋于0，那么代价函数将趋于无穷。</li>
</ul>
<p>Note that writing the cost function in this way guarantees that J(θ) is convex for logistic regression.</p>
<ul>
<li>注意，这样写代价函数保证了J(θ)在逻辑回归中是凸的。</li>
</ul>
<h2 id="Simplified-Cost-Function-and-Gradient-Descent-简化的代价函数和梯度下降"><a href="#Simplified-Cost-Function-and-Gradient-Descent-简化的代价函数和梯度下降" class="headerlink" title="Simplified Cost Function and Gradient Descent  简化的代价函数和梯度下降"></a>Simplified Cost Function and Gradient Descent  简化的代价函数和梯度下降</h2><p>We can compress our cost function’s two conditional cases into one case:</p>
<blockquote>
<p>逻辑回归简化后的代价函数：</p>
<p>Cost(<em>hθ</em>(<em>x</em>),<em>y</em>) = −<em>y</em>log(<em>hθ</em>(<em>x</em>)) − (1−<em>y</em>)log(1−<em>hθ</em>(<em>x</em>))</p>
</blockquote>
<p>Notice that when y is equal to 1, then the second term (1−<em>y</em>)log(1−<em>hθ</em>(<em>x</em>)) will be zero and will not affect the result. If y is equal to 0, then the first term −<em>y</em>log(<em>hθ</em>(<em>x</em>)) will be zero and will not affect the result.注意，当y = 1时，第二项(1 y)log(1 hθ(x))为零，不会影响结果。如果y = 0，那么第一项ylog(hθ(x))为0，不会影响结果。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/38.png"> </p>
<p>整体如下图所示：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/39.png">  </p>
<p><strong>逻辑回归中的梯度下降：</strong></p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/40.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/41.png"> </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/42.png">  </p>
<h1 id="Multiclass-Classification-多类分类"><a href="#Multiclass-Classification-多类分类" class="headerlink" title="Multiclass Classification 多类分类"></a>Multiclass Classification 多类分类</h1><blockquote>
<p>Multiclass Classification 本质上就是利用n个分类过滤器将一个类和其他样本分开，分n次；通过训练不同的过滤器后，在将x输入，看那个概率大就分到那个类</p>
</blockquote>
<p>Now we will approach the classification of data when we have more than two categories. Instead of y = {0,1} we will expand our definition so that y = {0,1…n}.</p>
<p>现在，当我们有两个以上的类别时，我们将处理数据的分类。我们将扩展定义，使y ={0,1…n}，而不是y ={0,1}。</p>
<p>Since y = {0,1…n}, we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that ‘y’ is a member of one of our classes.</p>
<p> <img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/44.png"> </p>
<p>We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.</p>
<p>我们基本上是选择一个类，然后把所有其他类合并成一个单独的第二个类。我们重复这样做，对每个案例应用二元逻辑回归，然后使用返回最高值的假设作为我们的预测。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/43.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/45.png">  </p>
<p><strong>To summarize:</strong> </p>
<p>Train a logistic regression classifier hθ*(*x) for each class i to predict the probability that y = i. </p>
<p>To make a prediction on a new x, pick the class i that maximizes   <em>hθ</em>(<em>x</em>)</p>
<h1 id="Solving-the-Problem-of-Overfitting-过拟合的问题"><a href="#Solving-the-Problem-of-Overfitting-过拟合的问题" class="headerlink" title="Solving the Problem of Overfitting 过拟合的问题"></a>Solving the Problem of Overfitting 过拟合的问题</h1><h2 id="The-Problem-of-Overfitting"><a href="#The-Problem-of-Overfitting" class="headerlink" title="The Problem of Overfitting"></a>The Problem of Overfitting</h2><blockquote>
<p>what is overfitting?</p>
</blockquote>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/46.png">  </p>
<ul>
<li>欠拟合：Underfitting, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. </li>
<li>过拟合：Overfitting, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</li>
</ul>
<p>主要的两种解决方法：</p>
<p>There are two main options to address the issue of overfitting:</p>
<ol>
<li>Reduce the number of features:</li>
</ol>
<ul>
<li>Manually select which features to keep.</li>
<li>Use a model selection algorithm (studied later in the course).</li>
</ul>
<ol start="2">
<li>Regularization 正则化</li>
</ol>
<ul>
<li>Keep all the features, but reduce the magnitude of parameters θj.</li>
<li>Regularization works well when we have a lot of slightly useful features.</li>
</ul>
<h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p><strong>If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.</strong></p>
<p><strong>如果我们的假设函数存在过拟合，我们可以通过增加函数中某些项的代价来减少它们的权重。</strong></p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/47.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/48.png"> </p>
<p>We could also regularize all of our theta parameters in a single summation as:</p>
<p>我们也可以正则化所有的参数在一个求和中，如下：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/49.png">  </p>
<p>The λ, or lambda, is the <strong>regularization parameter</strong>. It determines how much the costs of our theta parameters are inflated.</p>
<p>λ，或lambda，是正则化参数。它决定了参数膨胀的代价有多大。</p>
<p>Using the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting.</p>
<p>使用上面的代价函数和额外的求和，我们可以平滑我们的假设函数的输出，以减少过拟合。如果选择的lambda太大，可能会使函数过于平滑，导致欠拟合。</p>
<h2 id="正则化的例子"><a href="#正则化的例子" class="headerlink" title="正则化的例子"></a>正则化的例子</h2><h3 id="Regularized-Linear-Regreesion-正则化的线性回归"><a href="#Regularized-Linear-Regreesion-正则化的线性回归" class="headerlink" title="Regularized Linear Regreesion 正则化的线性回归"></a>Regularized Linear Regreesion 正则化的线性回归</h3><blockquote>
<p><strong>Gradient Descent</strong> </p>
</blockquote>
<p>We will modify our gradient descent function to separate out θ<em>0 from the rest of the parameters because we do not want to penalizeθ</em>0.我们将修改梯度下降函数，将θ0从其余参数中分离出来，因为我们不想惩罚θ0。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/50.png">  </p>
<p>经过一些操作，可以变换为如下：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/51.png">  </p>
<p><strong>The first term in the above equation, 1− α * λ/m will always be less than 1.</strong> </p>
<p>Intuitively you can see it as reducing the value of θj by some amount on every update.</p>
<p> <strong>Notice that the second term is now exactly the same as it was before.</strong></p>
<blockquote>
<p><strong>Normal Equation</strong>  在正规方程中 使用正则化</p>
</blockquote>
<p>Now let’s approach regularization using the alternate method of the non-iterative normal equation.</p>
<p>To add in regularization, the equation is the same as our original, except that we add another term inside the parentheses:加入正则化，方程和原来一样，除了我们在括号里加了另一项</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/52.png">  </p>
<p>Recall that if m &lt; n, then X<sup>T</sup>X is non-invertible. However, when we add the term λ⋅L, then X<sup>T</sup>X + λ⋅L becomes invertible.</p>
<h3 id="Regularized-Logistic-Regression-正则化的逻辑回归"><a href="#Regularized-Logistic-Regression-正则化的逻辑回归" class="headerlink" title="Regularized Logistic Regression 正则化的逻辑回归"></a>Regularized Logistic Regression 正则化的逻辑回归</h3><p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/53.png">  </p>
<p>类似的，加入正则化项后的代价函数，如上图蓝色水笔所标记；</p>
<p>与线性回归类似，梯度下降的算法是：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/54.png">    </p>
<h1 id="Ex1-线性回归-–-Ex2-逻辑回归"><a href="#Ex1-线性回归-–-Ex2-逻辑回归" class="headerlink" title="Ex1 线性回归 – Ex2 逻辑回归"></a>Ex1 线性回归 – Ex2 逻辑回归</h1><p>线性回归： <a target="_blank" rel="noopener" href="https://www.bilibili.com/read/cv1664490">https://www.bilibili.com/read/cv1664490</a></p>
<h1 id="Neural-Networks-Representation-神经网络"><a href="#Neural-Networks-Representation-神经网络" class="headerlink" title="Neural Networks: Representation 神经网络"></a>Neural Networks: Representation 神经网络</h1><p><strong>什么是神经网络？</strong></p>
<blockquote>
<p>At a very simple level, neurons are basically computational units that take inputs (<strong>dendrites</strong>) as electrical inputs (called “spikes”) that are channeled to outputs (<strong>axons</strong>).</p>
<p>在非常简单的层面上，神经元基本上是计算单位，将输入(树突)作为电输入(称为“spikes”)，然后传导到输出(轴突)。</p>
</blockquote>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/58.png">  </p>
<p>一个神经单元的逻辑模型：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/55.png">  </p>
<p>In our model, our dendrites are like the input features x<em>1⋯</em>xn, and the output is the result of our hypothesis function. </p>
<p>在我们的模型中，我们的树突就像输入特征x1…….xn，而输出是我们的假设函数的结果。</p>
<p>In this model our <em>x</em>0 input node is sometimes called the “bias unit.” It is always equal to 1. </p>
<p>在这个模型中，我们的x0输入节点有时被称为“偏差单位”。它总是等于1。</p>
<p>In neural networks, we use the same logistic function as in classification, 1/ 1+e<sup>−θ<sup>T</sup>x</sup>,  yet we sometimes call it a sigmoid (logistic) <strong>activation</strong> function. In this situation, our “theta” parameters are sometimes called “weights”.</p>
<p>在神经网络中，我们使用与分类相同的逻辑函数，1/ 1+e θTx，但我们有时称其为sigmoid(逻辑)激活函数。在这种情况下，我们的“θ”参数有时被称为“权值”。</p>
<p><strong>输入层</strong>，  <strong>输出层</strong>  and <strong>隐藏层</strong></p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/56.png">  </p>
<p>Our input nodes (layer 1), also known as the “input layer”, go into another node (layer 2), which finally outputs the hypothesis function, known as the “output layer”.</p>
<p>我们的输入节点(layer 1)，也称为“输入层”，进入另一个节点(layer 2)，最后输出假设函数，称为“输出层”。</p>
<p>We can have intermediate layers of nodes between the input and output layers called the “hidden layers.”</p>
<p>我们可以在输入和输出层之间有称为“隐藏层”的中间节点层。</p>
<p><strong>激活单元</strong></p>
<p>In this example, we label these intermediate or “hidden” layer nodes a<sub>0</sub><sup>2</sup>  …… a<sub>n</sub><sup>2</sup>  and call them “activation units.”</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/57.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/59.png"> </p>
<p>This is saying that we compute our activation nodes by using a 3×4 matrix of parameters. We apply each row of the parameters to our inputs to obtain the value for one activation node. 这就是说，我们通过使用一个参数矩阵来计算激活节点。我们将每一行参数应用于输入，以获得一个激活节点的值。</p>
<p><strong>Our hypothesis output is the logistic function applied to the sum of the values of our activation nodes,</strong> which have been multiplied by yet another parameter matrix Θ<sup>(2)</sup> containing the weights for our second layer of nodes. </p>
<p><strong>我们的假设输出是应用激活节点值和的logistic函数</strong>，这些值乘以另一个参数矩阵Θ(2)，该矩阵包含第二层节点的权值。</p>
<p><strong>参数矩阵的维度：</strong></p>
<p>The dimensions of these matrices of weights is determined as follows:</p>
<blockquote>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/60.png">  </p>
</blockquote>
<p>一个神经单元的计算过程：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/61.png"> </p>
<p> <strong>对于神经单元计算的向量化：</strong></p>
<p> We’re going to define a new variable z<sub>k</sub><sup>(j)</sup> that encompasses the parameters inside our g function. </p>
<p>我们将定义一个新的变量zk(j)，它包含了g函数中的参数。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/59.png"> </p>
<p>In our previous example if we replaced by the variable z for all the parameters we would get:</p>
<p>在我们之前的例子中，如果我们用变量z替换所有的参数，我们将得到： </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/62.png">   </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/64.png">  </p>
<p>We are multiplying our matrix Θ<sup>(<em>j</em>−1)</sup> with dimensions sj×(n*+1) . This gives us our vector z*(<em>j</em>) with height s_j. </p>
<p>Now we can get a vector of our activation nodes for layer j as follows:</p>
<blockquote>
<p>a^(j) = g(z^(j))</p>
</blockquote>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/63.png">  </p>
<p>Notice that in this <strong>last step</strong>, between layer j and layer j+1, we are doing <strong>exactly the same thing</strong> as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses. 注意最后一步，在j层和j+1层之间，我们做的和逻辑回归是一样的。在神经网络中加入所有这些中间层，可以让我们更优雅地产生有趣和复杂的非线性假设。</p>
<h2 id="神经网络的实例应用"><a href="#神经网络的实例应用" class="headerlink" title="神经网络的实例应用"></a>神经网络的实例应用</h2><p>接下来的两个例子将会解释为什么神经网络算法可以适用于非线性回归的问题：</p>
<p><strong>异或的模拟</strong>：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/66.png">   </p>
<h3 id="例子1-模拟-AND-，OR-，-NOR-并加一个隐藏层得到-XNOR"><a href="#例子1-模拟-AND-，OR-，-NOR-并加一个隐藏层得到-XNOR" class="headerlink" title="例子1: 模拟 AND ，OR ， NOR 并加一个隐藏层得到 XNOR"></a>例子1: 模拟 AND ，OR ， NOR 并加一个隐藏层得到 XNOR</h3><p>A simple example of applying neural networks is by predicting x_1 AND x_2, which is the logical ‘and’ operator and is only true if both x_1 and x_2 are 1.</p>
<p>应用神经网络的一个简单例子是通过预测x 1和x 2，这是逻辑的“和”运算符，只有当x 1和x 2都是1时才成立。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/65.png">  </p>
<p>所以我们用一个小的神经网络而不是实际的和门构建了计算机的一个基本操作。</p>
<p>神经网络也可以用来模拟所有其他逻辑门。</p>
<p>下面是逻辑运算符’OR’的例子，表示x 1x 1为真或x 2x 2为真，或两者都为真</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/67.png">  </p>
<p><strong>对于异或 XNOR 逻辑操作的模拟</strong>  </p>
<p> <img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/69.png"> </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/68.png">  </p>
<p>这样我们就有了使用带有两个节点的隐藏层的XNOR操作符! 以下是对上述算法的总结</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/70.png">  </p>
<h3 id="例子2-多类问题的分类器"><a href="#例子2-多类问题的分类器" class="headerlink" title="例子2: 多类问题的分类器"></a>例子2: 多类问题的分类器</h3><p>To classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four categories. We will use the following example to see how this classification is done. This algorithm takes as input an image and classifies it accordingly: </p>
<p>为了将数据分类为多个类，我们让假设函数返回一个值的向量。假设我们想将数据分为四类。我们将使用下面的示例来了解如何进行这种分类。该算法以图像作为输入，并对其进行相应的分类</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/71.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/72.png">  </p>
<p>Each y<sup>(i)</sup> represents a different image corresponding to either a car, pedestrian, truck, or motorcycle.</p>
<p>The inner layers, each provide us with some new information which leads to our final hypothesis function. </p>
<p>隐藏层，每一层都为我们提供了一些新的信息这些信息会引导我们最终的假设函数。</p>
<p>The setup looks like:</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/73.png">  </p>
<h2 id="Cost-Function-代价函数-2"><a href="#Cost-Function-代价函数-2" class="headerlink" title="Cost Function  代价函数"></a>Cost Function  代价函数</h2><p>Let’s first define a few variables that we will need to use:</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/74.png">  </p>
<p>对于神经网络的代价函数，实际上是逻辑回归的代价函数的泛化：</p>
<p>回想一下逻辑回归的代价函数如下：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/75.png">  </p>
<p>对比一下，神经网络的代价函数如下：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/76.png">  </p>
<p>We have added a few nested summations to account for our multiple output nodes. In the first part of the equation, before the square brackets, we have an additional nested summation that loops through the number of output nodes.</p>
<p>我们添加了一些嵌套的求和来说明我们的多个输出节点。在方程的第一部分，方括号之前，我们有一个额外的嵌套求和，它循环遍历输出节点的数量。</p>
<p>In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.</p>
<p>在正则化部分，方括号之后，我们必须考虑多个矩阵。当前矩阵中的列数等于当前层(包括偏置单元)中的节点数。当前矩阵的行数等于下一层的节点数(不包括偏置单元)。和以前使用逻辑回归时一样，我们将每一项平方。</p>
<p>Note:</p>
<ul>
<li>the double sum simply adds up the logistic regression costs calculated for each cell in the output layer</li>
<li>the triple sum simply adds up the squares of all the individual Θs in the entire network.</li>
<li>the i in the triple sum does <strong>not</strong> refer to training example i</li>
</ul>
<h2 id="Back-Propagation-反向传播"><a href="#Back-Propagation-反向传播" class="headerlink" title="Back Propagation 反向传播"></a>Back Propagation 反向传播</h2><blockquote>
<p>“Backpropagation” is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression. </p>
<p>“反向传播”是神经网络术语，用于最小化我们的成本函数，就像我们在逻辑回归和线性回归中所做的梯度下降一样。</p>
</blockquote>
<p>That is, we want to minimize our cost function J using an optimal set of parameters in theta. In this section we’ll look at the equations we use to compute the partial derivative of J(Θ):</p>
<p>也就是说，我们想要最小化代价函数J用的是一组最优的参数。在本节中，我们将研究用于计算J(Θ)的偏导数的方程。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/77.png"> </p>
<p><strong>反向传播算法的基本流程如下所示：</strong></p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/78.png">  </p>
<p><strong>反向传播的详细过程：</strong></p>
<p><strong>Back propagation Algorithm</strong></p>
<p>Given training set { ( <em>x</em>(1),<em>y</em>(1))⋯(<em>x</em>(<em>m</em>),<em>y</em>(<em>m</em>) ) }</p>
<ul>
<li>Set Δ<em>i</em>,<em>j</em>(<em>l</em>) := 0 for all (l,i,j), (hence you end up having a matrix full of zeros)</li>
</ul>
<p>For training example t =1 to m:</p>
<ol>
<li>Set a<sup>(1)</sup> := x<sup>(t)</sup>&gt;</li>
<li>Perform forward propagation to compute <em>a</em><sup>(<em>l</em>)</sup> for l=2,3,…,L</li>
</ol>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/79.png">    </p>
<ol start="3">
<li>Using y<sup>(<em>t</em>)</sup>, compute δ<sup>(L)</sup>&gt;=a<sup>(L)</sup>−y<sup>(<em>t</em>)</sup> </li>
</ol>
<p>Where L is our total number of layers and a<sup>(<em>L</em>)</sup> is the vector of outputs of the activation units for the last layer. So our “error values” for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y. To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left:</p>
<p>其中L是我们的总层数，a(L)是最后一层激活单元的输出向量。所以我们最后一层“错误的值”只是我们实际结果的差异在最后一层和正确输出y。之前的δ值层最后一层,我们可以使用一个方程的步骤从右到左:</p>
<ol start="4">
<li><p>The delta values of layer l are calculated by multiplying the delta values in the next layer with the theta matrix of layer l. We then element-wise multiply that with a function called g’, or g-prime, which is the derivative of the activation function g evaluated with the input values given by z^{(l)}<em>z</em>(<em>l</em>).</p>
<p>他的δ值计算了层l乘以δ值在下一层一层的θ矩阵l。然后我们element-wise乘以这一函数g’,或g ‘,这是激活函数的导数g评估的输入值z<sup>(l)</sup></p>
<p>g ‘的导数项也可以写成</p>
<p>The g-prime derivative terms can also be written out as:</p>
</li>
</ol>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/80.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/81.png"> </p>
<h1 id="Advices-for-Applying-Machine-learning-应用机器学习的建议"><a href="#Advices-for-Applying-Machine-learning-应用机器学习的建议" class="headerlink" title="Advices for Applying Machine learning 应用机器学习的建议"></a>Advices for Applying Machine learning 应用机器学习的建议</h1><h2 id="Evaluating-a-Hypothesis-评估一个假设函数"><a href="#Evaluating-a-Hypothesis-评估一个假设函数" class="headerlink" title="Evaluating a Hypothesis  评估一个假设函数"></a>Evaluating a Hypothesis  评估一个假设函数</h2><p>一旦我们通过下列方法解决了预测中的错误：</p>
<p>Once we have done some trouble shooting for errors in our predictions by: </p>
<ul>
<li>Getting more training examples</li>
<li>Trying smaller sets of features</li>
<li>Trying additional features</li>
<li>Trying polynomial features</li>
<li>Increasing or decreasing λ</li>
</ul>
<p>We can move on to evaluate our new hypothesis.  就该开始评估我们的假设函数了</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/83.png">  </p>
<p><strong>划分训练集和测试集</strong></p>
<p>A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). </p>
<p>Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a <strong>training set</strong> and a <strong>test set</strong>. </p>
<p>Typically, the <strong>training set consists of 70 %</strong> of your data and the <strong>test set is the remaining 30 %</strong>. </p>
<blockquote>
<p><strong>The new procedure using these two sets is then:</strong></p>
<ol>
<li><strong>Learn Θ and minimize J<sub>train</sub>(Θ) using the training set</strong></li>
<li><strong>Compute the test set error J<sub>test</sub>(Θ)</strong></li>
</ol>
</blockquote>
<p>测试集误差计算：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/82.png">  </p>
<h2 id="Model-Selection-and-Train-Validation-Test-Sets-模型选择和训练-验证-测试集"><a href="#Model-Selection-and-Train-Validation-Test-Sets-模型选择和训练-验证-测试集" class="headerlink" title="Model Selection and Train/Validation/Test Sets   模型选择和训练/验证/测试集"></a>Model Selection and Train/Validation/Test Sets   模型选择和训练/验证/测试集</h2><p>Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. </p>
<p>It could over fit and as a result your predictions on the test set would be poor. </p>
<p>The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set. 在你训练参数的数据集上测量的假设的误差将低于任何其他数据集上的误差。</p>
<p>Given many models with different polynomial degrees, we can use a systematic approach to identify the ‘best’ function. 给定许多具有不同多项式度的模型，我们可以使用系统的方法来确定“最佳”函数。</p>
<p>In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result.</p>
<p>为了选择你的假设模型，你可以测试每一个多项式的次数，看看错误的结果。</p>
<p><strong>模型选择</strong> </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/84.png"> </p>
<p>One way to break down our dataset into the three sets is:</p>
<ul>
<li>Training set: 60%</li>
<li>Cross validation set: 20%</li>
<li>Test set: 20%</li>
</ul>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/85.png">  </p>
<p>We can now calculate three separate error values for the three different sets using the following method:</p>
<p>我们现在可以使用下面的方法为这三个不同的集合计算三个独立的误差值：</p>
<ol>
<li><p>Optimize the parameters in Θ using the training set for each polynomial degree.</p>
<p>使用每个多项式度的训练集优化Θ中的参数</p>
</li>
<li><p>Find the polynomial degree d with the least error using the cross validation set.</p>
<p>使用交叉验证集找出误差最小的多项式d次</p>
</li>
<li><p>Estimate the generalization error using the test set with J<sub>test</sub>&gt;(Θ(d)), (d = theta from polynomial with lower error);</p>
<p>使用Jtest&gt;(Θ(d))测试集估计泛化误差，(d = theta from误差较小的多项式)</p>
</li>
</ol>
<p>This way, the degree of the polynomial d has not been trained using the test set.</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/86.png">  </p>
<h2 id="Diagnosing-Bias-vs-Variance-判断-偏差-误差"><a href="#Diagnosing-Bias-vs-Variance-判断-偏差-误差" class="headerlink" title="Diagnosing Bias vs. Variance 判断 偏差 / 误差"></a>Diagnosing Bias vs. Variance 判断 偏差 / 误差</h2><blockquote>
<ul>
<li>We need to distinguish whether <strong>bias</strong> or <strong>variance</strong> is the problem contributing to bad predictions.</li>
<li>High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two.</li>
<li>高偏差为欠拟合，高方差为过拟合。理想情况下，我们需要在这两者之间找到一个黄金平衡点。</li>
</ul>
</blockquote>
<p><strong>High bias (underfitting)</strong>: both J<sub>train</sub>&gt;(Θ) and J<sub>CV</sub>(Θ) will be high. Also, J<sub>CV</sub>&gt;(Θ)≈J<sub>train</sub>(Θ).</p>
<p><strong>High variance (overfitting)</strong>: J_{train}(\Theta)* will be low and J_{CV}(\Theta) will be much greater than J_{train}(\Theta)</p>
<p> <img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/87.png"> </p>
<h2 id="Regularization-and-Bias-Variance-正则化和偏差误差"><a href="#Regularization-and-Bias-Variance-正则化和偏差误差" class="headerlink" title="Regularization and Bias/Variance 正则化和偏差误差"></a>Regularization and Bias/Variance 正则化和偏差误差</h2><p>In the figure above, we see that as λ increases, our fit becomes more rigid. On the other hand, as λ approaches 0, we tend to over overfit the data. So how do we choose our parameter <em>λ</em> to get it ‘just right’ ? In order to choose the model and the regularization term λ, we need to:</p>
<ol>
<li>Create a list of lambdas (i.e. λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});</li>
<li>Create a set of models with different degrees or any other variants.</li>
<li>Iterate through the λs and for each <em>λ</em> go through all the models to learn some Θ.</li>
<li>Compute the cross validation error using the learned Θ (computed with λ) on the J_{CV}(\Theta) without regularization or λ = 0.</li>
<li>Select the best combo that produces the lowest error on the cross validation set.</li>
<li>Using the best combo Θ and λ, apply it on J_{test}(\Theta) to see if it has a good generalization of the problem.</li>
</ol>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/91.png"> </p>
<h2 id="Learning-Curve-学习曲线"><a href="#Learning-Curve-学习曲线" class="headerlink" title="Learning Curve 学习曲线"></a>Learning Curve 学习曲线</h2><p>Training an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence:</p>
<ul>
<li>As the training set gets larger, the error for a quadratic function increases.<ul>
<li>随着训练集的增大，二次函数的误差也会增大</li>
</ul>
</li>
<li>The error value will plateau out after a certain m, or training set size.<ul>
<li>误差值在达到一定的m或训练集大小后会趋于稳定</li>
</ul>
</li>
</ul>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/88.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/89.png">  </p>
<h2 id="Deciding-What-to-Do-Next-Revisited-决定下一步做什么"><a href="#Deciding-What-to-Do-Next-Revisited-决定下一步做什么" class="headerlink" title="Deciding What to Do Next Revisited 决定下一步做什么"></a>Deciding What to Do Next Revisited 决定下一步做什么</h2><p>Our decision process can be broken down as follows:</p>
<ul>
<li><p><strong>Getting more training examples:</strong> Fixes high variance</p>
</li>
<li><p><strong>Trying smaller sets of features:</strong> Fixes high variance</p>
</li>
<li><p><strong>Adding features:</strong> Fixes high bias</p>
</li>
<li><p><strong>Adding polynomial features:</strong> Fixes high bias</p>
</li>
<li><p><strong>Decreasing λ:</strong> Fixes high bias</p>
</li>
<li><p><strong>Increasing λ:</strong> Fixes high variance.</p>
</li>
</ul>
<h3 id="Diagnosing-Neural-Networks-对于神经网络来说"><a href="#Diagnosing-Neural-Networks-对于神经网络来说" class="headerlink" title="Diagnosing Neural Networks 对于神经网络来说"></a><strong>Diagnosing Neural Networks</strong> 对于神经网络来说</h3><ul>
<li>A neural network with fewer parameters is <strong>prone to underfitting</strong>. It is also <strong>computationally cheaper</strong>.<ul>
<li>参数较少的神经网络容易出现欠拟合现象。它在计算上也更简单</li>
</ul>
</li>
<li>A large neural network with more parameters is <strong>prone to overfitting</strong>. It is also <strong>computationally expensive</strong>. In this case you can use regularization (increase λ) to address the overfitting.<ul>
<li>具有较多参数的大型神经网络容易出现过拟合现象。它在计算上也很复杂。在这种情况下，你可以使用正则化(增加λ)来处理过拟合</li>
</ul>
</li>
</ul>
<p>Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best. </p>
<p>使用单个隐藏层是一个很好的初始默认值。您可以使用交叉验证集在多个隐藏层上训练神经网络。然后您可以选择性能最好的一个。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/90.png">  </p>
<h1 id="Machine-Learning-System-Design-机器学习的系统设计"><a href="#Machine-Learning-System-Design-机器学习的系统设计" class="headerlink" title="Machine Learning System Design 机器学习的系统设计"></a>Machine Learning System Design 机器学习的系统设计</h1><h2 id="Building-a-Spam-Classifier-建造一个垃圾分类器"><a href="#Building-a-Spam-Classifier-建造一个垃圾分类器" class="headerlink" title="Building a Spam Classifier 建造一个垃圾分类器"></a>Building a Spam Classifier 建造一个垃圾分类器</h2><p>给定电子邮件的数据集，我们可以为每一封电子邮件构造一个向量。这个向量中的每一项都代表一个单词。向量通常包含10,000到50,000个条目，这些条目是通过在我们的数据集中找到最常使用的单词而收集的。如果在电子邮件中找到一个单词，我们将其相应的条目赋值为1，否则，如果没有找到，该条目将为0。一旦我们准备好了所有的x向量，我们训练我们的算法，最后，我们可以使用它来分类电子邮件是否是垃圾邮件。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/92.png">  </p>
<p>So how could you spend your time to improve the accuracy of this classifier?</p>
<ul>
<li>Collect lots of data (for example “honeypot” project but doesn’t always work)</li>
<li>Develop sophisticated features (for example: using email header data in spam emails)</li>
<li>Develop algorithms to process your input in different ways (recognizing misspellings in spam).</li>
</ul>
<p>It is difficult to tell which of the options will be most helpful.</p>
<h3 id="Error-Analysis-误差分析"><a href="#Error-Analysis-误差分析" class="headerlink" title="Error  Analysis 误差分析"></a>Error  Analysis 误差分析</h3><p>The recommended approach to solving machine learning problems is to:</p>
<ul>
<li><strong>Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.</strong></li>
<li><strong>Plot learning curves to decide if more data, more features, etc. are likely to help.</strong></li>
<li><strong>Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.</strong></li>
</ul>
<p>​       例如，假设我们有500封电子邮件，而我们的算法将其中的100封错误分类。我们可以手动分析这100封邮件，并根据它们的类型对它们进行分类。然后，我们可以尝试想出新的线索和特征，来帮助我们正确地对这100封邮件进行分类。因此，如果我们错误分类的邮件大部分都是那些试图窃取密码的邮件，那么我们可以找到一些特定于这些邮件的特征，并将它们添加到我们的模型中。我们还可以看到根据词根对每个单词进行分类如何改变我们的错误率。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/93.png">  </p>
<p>将误差结果作为一个单一的数值得到是非常重要的。否则就很难评估算法的性能。</p>
<p>例如，如果我们使用词干分析(词干分析是将相同的单词以不同的形式(fail/fail /failed)处理为一个单词(fail)的过程，并且得到3%的错误率，而不是5%，那么我们一定要将其添加到我们的模型中。然而，如果我们试图区分大写字母和小写字母，最终得到3.2%的错误率，而不是3%，那么我们应该避免使用这个新特性。因此，我们应该尝试新事物，为我们的错误率获得一个数值，并根据我们的结果决定是否要保留新特性。</p>
<h2 id="Handing-Skewed-Data-处理倾斜数据"><a href="#Handing-Skewed-Data-处理倾斜数据" class="headerlink" title="Handing Skewed Data 处理倾斜数据"></a>Handing Skewed Data 处理倾斜数据</h2><p>WHAT IS SKEWED DATA？</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/94.png">  </p>
<p>拿这个癌症分类器的例子来说，如果我们设置一个忽略特征变量X的类，也就是说把所有的都看做是没有癌症的，那么他得到的结果却比运用了逻辑回归模型的结果还要好，这是不合理的，我们称这样的类叫做倾斜类。</p>
<p>那么，该怎么处理倾斜类呢，我们可以换个角度来判断模型的误差：</p>
<h3 id="Precision-Recall-准确率和召回"><a href="#Precision-Recall-准确率和召回" class="headerlink" title="Precision/Recall 准确率和召回"></a>Precision/Recall 准确率和召回</h3><p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/95.png">  </p>
<h2 id="Trading-off-precision-and-recall-权衡准确率和召回-F值"><a href="#Trading-off-precision-and-recall-权衡准确率和召回-F值" class="headerlink" title="Trading off precision and recall 权衡准确率和召回 / F值"></a>Trading off precision and recall 权衡准确率和召回 / F值</h2><p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/96.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/97.png">  </p>
<h2 id="Data-for-Machine-Learning-对于机器学习的数据"><a href="#Data-for-Machine-Learning-对于机器学习的数据" class="headerlink" title="Data  for Machine Learning 对于机器学习的数据"></a>Data  for Machine Learning 对于机器学习的数据</h2><p>对于机器学习来说，训练集的数据量的大小在某些情况下也是十分重要的，大量的数据对于算法的优化，可以使一个不太好的算法优化到一个足够好的地步。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/98.png">  </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/99.png">  </p>
<p>总之就是拥有大量参数的算法，或者说有多层隐藏层的神经网络，使用小数据量的时候容易过拟合的算法，可以使用大量的数据进行优化，效果会比较显著；而对于参数较少，且对于预测结果影响不大的算法，加大数据集并不是一个好的选择，比如你只给房子的大小来预测房价。</p>
<h1 id="Support-Vector-Machine-支持向量机"><a href="#Support-Vector-Machine-支持向量机" class="headerlink" title="Support Vector Machine 支持向量机"></a>Support Vector Machine 支持向量机</h1><h1 id="Unsupervised-Learning-无监督学习"><a href="#Unsupervised-Learning-无监督学习" class="headerlink" title="Unsupervised Learning 无监督学习"></a>Unsupervised Learning 无监督学习</h1><h2 id="Clustering-聚类"><a href="#Clustering-聚类" class="headerlink" title="Clustering 聚类"></a>Clustering 聚类</h2><h3 id="K-Means-Algorithm-K-均值算法"><a href="#K-Means-Algorithm-K-均值算法" class="headerlink" title="K-Means Algorithm  K-均值算法"></a>K-Means Algorithm  K-均值算法</h3><p><strong>算法思想：</strong></p>
<p>预将数据分为K组，则随机选取K个对象作为初始的聚类中心，然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心。</p>
<p>聚类中心以及分配给它们的对象就代表一个<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E8%81%9A%E7%B1%BB/593695">聚类</a>。每分配一个样本，聚类的聚类中心会根据聚类中现有的对象被重新计算。</p>
<p>这个过程将不断重复直到满足某个终止条件。终止条件可以是没有（或最小数目）对象被重新分配给不同的聚类，没有（或最小数目）聚类中心再发生变化，<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E8%AF%AF%E5%B7%AE/738024">误差</a><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B9%B3%E6%96%B9%E5%92%8C/783894">平方和</a>局部最小。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/106.png">  </p>
<p><strong>K-Means 算法步骤展示：</strong></p>
<p>第一步： 选择Cluster centroids；</p>
<img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/100.png" style="zoom: 50%;"> 

<p>第二步： 根据输入的值按照离聚类的中心（cluster centroids）的距离，将输入值分到不同的Cluster当中；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/101.png">  </p>
<p>第三步：根据不同的Cluster的中心不同，重新定义Cluster Centroids的位置；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/102.png">  </p>
<p>第四步：重复第二步和第三步；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/103.png"> </p>
<p>再次迭代2，3步</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/104.png"> </p>
<p>第五步：等到N次迭代后，K-Means算法会将输入值稳定的分为不同的Clusters</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/105.png"> </p>
<p><strong>K-Mean for non-separated clusters</strong></p>
<p>对于未明显分开的聚类来说，也可以使用K-Means算法来进行分类；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/107.png">  </p>
<h3 id="Optimazation-Objective-优化目标"><a href="#Optimazation-Objective-优化目标" class="headerlink" title="Optimazation Objective 优化目标"></a>Optimazation Objective 优化目标</h3><p>在之前所记录的回归算法中的优化都会有一个目标，比如代价函数损失函数之类；</p>
<p>在K-Means算法中的优化目标如下：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/108.png">  </p>
<p>范数，是具有“长度”概念的函数。上图中的优化目标函数中的范式就代表这 数据集中的点到该聚类中心的距离；</p>
<p>下图中所代表的表示先计算一遍将u看作常数，x<sup>（i）</sup> 为变量的最优化的情况，在把u看作变量，x<sup>(i)</sup> 看做常数在计算一边的优化情况，随后进入下一轮迭代；<img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/109.png"> </p>
<h3 id="Random-Initialization-随机初始化"><a href="#Random-Initialization-随机初始化" class="headerlink" title="Random Initialization 随机初始化"></a>Random Initialization 随机初始化</h3><p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/110.png">  </p>
<p>随机初始化的设定 Cluster Centroids 可能会因为各种原因，导致不能够完成最佳的聚类分类或者是卡到局部最优解处；如下图所示</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/111.png">  </p>
<p>所以为了能够找到可以使优化目标到达最佳的初始点，或者说找到可以适当分类的初始点，我们可以多做几次初始化选择；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/112.png">  </p>
<h3 id="Choosing-the-number-of-clusters-对于聚类中心个数的选择"><a href="#Choosing-the-number-of-clusters-对于聚类中心个数的选择" class="headerlink" title="Choosing the number of clusters 对于聚类中心个数的选择"></a>Choosing the number of clusters 对于聚类中心个数的选择</h3><p>常常我们在进行K-Means算法之前，我们需要决定多少个聚类中心，所以这里会有一个Elbow Method（手肘方法）来帮助我们判断，如下图中的第一个图，在K = 3 和 K = 4 之间有一个明显的拐点，所以这个K = 3可以当作优先考虑的点；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/113.png"> </p>
<p>但是有时候这个手肘方法并不是那么的见效，可能像上图的第二的所展示的；那么这个时候，我们就要考虑到现实的意义了，比如说你现在做这个聚类的目的是什么，像T-shirt的分类，你可以分为三个，S M L，也可以进一步分为五个，XS S M L XL</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/119.png"> </p>
<h1 id="Dimensionatity-Reduction-降维"><a href="#Dimensionatity-Reduction-降维" class="headerlink" title="Dimensionatity  Reduction 降维"></a>Dimensionatity  Reduction 降维</h1><blockquote>
<p>降维：在<a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>和<a href="https://link.zhihu.com/?target=https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%AD%A6">统计学</a>领域，<strong>降维</strong>是指在某些限定条件下，降低随机变量个数，得到一组“不相关”主变量的过程（较本质的解释）。</p>
<p>换言之，降维其更深层次的意义在于<strong>有效信息的提取综合及无用信息的摈弃。</strong></p>
<p>数据降维算法是机器学习算法中的大家族，与分类、回归、聚类等算法不同，它的目标是将向量投影到<strong>低维空间</strong>，以达到某种目的如可视化，或是做分类。</p>
</blockquote>
<p>为什么要降低维度？</p>
<h2 id="Motivation-目的"><a href="#Motivation-目的" class="headerlink" title="Motivation 目的"></a>Motivation 目的</h2><h3 id="Data-Compression-数据压缩"><a href="#Data-Compression-数据压缩" class="headerlink" title="Data Compression 数据压缩"></a>Data Compression 数据压缩</h3><p>将二维的特征变量变为一维的特征变量，如下图中所示的 x1（cm） 和 x2（inches）表示的是都是长度，就可以将这两个特征变量降到一维的，用Z1（一条直线）来表示；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/114.png">   </p>
<p>接下来是一个三维降到二维的例子；（实际上降维可以从很高维降到低维度）</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/115.png">  </p>
<p>图中的三维特征变量，先从第一步未降维的点云降到第二步当中的压缩到一个平面当中，并用z1和z2来表示，在第三步当中用一个平面坐标轴表示；</p>
<h3 id="Visualization-可视化"><a href="#Visualization-可视化" class="headerlink" title="Visualization 可视化"></a>Visualization 可视化</h3><p>降维的另外一个目的就是将原本高维的，不可视的数据集来降到2D or 3D来进行可视化；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/116.png">  </p>
<p>如这个数据集当中，有50个特征变量，如GDP，人均GDP……..</p>
<p>如果直接按照这个来进行，维度太高是不可以进行可视化的，那么我们就将特征变量降到2维的情况；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/117.png"> </p>
<p>降维到2D后，就可以用z1 和 z2 来进行可视化表示，每个点就代表来一个国家；而z1 和 z2 很难赋予他一个准确的物理意义，可能z1是各个特征变量一起降维而得到的GDP ，z2 也是同样而得到的人均GDP。</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/118.png">  </p>
<h2 id="PCA（Principal-Component-Analysis-problem-formula1on）主成分分析"><a href="#PCA（Principal-Component-Analysis-problem-formula1on）主成分分析" class="headerlink" title="PCA（Principal Component Analysis problem formula1on）主成分分析"></a>PCA（Principal Component Analysis problem formula1on）主成分分析</h2><p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/120.png"> </p>
<p>主成分分析是用来将原先高维的特征变量转化为低维的特征变量；</p>
<p>通过正交变换将一组可能存在相关性的变量转换为一组线性不相关的变量，转换后的这组变量叫主成分。</p>
<p>主成分分析是设法将原来众多具有一定相关性（比如P个指标），重新组合成一组新的互相无关的综合指标来代替原来的指标。</p>
<p>主成分分析法是一种降维的统计方法，它借助于一个正交变换，将其分量相关的原随机向量转化成其分量不相关的新随机向量，这在代数上表现为将原随机向量的协方差阵变换成对角形阵，在几何上表现为将原坐标系变换成新的正交坐标系，使之指向样本点散布最开的p 个正交方向，然后对多维变量系统进行降维处理，使之能以一个较高的精度转换成低维变量系统，再通过构造适当的价值函数，进一步把低维系统转化成一维系统。</p>
<p>PCA降维的目的，就是为了在尽量保证“信息量不丢失”的情况下，对原始特征进行降维，也就是尽可能将原始特征往具有最大投影信息量的维度上进行投影。将原特征投影到这些维度上，使降维后信息量损失最小。<img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/121.png"></p>
<h3 id="求解步骤"><a href="#求解步骤" class="headerlink" title="求解步骤"></a>求解步骤</h3><blockquote>
<p>去除平均值<br>计算协方差矩阵<br>计算协方差矩阵的特征值和特征向量<br>将特征值排序<br>保留前N个最大的特征值对应的特征向量<br>将原始特征转换到上面得到的N个特征向量构建的新空间中（最后两步，实现了特征压缩）</p>
</blockquote>
<p>Data Preprocessing 数据预处理</p>
<p>  在进行算法之前要对数据进行预处理，进行均值归一化，有必要还需要使用特征缩放；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/122.png"> </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/123.png"> </p>
<p>u1 和 u2 是特征变量将要映射到的平面，而PCA算法需要将各个数据点映射到该平面上的距离最短</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/124.png"> </p>
<p>在octave or matlab中 可以使用 svd 来计算协方差矩阵</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/125.png"> </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/126.png"> </p>
<h3 id="选择主成分的个数"><a href="#选择主成分的个数" class="headerlink" title="选择主成分的个数"></a>选择主成分的个数</h3><p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/127.png"> </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/128.png"> </p>
<p>最后应该选择使 上述式子&lt;= 0.01 的 k 的最小的值；</p>
<h1 id="Anomaly-Detection-误差检测"><a href="#Anomaly-Detection-误差检测" class="headerlink" title="Anomaly Detection 误差检测"></a>Anomaly Detection 误差检测</h1><h2 id="Density-Estimation-密度估计"><a href="#Density-Estimation-密度估计" class="headerlink" title="Density Estimation 密度估计"></a>Density Estimation 密度估计</h2><p>以航天发动机的例子来说明这个异常检测：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/129.png"> </p>
<p>P（x）为检测的模型： <img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/130.png">  </p>
<p>异常检测的例子：</p>
<p>欺诈检测；工厂检测；电脑监控</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/131.png">  </p>
<h2 id="Gassian-distribution-高斯分布"><a href="#Gassian-distribution-高斯分布" class="headerlink" title="Gassian distribution 高斯分布"></a>Gassian distribution 高斯分布</h2><p>高斯分布，不介绍了，主要就是有两个参树；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/132.png">  </p>
<p> u /miu 代表了特征的平均值 ，variance 就是方差，和宽度有关系；</p>
<p>下面有四个高斯分布的例子：</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/133.png">  </p>
<p>mean 和 variance都是可以用公式来计算的；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/134.png">  </p>
<h2 id="Algorithm-算法"><a href="#Algorithm-算法" class="headerlink" title="Algorithm 算法"></a>Algorithm 算法</h2><p>P（x）</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/135.png"></p>
<blockquote>
<p>Anomaly detection algorithm的步骤：</p>
<ol>
<li>选择你认为可能是异常例子的特点的特征x<sub>i</sub></li>
<li>拟合出参数 u1，….,un; 方差1，……方差n</li>
<li>给一个新的例子x，计算p（x）</li>
</ol>
</blockquote>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/136.png"> </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/137.png"></p>
<h2 id="Developing-and-evaluating-an-anomaly-detection-system-评估异常检测系统"><a href="#Developing-and-evaluating-an-anomaly-detection-system-评估异常检测系统" class="headerlink" title="Developing and evaluating an anomaly detection system 评估异常检测系统"></a>Developing and evaluating an anomaly detection system 评估异常检测系统</h2><p>对于任何事物来说，有一个可以用数值来表示的评估方案是非常重要的；</p>
<p>下面是一个航天发动机的例子：</p>
<p>粉色框框表示将交叉验证集和测试集放在一起了，不推荐这种做法；</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/138.png"> </p>
<h3 id="Algorithm-evaluation-算法评估"><a href="#Algorithm-evaluation-算法评估" class="headerlink" title="Algorithm evaluation 算法评估"></a>Algorithm evaluation 算法评估</h3><p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/139.png">  </p>
<h2 id="Choosing-what-features-to-use-选择特征变量"><a href="#Choosing-what-features-to-use-选择特征变量" class="headerlink" title="Choosing what features to use 选择特征变量"></a>Choosing what features to use 选择特征变量</h2><p>非高斯分布的特征变量的值可以经过变换</p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/140.png"> </p>
<p><img src="/2021/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/141.png"> </p>
<h1 id="Recommand-system-推荐系统"><a href="#Recommand-system-推荐系统" class="headerlink" title="Recommand system 推荐系统"></a>Recommand system 推荐系统</h1><h2 id="Predicting-Movie-Ratings-预测电影评分例子"><a href="#Predicting-Movie-Ratings-预测电影评分例子" class="headerlink" title="Predicting Movie Ratings 预测电影评分例子"></a>Predicting Movie Ratings 预测电影评分例子</h2><h2 id="Collaborative-Filtering-协同过滤"><a href="#Collaborative-Filtering-协同过滤" class="headerlink" title="Collaborative Filtering 协同过滤"></a>Collaborative Filtering 协同过滤</h2><h1 id="Large-Scale-Machine-Learning-大规模机器学习"><a href="#Large-Scale-Machine-Learning-大规模机器学习" class="headerlink" title="Large Scale Machine Learning 大规模机器学习"></a>Large Scale Machine Learning 大规模机器学习</h1><h2 id="Gradient-Descent-with-Large-Datasets"><a href="#Gradient-Descent-with-Large-Datasets" class="headerlink" title="Gradient Descent with Large Datasets"></a>Gradient Descent with Large Datasets</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayi8991.github.io/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiayi Liang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiayiSpace">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/" class="post-title-link" itemprop="url">搜索问题与图搜索</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-14 10:36:17" itemprop="dateCreated datePublished" datetime="2021-12-14T10:36:17+08:00">2021-12-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-19 17:50:44" itemprop="dateModified" datetime="2021-12-19T17:50:44+08:00">2021-12-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="状态图"><a href="#状态图" class="headerlink" title="状态图"></a>状态图</h1><ul>
<li><strong>1. 状态</strong><ul>
<li> 状态就是问题在任一确定时刻的状况,它表征了问题特征和结构等。状态在状态图中表示为节点。状态一般用一组数据表示。在程序中用字符、数字、记录、数组、结构、对象等表示</li>
</ul>
</li>
<li><strong>2. 状态转换规则</strong><ul>
<li> 状态转换规则就是能使问题状态改变的某种操作、规则、 行为、变换、关系、函数、算子、过程等等。状态转换规则也称为操作,问题的状态也只能经定义在其上的这种操作而改变。状态转换规则在状态图中表示为边。在程序中状态转换规则可用数据对、条件语句、规则、函数、过程等表示</li>
</ul>
</li>
<li><strong>3. 状态图表示</strong><ul>
<li>一个问题的状态图是一个三元组</li>
<li>​           (<em>S, F, G</em>) </li>
<li>其中S是问题的初始状态集合, F是问题的状态转换规则集合, G是问题的目标状态集合。 </li>
<li>  一个问题的全体状态及其关系就构成一个空间, 称为状态空间。所以,状态图也称为状态空间图。</li>
</ul>
</li>
</ul>
<h2 id="启发式搜索"><a href="#启发式搜索" class="headerlink" title="启发式搜索"></a>启发式搜索</h2><p><strong>启发性信息：</strong></p>
<ol>
<li>用于扩展节点的选择, 即用于决定应先扩展哪一个节点, 以免盲目扩展。</li>
<li>用于生成节点的选择,即用于决定应生成哪些后续节点,以免盲目地生成过多无用节点。</li>
<li>用于删除节点的选择,即用于决定应删除哪些无用节点, 以免造成进一步的时空浪费。</li>
</ol>
<p><strong>启发函数（距离目标函数，表示与目标的距离，越小越好）：</strong></p>
<ul>
<li>启发函数是用来估计搜索树上节点<em>x</em>与目标节点<em>S**g</em>接近程度的一种函数, 通常记为<em>h</em>(<em>x</em>)。使用启发函数的搜索实际上也是一种深度优先搜索</li>
</ul>
<p><strong>启发式搜索算法：</strong></p>
<ul>
<li>全局择优搜索</li>
<li>局部择优搜索</li>
</ul>
<p><strong>全局和局部：</strong></p>
<ul>
<li>全局：考虑Open表中所有节点</li>
<li>局部： 仅仅考虑当前节点N的扩展子节点，且插入到Open表首部</li>
</ul>
<h3 id="全局择优搜索算法"><a href="#全局择优搜索算法" class="headerlink" title="全局择优搜索算法"></a>全局择优搜索算法</h3><ul>
<li><strong>全局启发函数h(x)最小，即把当前节点子节点加入到Open表，再优选Open表中节点与目标节点距离小的节点）</strong></li>
</ul>
<blockquote>
<ul>
<li>步1 把初始节点<em>S</em>o放入<em>OPEN</em>表中,计算<em>h</em>(<em>S</em>o)。</li>
<li>步2 若<em>OPEN</em>表为空,则搜索失败, 退出。 </li>
<li>步3 移出<em>OPEN</em>表中第一个节点N放入CLOSED表中, 并冠以序号<em>n</em>。</li>
<li>步4 若目标节点<em>S</em>g=<em>N</em>, 则搜索成功, 结束。 </li>
<li>步5 若<em>N</em>不可扩展, 则转步2。</li>
<li>步6 扩展<em>N</em>, 计算每个子节点<em>x</em>的函数值<em>h</em>(<em>x</em>), 并将所有子节点配以指向<em>N</em>的返回指针后放入<em>OPEN</em>表中, <strong>再对</strong><em><strong>OPEN</strong></em><strong>表中的所有节点按其启发函数值大小以升序排序</strong>,转步2。 </li>
</ul>
</blockquote>
<p><strong>用全局择优搜索法解八数码难题（最好优先，实际上是一种深度优先搜索）</strong></p>
<p><strong>例图：</strong></p>
<p><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1412-23-54.png">  </p>
<p><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1412-28-42.png">  </p>
<h3 id="局部择优搜索算法"><a href="#局部择优搜索算法" class="headerlink" title="局部择优搜索算法"></a>局部择优搜索算法</h3><ul>
<li><strong>局部启发函数h(x)最小，即优选当前节点子节点中与目标节点距离小的节点</strong></li>
</ul>
<blockquote>
<ul>
<li>步1 把初始节点<em>S</em>o放入<em>OPEN</em>表中,计算<em>h</em>(<em>S</em>o)。</li>
<li> 步2 若<em>OPEN</em>表为空,则搜索失败, 退出。 </li>
<li> 步3 移出<em>OPEN</em>表中第一个节点N放入CLOSED表中, 并冠以序号<em>n</em>。</li>
<li> 步4 若目标节点<em>S</em>g=<em>N</em>, 则搜索成功, 结束。 </li>
<li> 步5 若<em>N</em>不可扩展, 则转步2。</li>
<li> 步6 扩展<em>N</em>, 计算每个子节点<em>x</em>的函数值<em>h</em>(<em>x</em>), 并将所有子节点配以指向<em>N</em>的返回指针后<strong>按照h(x)升序排序插入到OPEN表已有节点的前面</strong>,转步2。 </li>
</ul>
</blockquote>
<h3 id="加权状态图搜索"><a href="#加权状态图搜索" class="headerlink" title="加权状态图搜索"></a>加权状态图搜索</h3><h4 id="分支界限法（全局代价最小）"><a href="#分支界限法（全局代价最小）" class="headerlink" title="分支界限法（全局代价最小）"></a>分支界限法（全局代价最小）</h4><ul>
<li>基本思想是：<ul>
<li>每次从<em>OPEN</em>表中选出**代价函数g(x)**值最小的节点进行考察, 而不管这个节点在搜索树的什么位置上。</li>
</ul>
</li>
<li> 算法与前面的“全局择优法” 仅有引导搜索的函数不同，前者为启发函数<em>h</em>(<em>x</em>)，后者为代价<em>g</em>(<em>x</em>)。</li>
<li> 但注意：代价值<em>g</em>(<em>x</em>)是从初始节点<em>S</em>o方向计算而来的,而启发函数值<em>h</em>(<em>x</em>)则是朝目标节点方向计算的。</li>
</ul>
<blockquote>
<p> 所谓代价,可以是两点之间的距离、交通费用或所需时间等等</p>
</blockquote>
<blockquote>
<p>通常用<em>g</em>(<em>x</em>)表示从初始节点<em>S</em>o到节点<em>x</em>的代价, 用<em>c</em>(<em>xi</em>,x j*)表示 父节点xi 到 子节点x j 的代价,即边(<em>xi</em>,<em>x</em>j)的代价, 所以</p>
<ul>
<li><em>g</em>(<em>x j</em>)＝<em>g</em>(<em>xi</em>)＋<em>c</em>(<em>x i</em>, <em>x j</em>) </li>
</ul>
</blockquote>
<ul>
<li><strong>循环：</strong></li>
<li><strong>每次处理Open表第一个节点，如果是目标节点就返回成功</strong></li>
<li><strong>存下已处理的节点到Closed表</strong></li>
<li><strong>找出当前节点的子节点，更新节点间关系</strong></li>
<li><strong>按代价从小到大给选择下一个要处理的节点</strong></li>
</ul>
<p><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1415-43-38.png"> </p>
<h4 id="最近择优法（瞎子爬山，局部代价最小）"><a href="#最近择优法（瞎子爬山，局部代价最小）" class="headerlink" title="最近择优法（瞎子爬山，局部代价最小）"></a>最近择优法（瞎子爬山，局部代价最小）</h4><ul>
<li>把局部择优法算法中的<em>h</em>(<em>x</em>)换成<em>g</em>(<em>x</em>)就可得最近择优法的算法。 Open从右往前插入如右图</li>
<li>这里由D计算出B的代价如果少于4，则应该代替Open表中原有的B并把B移动到前面来。这里从D算出来B的代价是3+2+4 &gt;4，所以不代替。</li>
</ul>
<p><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1415-49-41.png"> </p>
<h2 id="A算法和A-算法"><a href="#A算法和A-算法" class="headerlink" title="A算法和A*算法"></a>A算法和A*算法</h2><h3 id="估价函数"><a href="#估价函数" class="headerlink" title="估价函数"></a>估价函数</h3><ul>
<li><em>f</em>(<em>x</em>)＝<em>g</em>(<em>x</em>)＋<em>h</em>(<em>x</em>) </li>
</ul>
<ol>
<li>**代价函数g(x)**表示从起点到达目前节点x的代价函数，一般设置难度小，因为从起点到目前节点x的路径已经求出；</li>
<li><strong>启发函数h(x</strong>)表示从目前节点到目标节点的距离函数，一般设置难度大，因为从目前节点到目标节点的路径正是我们要搜索的；</li>
<li><strong>估价函数f(x)是代价函数g(x)和h(x)的折中</strong>。</li>
</ol>
<ol>
<li>代价函数g(x)表示从起点到达目前节点x的代价函数，g(x)越小表示节点x越靠近初始节点，有利于搜索的横向搜索，可提高搜索的完备性，但影响搜索效率；</li>
<li>启发函数h(x)表示从目前节点到目标节点的距离函数，h(x)越小表示距离目标越近，有利于搜索的纵向搜索，可提高搜索效率，但影响完备性；</li>
<li>代价函数g(x)=0时，A算法退化成普通的启发式搜索，类似于深度优先；</li>
<li>启发函数h(x)=0时，A算法退化成普通的加权状态图搜索，类似于广度优先。</li>
</ol>
<h3 id="A算法"><a href="#A算法" class="headerlink" title="A算法"></a>A算法</h3><ul>
<li> <em>A</em> 算法是基于估价函数<em>f</em>(<em>x</em>)的一种加权状态图启发式搜索算法。其具体步骤如下： </li>
<li>步1 把附有<em>f</em>(<em>S</em>o)的初始节点<em>S</em>o放入OPEN表。</li>
<li> 步2 若OPEN表为空, 则搜索失败, 退出。 </li>
<li> 步3 移出OPEN表中第一个节点<em>N</em>放入CLOSED表中, 并冠以顺序编号<em>n</em>。</li>
<li> 步4 若目标节点<em>S**g</em>=<em>N</em>, 则搜索成功, 结束。 </li>
<li> 步5 若<em>N</em>不可扩展, 则转步2。 </li>
<li>步6 扩展<em>N</em>,生成一组附有<em>f</em>(<em>x</em>)的子节点,对这组子节点做如下处理： </li>
<li> (1)考察是否有已在OPEN表或CLOSED表中存在的节点；若有则再考察其中有无N的先辈节点,若有则删除之；对于其余节点, 也删除之,但由于它们又被第二次生成, 因而需考虑是否修改已经存在于OPEN表或CLOSED表中的这些节点及其后裔的返回指针和<em>f</em>(<em>x</em>)值, 修改原则是“抄<em>f</em>(<em>x</em>)值小的路走”。</li>
<li>   (2)对其余子节点配上指向<em>N</em>的返回指针后放入OPEN表中, 并对OPEN表按<em>f</em>(<em>x</em>)值以升序排序, 转步2。 </li>
</ul>
<h3 id="A-算法（最佳图搜索算法）"><a href="#A-算法（最佳图搜索算法）" class="headerlink" title="A*算法（最佳图搜索算法）"></a>A*算法（最佳图搜索算法）</h3><ul>
<li>如果对上述A算法再限制其估价函数中的启发函数<em>h</em>(<em>x</em>)满足： 对所有的节点<em>x</em>均有<ul>
<li>​                    <em>h</em>(<em>x</em>)≤d(<em>x</em>) </li>
</ul>
</li>
<li>其中<strong>d(<em>x</em>)是最优解中从节点<em>x</em>到目标节点的启发函数（即节点x到目标节点的距离函数，最小启发函数值，实际上这个d(x)很难估计准确值）</strong>, <strong>即最佳路径上的实际代价</strong>，则它就称为A<em>算法。理论证明A</em>算法可以保证找到最优解。</li>
<li>如果h(x)&lt; d(x)到目标状态的实际距离，这种情况下，搜索的点数多，搜索范围大，效率低。但能得到最优解。</li>
<li>如果h(x)=d(x)，即距离估计h(x)等于最短距离，那么搜索将严格沿着最短路径进行， 此时的搜索效率是最高的。</li>
<li>如果 h(x)&gt;d(x)，搜索的点数少，搜索范围小效率高，但不保证得到最优解。</li>
</ul>
<p><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1416-27-22.png"> </p>
<h2 id="小结：图的启发式搜索算法"><a href="#小结：图的启发式搜索算法" class="headerlink" title="小结：图的启发式搜索算法"></a>小结：图的启发式搜索算法</h2><ul>
<li><strong>选择依据：</strong><ul>
<li><strong>（1）启发函数（距离目标函数，表示与目标的距离）</strong></li>
<li><strong>（2）代价函数 （表示已经付出代价的函数）</strong></li>
</ul>
</li>
<li><strong>全局vs局部的在本课程状态图搜索中的不同：</strong><ul>
<li><strong>全局：考虑Open表中所有节点</strong></li>
<li><strong>局部：只考虑当前节点N的扩展子节点</strong></li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>启发函数h(x)</th>
<th><strong>代价函数</strong>g(x)</th>
<th>估价函数f(x)=g(x)+h(x)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>全局</strong></td>
<td><strong>全局择优</strong></td>
<td><strong>分支界限</strong></td>
<td><strong>A算法 ，A*算法</strong></td>
</tr>
<tr>
<td><strong>局部</strong></td>
<td><strong>局部择优</strong></td>
<td><strong>最近择优</strong></td>
<td></td>
</tr>
</tbody></table>
<h1 id="与或图"><a href="#与或图" class="headerlink" title="与或图"></a>与或图</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><ul>
<li>状态图是与或图中的特例（与或图中的或图）  <ul>
<li>搜索方式，解图（树）<ul>
<li>状态图搜索只是简单不断扩展节点，与或图扩展子节点时需要判断初始节点是否可解</li>
</ul>
</li>
</ul>
</li>
</ul>
<p> <img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1416-41-04.png"></p>
<ul>
<li>图中的弧线表示所连边为“与”关系,不带弧线的边为或关系。这个图中既有与关系又有或关系,因此被称为与或图</li>
</ul>
<h2 id="与或图搜索"><a href="#与或图搜索" class="headerlink" title="与或图搜索"></a>与或图搜索</h2><ul>
<li><p><strong>1. 搜索方式,解图(树)</strong></p>
<ul>
<li> 整个状态图转换过程已知，搜索问题图的解图（树）。</li>
<li>  解图（树）不一定是最优解。</li>
</ul>
</li>
<li><p><strong>2. 可解性判别</strong></p>
<ul>
<li><strong>（参考右图：黑色节点表示可解）</strong></li>
</ul>
</li>
<li><p> 一个节点的可解性判别准：</p>
</li>
<li><p>  (1) 一个节点是可解, 则节点须满足下列条件之一:     -   ① 终止节点是可解节点。（实心点，如t2, t3, t4)    -   ② 一个与节点可解, 当且仅当其子节点全都可解。(如5,2,1)    -   ③ 一个或节点可解, 只要其子节点至少有一个可解。(如4, 3) </p>
</li>
<li><p>  (2) 一个节点是不可解, 则节点须满足下列条件之一:     -   ① 非终止节点的端节点是不可解节点。 (空心叶子节点，如A, B)    -   ② 一个与节点不可解, 只要其子节点至少有一个不可解。（图中没有）    -   ③ 一个或节点不可解, 当且仅当其子节点全都不可解。 （图中没有）</p>
</li>
<li><p><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1416-46-00.png"> </p>
</li>
<li><p><strong>3. 搜索策略</strong></p>
<ul>
<li> 与或图搜索也分为 盲目搜索 和 启发式搜索 两大类。</li>
<li>前者又分为穷举搜索和盲目碰撞搜索。穷举搜索又分为深度优先和广度优先两种基本策略。</li>
</ul>
</li>
<li><p><strong>4.状态变化需递推的穷举搜索算法</strong></p>
<ul>
<li> 步1 把初始节点Qo放入<em>OPEN</em>表。 </li>
<li> 步2 移出<em>OPEN</em>表的第一个节点<em>N</em>放入<em>CLOSED</em>表, 并冠以序号<em>n</em>。 </li>
<li> 步3 若节点N可扩展, 则做下列工作： </li>
</ul>
</li>
<li><p> (1) 扩展N, 将其子节点配上指向父节点的指针后放入OPEN表。</p>
</li>
<li><p>放入尾部是宽度优先，放在前面是深度优先；</p>
</li>
<li><p>考虑Open表全部节点是全局算法，只考虑当前节点子节点是局部算法</p>
</li>
<li><p>  (2)<strong>判断可解性</strong>：考察这些子节点中是否有终止节点（从而有点类似深度优先）。 若有, 则标记它们为可解节点, 并将它们也放入CLOSED表, 然后由它们的可解反向推断其先辈节点的可解性, 并对其中的可解节点进行标记。 如果初始节点也被标记为可解节点, 则搜索成功,结束。</p>
</li>
<li><p> (3)删去OPEN表中那些具有可解先辈的节点(因为其先辈节点已经可解, 故已无再考察该节点的必要), 转步2。 </p>
</li>
<li><p>  步4 若N不可扩展, 则做下列工作：     -  (1)标记<em>N</em>为不可解节点, 然后由它的不可解反向推断其先辈节点的可解性, 并对其中的不可解节点进行标记。如果初始节点So也被标记为不可解节点, 则搜索失败, 退出。    -  (2)删去OPEN表中那些具有不可解先辈的节点(因为其先辈节点已不可解,故已无再考察这些节点的必要), 转步2。 </p>
</li>
</ul>
<p><strong>穷举搜索算法例子：</strong></p>
<p><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1416-46-00.png"> </p>
<ul>
<li>设有与或树如图3-15所示, 其中1号节点为初始节点,<em>t</em>1、<em>t</em>2、<em>t</em>3、<em>t</em>4均为终止节点, A和B是不可解的端节点。 采用广度(优先)搜索策略, 搜索过程如下）： <ul>
<li>（1）将初始节点1号节点移入Open表，再取出后扩展1号节点并移入Closed表, 得2号和3号节点, 依次放入OPEN表尾部。由于这两个节点都非终止节点, 所以接着扩展2号节点移入Closed表。此时OPEN表中只有3号节点。 </li>
<li>（2）2号节点扩展后,得4号节点和<em>t</em>1节点。此时OPEN表中依次有3号、4号和<em>t</em>1节点。由于<em>t</em>1是终止节点,故标记它为可解节点, 并将<em>t</em>1从Open表移入CLOSED表, 再判断其先辈节点的可解性,但<em>t</em>1的父节点2是一个与节点, 故仅由t1的可解还不能确定2号节点可解。所以, 就继续搜索。</li>
<li>(3) 扩展3号节点并移入Closed表,得5号节点和B节点加入到Open表。两者均非终止节点, 所以继续扩展4号节点。 </li>
<li> (4) 4号节点扩展后得节点<em>A</em>和<em>t</em>2加入到Open表，<em>t</em>2是终止节点,标记为可解节点, 放入CLOSED表。这时其先辈节点4和2也为可解节点, 但1号节点还不能确定。这时从OPEN表中删去节点<em>A</em>,因为其父节点4已经可解。 </li>
<li> (５) 扩展5号节点并移入Closed表得<em>t</em>3和<em>t</em>4。由于<em>t</em>3和<em>t</em>4都为终止节点(放入CLOSED表), 故可推得节点5、3、1均为可解节点。 搜索成功, 结束。 </li>
<li> 这时,由CLOSED表便得到由节点1、2、3、4、5和<em>t</em>1、<em>t</em>2、 <em>t</em>3、<em>t</em>4构成的解树，如图3-15 中的粗线所示。</li>
</ul>
</li>
<li><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1416-57-12.png"> </li>
</ul>
<h2 id="启发式与或树搜索"><a href="#启发式与或树搜索" class="headerlink" title="启发式与或树搜索"></a>启发式与或树搜索</h2><p><strong>（针对图中根节点代价非完全已知，需要不停扩展）</strong></p>
<ul>
<li><strong>1. 解树的代价</strong></li>
<li> <strong>2. 希望树</strong></li>
<li>希望树的定义： <ul>
<li>希望树是指搜索过程中最有可能成为最优解树的那棵树。</li>
<li>与/或树的启发式搜索过程就是不断地选择、修正希望树的过程，在该过程中，希望树是不断变化的。它需要符合以下三个条件：<ol>
<li>初始结点S0在希望树T</li>
<li>如果n是具有子结点n1, n2, … , nk的或结点，则n的某个子结点ni在希望树T中的充分必要条件是<br><img src="https://img-blog.csdnimg.cn/20190613062929330.png" alt="在这里插入图片描述"> </li>
<li>如果n是与结点，则n的全部子结点都在希望树T中。</li>
</ol>
</li>
</ul>
</li>
<li><strong>3. 与或树的有序搜索过程</strong></li>
<li> 与或树的有序搜索过程是一个不断选择、修正希望树的过程。如果问题有解, 则经有序搜索将找到最优解树。 </li>
<li> 其搜索过程如下： </li>
<li> 步1 把初始节点Qo放入OPEN表中。 </li>
<li> 步2 求出希望树T, 即根据当前搜索树中节点的代价g求出以Qo为根的希望树T。 </li>
<li> 步3 依次把OPEN表中T的端节点N选出放入CLOSED表中。 </li>
<li> 步4 如果节点N是终止节点, 则做下列工作： </li>
<li>  (1) 标示N为可解节点。 </li>
<li>  (2) 对T应用可解标记过程, 把N的先辈节点中的可解节点都标记为可解节点。 </li>
<li>  (3) 若初始节点Qo能被标记为可解节点, 则T就是最优解树,成功退出。 </li>
<li>  (4) 否则, 从OPEN表中删去具有可解先辈的所有节点</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20190613063108272.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N0YXJ0ZXJfX19fXw==,size_16,color_FFFFFF,t_70" alt="img"> </p>
<p><img src="https://img-blog.csdnimg.cn/20190613063120609.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N0YXJ0ZXJfX19fXw==,size_16,color_FFFFFF,t_70" alt="img">  </p>
<p><img src="https://img-blog.csdnimg.cn/20190613063931341.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N0YXJ0ZXJfX19fXw==,size_16,color_FFFFFF,t_70" alt="img"> <img src="https://img-blog.csdnimg.cn/20190613063944722.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N0YXJ0ZXJfX19fXw==,size_16,color_FFFFFF,t_70" alt="img"> </p>
<h1 id="博弈树"><a href="#博弈树" class="headerlink" title="博弈树"></a>博弈树</h1><p><strong>博弈树是一类特殊的与或图</strong></p>
<ul>
<li>节点交替分为Max，Min 2种。Max与Min价值取向正好相反（想象2个人下棋，一个人走一步）。</li>
<li>不方便区分为与或节点。节点可以看为都是或节点，按照不同代价选择路径。（也有资料说初始节点是或节点，然后与节点、或节点逐层交替出现）</li>
<li>应用广泛：下棋，故障诊断，风险投资</li>
</ul>
<p><strong>基本搜索策略</strong></p>
<ul>
<li>极小极大搜索，生成整个搜索树，从底往根倒推</li>
</ul>
<p><strong>为了提高搜索速度，用到了剪枝</strong></p>
<ul>
<li>α-β剪枝: 剪枝; 搜索和生成同时进行</li>
</ul>
<h2 id="博弈问题"><a href="#博弈问题" class="headerlink" title="博弈问题"></a>博弈问题</h2><ul>
<li>特点：<ul>
<li>双人对弈：轮流下，一人走一步。</li>
<li>信息完备：双方看到的信息一样</li>
<li>零和：双方利益冲突，对一方有利则对另一方不利。一般对节点N取一个估价函数f(N)，一共两类节点：    <ul>
<li>叫Max的极大节点追求最大化，有选择时肯定选值最大的；</li>
<li>叫Min的极小节点追求最小化，有选择时肯定选值最小的。</li>
</ul>
</li>
</ul>
</li>
<li>例子：<ul>
<li>两位选手对垒，轮流走步。这时每一方不仅知道对方过去已经走过的棋步，而且还能估计出对方未来可能的走步。对弈的结果是一方赢（另一方则输），或者双方和局。如象棋，围棋，五子棋，…</li>
</ul>
</li>
<li><strong>完全信息博弈任务原型：</strong><ul>
<li>给出（或逐步生成）一个博弈树，求出在指定搜索深度（层数）下的最佳路径和相应估价分数。比如下象棋，Max先走一步，Min再走一步，再轮到Max走，这时Max遇到的各个局面可以估分，再倒推回去Max的第一步应该选择走哪步最佳</li>
</ul>
</li>
<li>采用剪枝的方法加快极小极大搜索方法的速度，最主要的是α-β剪枝</li>
</ul>
<p><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1916-34-57.png"> </p>
<p><strong>不完全信息博弈</strong></p>
<ul>
<li>不完全信息博弈<ul>
<li>大部分纸牌游戏（如斗地主、拖拉机）</li>
<li>大部分即时策略游戏（如红警、星际、帝国）<ul>
<li>需要探路</li>
<li>游戏还受手速（APM）等影响</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>博弈问题常常不能简单穷举，如中国象棋</strong></p>
<ul>
<li>一盘棋平均走50步，总状态数约为10的161次方。</li>
<li>假设1毫微秒走一步，约需10的145次方年。</li>
<li>结论：不可能穷举。</li>
</ul>
<h2 id="极小极大搜索方法"><a href="#极小极大搜索方法" class="headerlink" title="极小极大搜索方法"></a>极小极大搜索方法</h2><ul>
<li><p>极小极大搜索方法是博弈树搜索的基本方法 。</p>
</li>
<li><p>首先假定，有一个评价函数可以对所有的棋局进行评估。当评价函数值大于0时，表示棋局对我方有利，对对方不利。当评价函数小于0时，表示棋局对我方不利，对对方有利。</p>
</li>
<li><p>方法：</p>
<ul>
<li>当轮到我方走棋时，首先按照一定的搜索深度生成出给定深度d以内的所有状态，计算所有叶节点的评价函数值。</li>
<li>然后从d-1层节点开始逆向计算：对于我方要走的节点（用MAX标记，称为极大节点）取其子节点中的最大值为该节点的值（因为我方总是选择对我方有利的棋）。<ul>
<li>补充注意：生成了深度d以内的整棵树</li>
</ul>
</li>
<li>对于对方要走的节点（用MIN标记，称为极小节点）取其子节点中的最小值为该节点的值（对方总是选择对我方不利的棋）。</li>
<li>一直到计算出根节点的值为止。获得根节点取值的那一分枝，即为所选择的最佳走步。</li>
</ul>
</li>
</ul>
<p><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1916-51-23.png"></p>
<ul>
<li>因此，极小极大过程是一种假定对手每次回应都正确的情况下，如何从中找出对我方最有利的走步的搜索方法。 </li>
<li>值得注意的是，不管设定的搜索深度是多少层，经过一次搜索以后，只决定了我方一步棋的走法。等到对方回应一步棋之后，需要在新的棋局下重新进行搜索，来决定下一步棋如何走。</li>
</ul>
<h3 id="静态估计函数f（x）"><a href="#静态估计函数f（x）" class="headerlink" title="静态估计函数f（x）"></a>静态估计函数f（x）</h3><ul>
<li>一般规定有利于MAX的势态，f(p)取正值，有利于MIN的势态，f(p)取负值，势均力敌的势态，f(p)取0值。<ul>
<li>若f(p)＝＋∞，则表示MAX赢，若f(p)＝－∞，则表示MIN赢。下面的讨论规定：顶节点深度d＝0，MAX代表程序方，MIN代表对手方，MAX先走。</li>
</ul>
</li>
<li>当用端节点的静态估计函数f（p）求倒推值时，两位选手应采取不同的策略，从下往上逐层交替使用极小和极大的选值方法，故称极小极大过程。</li>
</ul>
<h3 id="α-β剪枝搜索过程"><a href="#α-β剪枝搜索过程" class="headerlink" title="α-β剪枝搜索过程"></a>α-β剪枝搜索过程</h3><ul>
<li>能否在搜索深度不变的情况下，利用已有的搜索信息减少生成的节点数呢？ <ul>
<li>MIN-MAX过程是把搜索树的生成和格局估值这两个过程分开来进行，即先生成全部搜索树，然后再进行端节点静态估值和倒推值计算，这显然会导致低效率。</li>
</ul>
</li>
<li>实际上<strong>把生成和倒推估值结合起来进行，再根据一定的条件判定，有可能尽早修剪掉一些无用的分枝，同样可获得类似的效果，这就是α-β过程的基本思想</strong>。<ul>
<li>注意： α-β剪枝搜索和生成扩展子节点是同步的。其得到的最优解结果与Min-Max过程一致</li>
</ul>
</li>
<li>Min-Max过程和α-β剪枝搜索的结果都受生成/搜索策略顺序有关（比如选择广度优先或者深度优先，结果可能不一样） </li>
</ul>
<p><strong>例子</strong></p>
<ul>
<li>左边为原搜索树，右边为α-β剪枝搜索<ul>
<li>极小值节点I的N子树被剪枝了（ α剪枝）</li>
<li>极大值节点G的L子树被剪枝了（β剪枝）</li>
</ul>
</li>
</ul>
<p><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1917-02-42.png"> </p>
<ul>
<li><p>定义：</p>
<ul>
<li>Max节点的下界为，即Max确保能获得的最小得益。初始化为-inf。</li>
<li>Min节点的上界为，即Min付出的上界代价保障。初始化为+inf。</li>
<li>对于节点N的估计函数值f(N)，初始化α =-inf ≤ f(N) ≤ β=+inf</li>
<li>若 α ≤ β则N有解。若 α &gt; β 则N无解，该节点N的其他未访问子树会被剪枝</li>
<li>如图中节点I，从左子树DBE可以获得下界=3，并传送到右边的极小节点I，此时I的子节点M给了个上界=1。所以=3&gt; =1，下界大于上界 ，无解。搜索的目标就是确定尽可能紧的取值区间[α, β]</li>
</ul>
</li>
<li><p> <img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1917-10-31.png"> </p>
</li>
<li><p>理解：</p>
<ul>
<li>极大节点N，从一个子树获得的α值和β值，可以传送给其子节点</li>
<li>极大节点N 的α值只可能越改越大，否则极大节点N可以还选择原有α值</li>
<li>极小节点M的β值只可能越改越小，否则极小节点M可以还选择原有β值</li>
<li>从单边子节点往父节点推，极大值父节点只更改α值，极小值父节点只更改β值。</li>
</ul>
</li>
</ul>
<p><strong>α剪枝（发生在极小层节点，如图中的节点I， α 值来自父辈等祖先节点）</strong></p>
<ul>
<li>（1）α剪枝：若任一极小值层节点的β值小于或等于它任一先辈极大值层节点的α值，即α（先辈层）≥β（后继层），则可中止该极小值层中这个MIN节点以下的搜索过程。这个MIN节点最终的倒推值就确定为这个β值。</li>
<li>从一个子树获得的极大节点的α值，可以传送给该节点的其他子树，从而选择α剪枝机会（课本说法，和“先辈”节点α值比较，是和所有先辈节点比较，而不是仅仅和父节点比较）。</li>
<li>从单边子节点往父节点推，极大值父节点只更改α值，极小值父节点只更改β值。</li>
</ul>
<p><strong>β剪枝（发生在极大层节点，如图中的节点G ，β 值来自父辈等祖先节点）</strong></p>
<ul>
<li>（2）β剪枝：若任一极大值层节点的α值大于或等于它任一先辈极小值层节点的β值，即α（后继层）≥β（先辈层），则可以中止该极大值层中这个MAX节点以下的搜索过程。这个MAX节点的最终倒推值就确定为这个α值。</li>
<li>从一个子树获得的极小节点的β值，可以传送给该节点的其他子节点，从而选择β剪枝机会（课本说法，和“先辈”节点β值比较，是和所有先辈节点比较，而不是仅仅和父节点比较）。</li>
<li>从单边子节点往父节点推，极大值父节点只更改α值，极小值父节点只更改β值。</li>
</ul>
<blockquote>
<p>整体过程：</p>
</blockquote>
<ul>
<li><p>α - β搜索过程步骤1</p>
<ul>
<li>从根节点开始，初始化根节点的 α=-∞，β=∞，向左边的子节点展开。到D节点时，得到D的值为3（实际为估计而得），返回B节点，由于B节点是Min节点，所以更新B节点的β值为min(∞, 3)=3，如下图：</li>
<li><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1917-24-21.png"> </li>
</ul>
</li>
<li><p>α - β搜索过程步骤2</p>
<ul>
<li>接下来从B到E，E的值是6，再从E返回MIN节点B，min(3,6)仍为3。从B返回根节点A，A是 Max 节点，更新A的α值为 max(-∞, 3[B的β值])=3，如下图：</li>
<li><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-19.png"> </li>
</ul>
</li>
<li><p>α - β搜索过程步骤3</p>
<ul>
<li>接下来从根节点往右深入，把根节点的 β=∞, α=3 依次传给C、F、I，从 I 深入 M再返回时，I 是 MIN节点，更新 I 的 β = min(∞, 1) = 1。留意到此时 I 的 α=3 ＞ β，所以无需再探索 I 的剩余子节点，把未探索的子节点剪掉，如图3：</li>
<li><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-191.png"> </li>
</ul>
</li>
<li><p>α - β搜索过程步骤4</p>
<ul>
<li>从节点 I 返回到F，F 的 α 值仍为 max(3, 1)=3不变。从F到J再返回，更新 F 的 α = max(3, 5) = 5。从 F 返回 C，更新 C 的 β = min(∞, 5) = 5，如图4： </li>
<li><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-197-27-41.png"> </li>
</ul>
</li>
<li><p>α - β搜索过程步骤5</p>
<ul>
<li>从 C 到 G，把 C 的 β=5, α=3 传给 G，从 G 到 K 再返回 G，更新 G 的 α = max(3, 6) = 6。注意到 G 的 α=6 ＞β，把 G 的其余子节点剪掉，如图5： </li>
<li><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1917-28.png"> </li>
</ul>
</li>
<li><p>α - β搜索过程步骤6</p>
<ul>
<li>从 G 返回 C，C 的 β = min(5, 6) = 5 不变。从 C 到 H 然后返回，C 的 β = min(5, 4) = 4。最后，从 C 返回根节点 A，A 是 Max 节点，A 的 α = max(3, 4[C的β值]) = 4，如图6。 实际上可以更精确，极大节点A的两个子节点范围(-inf, 3]和[3, 4]都确定了，所以此时A的α = β=4</li>
<li><img src="/2021/12/14/%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E4%B8%8E%E5%9B%BE%E6%90%9C%E7%B4%A2/2021-12-1917-29-31.png"> </li>
</ul>
</li>
</ul>
<p>视频：</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1a7411K7g1/?spm_id_from=333.788.recommend_more_video.1">https://www.bilibili.com/video/BV1a7411K7g1/?spm_id_from=333.788.recommend_more_video.1</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayi8991.github.io/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Jiayi Liang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JiayiSpace">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">算法设计与分析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-07 16:10:39" itemprop="dateCreated datePublished" datetime="2021-12-07T16:10:39+08:00">2021-12-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-01-13 11:54:00" itemprop="dateModified" datetime="2022-01-13T11:54:00+08:00">2022-01-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="排序算法回顾"><a href="#排序算法回顾" class="headerlink" title="排序算法回顾"></a>排序算法回顾</h1><table>
<thead>
<tr>
<th>Algorithm</th>
<th>(worst) time</th>
<th>(best) time</th>
<th>(avg) time</th>
<th>method</th>
</tr>
</thead>
<tbody><tr>
<td>MergeSort</td>
<td>O(n log n)</td>
<td>O(n log n)</td>
<td>O(n log n)</td>
<td>divide &amp; conquer</td>
</tr>
<tr>
<td>QuickSort</td>
<td>n<sup>2</sup></td>
<td>O(n log n)</td>
<td>O(n log n)</td>
<td>divide &amp; conquer</td>
</tr>
<tr>
<td>InsertSort</td>
<td>n<sup>2</sup></td>
<td>n</td>
<td>n<sup>2</sup></td>
<td>insertion</td>
</tr>
<tr>
<td>SelectSort</td>
<td>n<sup>2</sup></td>
<td>n<sup>2</sup></td>
<td>n<sup>2</sup></td>
<td>selection</td>
</tr>
<tr>
<td>BubbleSort</td>
<td>n<sup>2</sup></td>
<td>n</td>
<td>n<sup>2</sup></td>
<td>swapping</td>
</tr>
<tr>
<td>HeapSort</td>
<td>O(n log n)</td>
<td>O(n log n)</td>
<td>O(n log n)</td>
<td>swapping</td>
</tr>
</tbody></table>
<h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><ul>
<li>a heap can be given by an array H[0..n-1] and an integer t. </li>
<li>The children of node H[h] are H[2h+1] and H[2h+2].</li>
<li>The parent of node H[h] is H[(h-1)/2]</li>
</ul>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2021-12-07-16-23-32.png" alt="2021-12-07-16-23-32.png">  </p>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/image-20211031131444192.png" alt="image-20211031131444192.png">  </p>
<ul>
<li>The height of a heap of n nodes is ≤ log<sub>2</sub> n .<ul>
<li>  <img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2021-12-07-16-21-29.png" alt="2021-12-07-16-21-29.png"> </li>
<li>1 + 2 + 22 + 23 + ‧‧‧ + 2h-1 = 2h - 1</li>
<li>So n ≥ 2h – 1 + 1 = 2h, and h ≤ log<sub>2</sub> n </li>
</ul>
</li>
</ul>
<h4 id="FixHeap"><a href="#FixHeap" class="headerlink" title="FixHeap()"></a>FixHeap()</h4><ul>
<li>FixHeap(H, t, i) takes <strong>time O(log n).</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">FixHeap(H, t, i)</span><br><span class="line">1. h = i; </span><br><span class="line">2. if (h ≠ root &amp; h &gt; its parent) </span><br><span class="line">      while (h ≠ root &amp; h &gt; its parent) \\插入到数组最后 </span><br><span class="line">         swap h and its parent;</span><br><span class="line">    else </span><br><span class="line">      while (h ≠ leaf &amp; h &lt; its larger child)</span><br><span class="line">         swap h and its larger child.</span><br><span class="line">         </span><br><span class="line">        </span><br><span class="line">FixHeap(H, t, i)</span><br><span class="line">1. h = i; </span><br><span class="line">2. if (h &gt; 0 &amp; H[h] &gt; H[⌊(h-1)/2⌋]) </span><br><span class="line">      while (h &gt; 0 &amp; H[h] &gt; H[⌊(h-1)/2⌋]) </span><br><span class="line">         H[h] ↔ H[⌊(h-1)/2⌋];</span><br><span class="line">         h = ⌊(h-1)/2⌋</span><br><span class="line">    else</span><br><span class="line">      while (2h+1 ≤ t)</span><br><span class="line">         H[g] = max&#123;H[2h+1], H[2h+2]&#125;;</span><br><span class="line">         if (H[h] &lt; H[g])</span><br><span class="line">            H[h] ↔ H[g];</span><br><span class="line">            h = g; </span><br><span class="line">         else h = t.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// In HeapSort, only FixHeapD is needed</span><br><span class="line">FixHeapD(H, t, i)   // FixHeapD only moves down along the root/leaf path. </span><br><span class="line">1. h = i; </span><br><span class="line">2. while (2h+1 ≤ t)</span><br><span class="line">       H[g] = max&#123;H[2h+1], H[2h+2]&#125;;</span><br><span class="line">       if (H[h] &lt; H[g])</span><br><span class="line">          H[h] ↔ H[g];</span><br><span class="line">          h = g; </span><br><span class="line">       else h = t.</span><br></pre></td></tr></table></figure>

<h4 id="SortHeap"><a href="#SortHeap" class="headerlink" title="SortHeap()"></a>SortHeap()</h4><ul>
<li>Sorting a heap takes time O(n log n).</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">H[0..n-1] is a heap</span><br><span class="line">1. t = n-1; </span><br><span class="line">2. while (t &gt; 0) </span><br><span class="line">      H[1] ↔ H[t];</span><br><span class="line">      t = t – 1; </span><br><span class="line">      FixHeap(H, t, 0).</span><br></pre></td></tr></table></figure>

<h4 id="MakeHeap"><a href="#MakeHeap" class="headerlink" title="MakeHeap()"></a>MakeHeap()</h4><ul>
<li>time of MakeHeap: O(n log n)</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for (i = ⌊(n-2)/2⌋; i ≥ 0; i--)</span><br><span class="line">      FixHeapD(H, n-1, i).</span><br></pre></td></tr></table></figure>

<h4 id="Main"><a href="#Main" class="headerlink" title="Main()"></a>Main()</h4><ul>
<li><strong>HeapSort sorts an array of size n in time O(n log n).</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">FixHeapD(H, t, i)</span><br><span class="line">1. h = i; </span><br><span class="line">2. while (2h+1 ≤ t)</span><br><span class="line">       H[g] = max&#123;H[2h+1], H[2h+2]&#125;;</span><br><span class="line">       if (H[h] &lt; H[g])</span><br><span class="line">          H[h] ↔ H[g];</span><br><span class="line">          h = g; </span><br><span class="line">       else h = t.</span><br><span class="line"></span><br><span class="line">MakeHeap(H[0..n-1])</span><br><span class="line">\\ make array H[0..n-1] a heap</span><br><span class="line">1. for (i = ⌊(n-2)/2⌋; i ≥ 0; i--)</span><br><span class="line">      FixHeapD(H, n-1, i).</span><br><span class="line"></span><br><span class="line">SortHeap(H[0..n-1])</span><br><span class="line">\\ H[0..n-1] is a heap</span><br><span class="line">1. t = n-1; </span><br><span class="line">2. while (t &gt; 0) </span><br><span class="line">      H[0] ↔ H[t];</span><br><span class="line">      t = t – 1; </span><br><span class="line">      FixHeapD(H, t, 0).</span><br><span class="line"></span><br><span class="line">main HeapSort(H[0..n-1])</span><br><span class="line">\\ sort the array H[0..n-1]</span><br><span class="line">1. MakeHeap(H[0..n-1]);</span><br><span class="line">2. SortHeap(H[0..n-1]). </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="算法动图演示"><a href="#算法动图演示" class="headerlink" title="算法动图演示"></a>算法动图演示</h4><p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/849589-20171015231308699-356134237.gif" alt="849589-20171015231308699-356134237.gif"> ‘ ``</p>
<h3 id="线性时间排序（Sorting-in-Linear-Time）"><a href="#线性时间排序（Sorting-in-Linear-Time）" class="headerlink" title="线性时间排序（Sorting in Linear Time）"></a>线性时间排序（Sorting in Linear Time）</h3><h4 id="计数排序（Courting-Sort）"><a href="#计数排序（Courting-Sort）" class="headerlink" title="计数排序（Courting Sort）"></a>计数排序（Courting Sort）</h4><blockquote>
<ul>
<li> `〔筆畫〕Algorithm. CountingSort</li>
<li>Step 1. for each value v, count the number of copies of v in A[0..n-1]</li>
<li>Step 2. for each A[i], count the number of elements in A[0..n-1] that are not larger than A[i] </li>
<li>Step 3. use the information in step 2 to output a sorted list. </li>
<li>计数排序的思想是：<ul>
<li>在待排序序列中，如果我们能统计出有多少元素小于或等于某一个元素，我们也就知道了该元素的正确位置。</li>
<li>例如，对于待排序序列{2,5,3,0,2,3,0,3}，我们统计出有8个元素小于等于5（包括5自己），那么5这个元素就应该被排序到第8位</li>
<li>算法的步骤如下：<ul>
<li>（1）找出待排序的数组中最大和最小的元素</li>
<li>（2）统计数组中每个值为i的元素出现的次数，存入数组C的第i项</li>
<li>（3）对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）</li>
<li>（4）反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
<p><strong>伪代码：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Algorithm. CountingSort(A[0..n-1])</span><br><span class="line"></span><br><span class="line">1. for (h=0; h&lt;k; h++) C[h] = 0;</span><br><span class="line">    for (i=0; i&lt;n; i++)</span><br><span class="line">       C[A[i]] = C[A[i]] + 1; </span><br><span class="line">2. for (h=1; h&lt;k; h++)</span><br><span class="line">       C[i] = C[i-1] + C[i]; </span><br><span class="line">3. for (i=n-1; i&gt;=0; i--)</span><br><span class="line">       C[A[i]] = C[A[i]] – 1.</span><br><span class="line">       B[C[A[i]]] = A[i];</span><br><span class="line">       </span><br><span class="line">其中数组A[1~n]是待排序数组；数组B[1~n]用来存放已排好序的元素。C[0~k]用来存放上面所说的统计数（具体的说C[i]就表示在数组A中，小于或等于i的元素的总个数）</span><br><span class="line"></span><br><span class="line">Algorithm. CountingSort</span><br><span class="line"></span><br><span class="line">- Step 1. for each value v, count the number of copies of v in A[0..n-1]</span><br><span class="line">- Step 2. for each A[i], count the number of elements in A[0..n-1] that are not larger than A[i] </span><br><span class="line">- Step 3. use the information in step 2 to output a sorted list. </span><br></pre></td></tr></table></figure>

<p><strong>模拟计数排序过程：</strong></p>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/1.png">  </p>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/countingSort.gif"> </p>
<h4 id="基数排序（Radix-Sort）"><a href="#基数排序（Radix-Sort）" class="headerlink" title="基数排序（Radix Sort）"></a>基数排序（Radix Sort）</h4><p><strong>基本思想</strong></p>
<p>原理是将整数按<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E4%BD%8D%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:126116878%7D">位数</a>切割成不同的数字，然后按每个位数分别比较。基数排序的方式可以采用LSD（Least significant digital）或MSD（Most significant digital），LSD的排序方式由键值的最右边开始，而MSD则相反，由键值的最左边开始。</p>
<ul>
<li><strong>MSD</strong>：先从高位开始进行排序，在每个关键字上，可采用计数排序</li>
<li><strong>LSD</strong>：先从低位开始进行排序，在每个关键字上，可采用桶排序</li>
</ul>
<p><strong>实现逻辑</strong></p>
<blockquote>
<p>① 将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。<br>② 从最低位开始，依次进行一次排序。<br>③ 这样从最低位排序一直到最高位排序完成以后, 数列就变成一个有序序列。</p>
</blockquote>
<p><strong>复杂度分析</strong></p>
<blockquote>
<p>时间复杂度：O(k*N)<br><a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:126116878%7D">空间复杂度</a>：O(k + N)<br>稳定性：稳定</p>
</blockquote>
<p><strong>算法动画演示</strong></p>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/radixSort.gif"> </p>
<h4 id="桶排序（Bucket-sort）"><a href="#桶排序（Bucket-sort）" class="headerlink" title="桶排序（Bucket sort）"></a>桶排序（Bucket sort）</h4><p>桶排序是计数排序的升级版。</p>
<p>桶排序的思想近乎彻底的<strong>分治思想</strong>。</p>
<p><strong>step1:</strong> 桶排序假设待排序的一组数均匀独立的分布在一个范围中，并将这一范围划分成几个子范围（桶）。</p>
<p><strong>step2:</strong> 然后基于某种映射函数f ，将待排序列的关键字 k 映射到第i个桶中 (即桶数组B 的下标i) ，那么该关键字k 就作为 B[i]中的元素 (每个桶B[i]都是一组大小为N/M 的序列 )。</p>
<p><strong>step3:</strong> 接着将各个桶中的数据有序的合并起来 : 对每个桶B[i] 中的所有元素进行比较排序 (可以使用快排)。然后依次枚举输出 B[0]….B[M] 中的全部内容即是一个有序序列。</p>
<blockquote>
<p>补充： 映射函数一般是 f = array[i] / k; k^2 = n; n是所有元素个数</p>
</blockquote>
<p><strong>它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。</strong>为了使桶排序更加高效，我们需要做到这两点：</p>
<ol>
<li>在额外空间充足的情况下，尽量增大桶的数量</li>
<li>使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中</li>
</ol>
<p>同时，对于桶中元素的排序，选择何种比较排序算法对于性能的影响至关重要。</p>
<p><strong>1. 什么时候最快</strong></p>
<p>当输入的数据可以均匀的分配到每一个桶中。</p>
<p><strong>2. 什么时候最慢</strong></p>
<p>当输入的数据被分配到了同一个桶中。</p>
<p><strong>示意图</strong>1</p>
<blockquote>
<p>元素分布在桶中</p>
</blockquote>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/Bucket_sort_1.svg_.png"> </p>
<blockquote>
<p>元素在每个桶中的排序</p>
</blockquote>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/Bucket_sort_2.svg_.png"> </p>
<p><strong>示意图2</strong></p>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/v2-ff4cdccdb1ff6b90ecdb3fc4d361f725_r.jpg"> </p>
<p><strong>复杂度分析</strong></p>
<blockquote>
<p>平均时间复杂度：O(n + k)<br>最佳时间复杂度：O(n + k)<br><a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E6%9C%80%E5%B7%AE%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:125737294%7D">最差时间复杂度</a>：O(n ^ 2)<br>空间复杂度：O(n * k)<br>稳定性：稳定</p>
</blockquote>
<h4 id="上述三种排序算法小结："><a href="#上述三种排序算法小结：" class="headerlink" title="上述三种排序算法小结："></a>上述三种排序算法小结：</h4><p>基数排序与计数排序、桶排序这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异：</p>
<ul>
<li>基数排序：根据键值的每位数字来分配桶；</li>
<li>计数排序：每个桶只存储单一键值；</li>
<li>桶排序：每个桶存储一定范围的数值；</li>
</ul>
<p>基数排序不是直接根据元素整体的大小进行元素比较，而是将原始列表元素分成多个部分，对每一部分按一定的规则进行排序，进而形成最终的有序列表。</p>
<h1 id="图论部分算法"><a href="#图论部分算法" class="headerlink" title="图论部分算法"></a>图论部分算法</h1><h2 id="一些容易忘记的概念"><a href="#一些容易忘记的概念" class="headerlink" title="一些容易忘记的概念"></a>一些容易忘记的概念</h2><p><strong>连通</strong></p>
<ul>
<li>An undirected graph is connected if every pair of vertices is connected by a path. <ul>
<li>如果每一对顶点都由一条路径连接，则无向图是连通的。</li>
</ul>
</li>
</ul>
<p><strong>森林</strong></p>
<ul>
<li>A forest is an acyclic graph (i.e., a graph without simple cycles). A tree is a connected acyclic graph. A tree of n vertices has n-1 edges. <ul>
<li>森林是一个无循环图(即没有简单循环的图)。树是一个连通的无环图。有n个顶点的树有n-1条边。</li>
</ul>
</li>
</ul>
<p><strong>路径</strong></p>
<ul>
<li>A path P(v0, vk) from a vertex v0 to a vertex vk is a sequence {v0, v1, v2, …, vk} of vertices, where [vi-1, vi] are edges in G for i = 1, 2,…, k. The path P(v0, vk) is a cycle if v0 = vk. The path P(v0, vk) is simple if all vertices are distinct. The length of a path is defined as the number of edges in the <ul>
<li>从顶点v0到顶点vk的路径P(v0, vk)是一个顶点序列{v0, v1, v2, vk}，其中[vi-1, vi]是G中的边(i = 1,2, k)。如果v0 = vk，路径P(v0, vk)是一个循环。如果所有顶点都是不同的，路径P(v0, vk)就很简单。路径的长度定义为路径中边的个数</li>
</ul>
</li>
</ul>
<p><strong>二分图（bipartite）</strong></p>
<ul>
<li>A graph G = (V, E) is bipartite if its vertex set V can be partitioned into two disjoint subsets V = R<img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAZCAYAAABZ5IzrAAAA90lEQVRIS+3VIUtDURiH8d8wCGITwW4aKJgsJjEMRMM+hMjQaDLJqiwPTJpWFhasfgOLn2AzGRQMMrNcOMIQ9d29MBlybj2H83/O877vuTVz9tXmjEcGiiryLwy18Ix+cNtF3OAIb5GZz/Uqhq4xwkUQspxAVvGSgb4xkA3lHvppMLp4xXkwOStpuopeGs9yys6wg2YQsose1qaFKfZVeYfW8YBNDH8Ju8UjTmYNVJx/iT000qv9NbONY2zh6S+AFnCFfXRwh3fUk5ENHOK+DEzVkk1mHOAU21hKJRokg1P/LiYPrNJDZS9dan8GinRlQ9lQZCBa/wApHTMa0RPaKgAAAABJRU5ErkJggg==" alt="img">B, R<img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/png;base64,iVBORw0KGgoAAAANSUhEUgAAABoAAAAZCAYAAAAv3j5gAAAA2klEQVRIS+3Uv0qCURjH8Y9Liy1Cjl6AU1NeQ4SjczQFIg11Be1OLq01CtbgDXgJ0Q2IObgZDREuBnHiFSSINx7B6ZzlHDh/vud8nx+nYk+tsieODAqb3lVdFR20kMYzjPHy+0a7gE7xgAUm+EQTbYzQxWoDjILOMMRl0W8/oIEnvCOt+0qTEdAhpujh8Y+i1fCMPu6ioHNc4aQkGRe4xnEUNCh03JSAksI5DrCOqLsvDrgtASXFHzjCWwSUkvaK/4LqWGZQKktWFw5D6AePpC6DfgxkdaEgpE3f90o0Gph66HgAAAAASUVORK5CYII=" alt="img"> B = <img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAZCAYAAADuWXTMAAABZUlEQVQ4T93UP0iVURjH8c+VcpFK1CHcaisKHNVNByNI98YgkFoaajGIILFwbQtJEaKgyH/VUlv2d7tLtDVZo0ZBDorGE88b7325Q3CH4B544eWc53vO7/d7DqemhVFrgdVm8FEM4wg28Abb6EI3vkZWVc89uIuzWMcmjuEkpjGGT5iqwn0JPMdN/Cp14jjeYhfxv1OFX+AjbjVp3xVMppJ5xPdX9iDu4zT2KvA5zGUGB/EyrewXnmfwA7MVcACvMIH3ufYOl1Ev4Id4hGcluB8fcA2PS/MhOeqWC/gBnmA1i6Ilr/EUtytqFrEUtQV8I/1HWB2xa4ZzoUl4dZzH5wKOPkaLTuAOwuuZoiWlDYYy2KhvuCTh5RQOZbJblVN7EWFdzYMa4HGs5cKlvJYFP4J7WEhlf+YL2Z3hARcxioC/4Xv29CeuY6Wspny348JHcYwDaeFwKvjS7NFos8fgn97F/+f5NyvPRhqyABylAAAAAElFTkSuQmCC" alt="img">, such that every edge in G has one end in R and one end in B. In other words, the graph is 2-colorable<ul>
<li>图G = (V, E)属于二分图，如果它的顶点集合V可以分割成两个不相交的子集V= R, R B , 这样每条边在G R和一端一端。换句话说,图为2-colorable</li>
<li><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2021-12-08-15-28-26.png"> </li>
</ul>
</li>
</ul>
<p><strong>连通分量</strong></p>
<p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%97%A0%E5%90%91%E5%9B%BE/1680427">无向图</a>G的极大连通子图称为G的连通分量( Connected Component)。</p>
<p>任何<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E8%BF%9E%E9%80%9A%E5%9B%BE/6460995">连通图</a>的连通分量只有一个，即是其自身，非连通的<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%97%A0%E5%90%91%E5%9B%BE/1680427">无向图</a>有多个连通分量。</p>
<ul>
<li>Let G be an undirected graph. A connected component (CC) of G is a maximal subgraph that is connected. <ul>
<li>设G是一个无向图。G的连通分量(CC)是一个极大的连通子图。</li>
</ul>
</li>
</ul>
<p><strong>强连通分量</strong></p>
<p>在<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%9C%89%E5%90%91%E5%9B%BE/1852743">有向图</a>G中，如果两个顶点vi,vj间（vi&gt;vj）有一条从vi到vj的有向路径，同时还有一条从vj到vi的有向路径，则称两个顶点<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%BC%BA%E8%BF%9E%E9%80%9A/1131406">强连通</a>(strongly connected)。</p>
<p>如果有向图G的每两个顶点都强连通，称G是一个<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%BC%BA%E8%BF%9E%E9%80%9A%E5%9B%BE/6769617">强连通图</a>。</p>
<p>有向图的极大强连通<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%AD%90%E5%9B%BE/8737707">子图</a>，称为强连通分量。</p>
<ul>
<li>Let G be a directed graph. Two vertices v and w in G are in the same strongly connected component (scc) if in the graph G, there exist a (directed) path from v to w and a (directed) path from w to v.<ul>
<li>设G是一个有向图。如果在图G中存在一条从v到w的(有向)路径和一条从w到v的(有向)路径，则图G中的两个顶点v和w在同一个强连通分量(scc)中。</li>
</ul>
</li>
</ul>
<h2 id="图的表示方法"><a href="#图的表示方法" class="headerlink" title="图的表示方法"></a>图的表示方法</h2><blockquote>
<p>Graphs can be given by adjacency matrix or adjacency list. </p>
</blockquote>
<ul>
<li><p>The adjacency matrix of a graph has value ai,j = 1 if vertices i and j share an edge; 0 otherwise. For a weighted graph, ai,j = wi,j, the weight of the edge. </p>
<ul>
<li>如果顶点i和j共用一条边，则图的邻接矩阵值为ai,j = 1;0。对于加权图，ai,j = wi,j，是边的权值。</li>
</ul>
</li>
<li><p>The adjacency list of a graph G = (V,E) is an array Adj[1..|V|] of lists, where Adj[v] is a list of all vertices adjacent to vertex v. </p>
<ul>
<li>图G = (V,E)的邻接表是一个数组Adj[1..|V|]的列表，其中Adj[V]是与顶点V相邻的所有顶点的列表。</li>
</ul>
</li>
</ul>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2021-12-08-15-15-25.png"> </p>
<ul>
<li><p>The adjacency matrix of a directed graph has value ai,j = 1 if [i, j] is an arc in the graph; 0 otherwise. For a weighted graph, ai,j = wi,j is the weight of the arc [i, j]. </p>
<ul>
<li>如果[i, j]是图中的弧，则有向图的邻接矩阵的值为ai,j = 1;0。对于加权图，ai,j = wi,j为弧[i, j]的权值。</li>
</ul>
</li>
<li><p>The adjacency list of a directed graph G = (V,E) is an array Adj[1..|V|] of lists, where Adj[v] is a list of all arcs form v. </p>
<ul>
<li>有向图G = (V,E)的邻接表是一个数组Adj[1..]|V|]的列表，其中Adj[V]是所有弧形成V的列表</li>
</ul>
</li>
</ul>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2021-12-086-03-05.png"> </p>
<h2 id="图的遍历（Traversing-a-graph）"><a href="#图的遍历（Traversing-a-graph）" class="headerlink" title="图的遍历（Traversing a graph）"></a>图的遍历（Traversing a graph）</h2><h3 id="广度优先遍历（BFS）"><a href="#广度优先遍历（BFS）" class="headerlink" title="广度优先遍历（BFS）"></a>广度优先遍历（BFS）</h3><ul>
<li>Breadth-First-Search (BFS). Starting from a vertex s, traverse the graph vertices “level by level.”<ul>
<li>广度优先搜索(BFS)。从顶点s开始，逐层遍历图顶点。</li>
</ul>
</li>
</ul>
<p><strong>伪代码：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">BFS(G, s)  \\ Q is a queue</span><br><span class="line">1. for (each vertex v) color[v] = white;</span><br><span class="line">2. color[s] = gray; </span><br><span class="line">3. enqueue(Q, s); </span><br><span class="line">4. while (Q is not empty)</span><br><span class="line">      w = dequeue(Q);</span><br><span class="line">      for (each edge [w, v])</span><br><span class="line">        if (color[v] == white) </span><br><span class="line">           color[v] = gray; </span><br><span class="line">           enqueue(Q, s);</span><br><span class="line">      color[w] = black.</span><br><span class="line">        </span><br></pre></td></tr></table></figure>

<p><strong>动画展示</strong></p>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/20191127135310793.gif"> </p>
<h3 id="深度优先遍历（DFS）"><a href="#深度优先遍历（DFS）" class="headerlink" title="深度优先遍历（DFS）"></a>深度优先遍历（DFS）</h3><ul>
<li>Depth-First-Search (DFS). Starting from a vertex s, explore the graph vertices as far as possible before backing tracking<ul>
<li>深度优先搜索(DFS)。从一个顶点开始，在回溯跟踪之前尽可能地探索图的顶点</li>
</ul>
</li>
</ul>
<p><strong>伪代码：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DFS(v)</span><br><span class="line">1. color[v] = gray; </span><br><span class="line">2. for (each edge [v, w])</span><br><span class="line">        if (color[w] == white) </span><br><span class="line">           DFS(w); </span><br><span class="line">3. color[v] = black.</span><br><span class="line"></span><br><span class="line">main()</span><br><span class="line">1. for (each vertex v) </span><br><span class="line">      color[v] = white;</span><br><span class="line">2. for (each vertex v)</span><br><span class="line">       if (color[v] == white) </span><br><span class="line">          DFS(v).   </span><br><span class="line">         </span><br></pre></td></tr></table></figure>

<ul>
<li>Running Time = O(m + n): </li>
<li>DFS(v) is called on each vertex exactly once.</li>
<li>Charge the time of DFS(v)  to the vertex v, not including the recursive calls DFS(w) in DFS(v).<ul>
<li>将DFS(v)的时间计费到顶点v，不包括DFS(v)中的递归调用DFS(w)。</li>
</ul>
</li>
</ul>
<p><strong>动画展示</strong></p>
<blockquote>
<p>递归方法：</p>
</blockquote>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/20191127135244696.gif"> </p>
<blockquote>
<p>非递归，利用了 栈 的思想</p>
</blockquote>
<p> <img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2019112716422925.gif"></p>
<blockquote>
<p>bfs 和 dfs的非递归遍历比较，可以看出，很相似，只不过一个用的栈一个用的队列</p>
</blockquote>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2021-12-0815-37-08.png"> </p>
<h2 id="DFS和DAG（有向无环图）"><a href="#DFS和DAG（有向无环图）" class="headerlink" title="DFS和DAG（有向无环图）"></a>DFS和DAG（有向无环图）</h2><h3 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h3><ul>
<li>Topological Sorting</li>
<li>Given a directed graph G, order the vertices of G into an array T[1..n] such that if [T[i], T[j]] is an arc, then i &lt; j, or report no such an order exists.<ul>
<li>给定有向图G，将G的顶点排序到数组T[1..]n]这样，如果[T[i]， T[j]是一个弧，则i &lt; j，或报告不存在这样的顺序。</li>
</ul>
</li>
</ul>
<ul>
<li>拓扑排序是对DAG的顶点进行排序，使得对每一条有向边(u, v)，均有u（在排序记录中）比v先出现。亦可理解为对某点v而言，只有当v的所有源点均出现了，v才能出现。</li>
</ul>
<p>下图给出有向无环图的拓扑排序：</p>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/399159-20151229144326901-1530781288.png"> </p>
<p>下图给出的顶点排序不是拓扑排序，因为顶点<code>D</code>的邻接点<code>E</code>比其先出现：</p>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/399159-20151229144340448-1796643731.png"> </p>
<blockquote>
<p>DFS判断是否有拓扑排序</p>
<ul>
<li>算法思想：</li>
<li>let [v, w] be an arc in G.</li>
<li>When we call DFS(v) on v, what is the color of w?<ul>
<li>当我们对v调用DFS(v)时，w的颜色是什么</li>
</ul>
</li>
<li>If color[w] = black, then w becomes black before v.<ul>
<li>如果color[w] = black，则w在v之前变为黑色。</li>
</ul>
</li>
<li>If color[w] = white, DFS(v) calls DFS(w), so w becomes black before v.<ul>
<li>如果color[w] = white, DFS(v)调用DFS(w)，那么w在v之前变成黑色。</li>
</ul>
</li>
<li>If color[w] = gray, then DFS(w) (recursively) calls DFS(v), so there is a path from w to v, which plus the arc [v, w] gives a cycle. So there is no topological order<ul>
<li>如果color[w] = gray，那么DFS(w)(递归地)调用DFS(v)，所以有一条从w到v的路径，加上弧[v, w]就得到一个循环。所以没有拓扑顺序</li>
</ul>
</li>
</ul>
</blockquote>
<p>代码实现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DFS(v)</span><br><span class="line">1. color[v] = gray; </span><br><span class="line">2. for (each arc [v, w])</span><br><span class="line">     if (color[w] == white) DFS(w)</span><br><span class="line">     else if (color[w] == gray)</span><br><span class="line">        Stop (“no topological order”); </span><br><span class="line">3. color[v] = black; T[t] = v; t = t - 1.</span><br><span class="line"></span><br><span class="line">T-Sort() \\ T[1..n] is the output array</span><br><span class="line">1. for (each vertex v) </span><br><span class="line">      color[v] = white;</span><br><span class="line">2. t = n; </span><br><span class="line">3. for (each vertex v)</span><br><span class="line">     if (color[v] == white) DFS(v).   </span><br></pre></td></tr></table></figure>

<blockquote>
<p>附： 非DFS方法</p>
</blockquote>
<p>动画演示：</p>
<p><a target="_blank" rel="noopener" href="https://www.cs.usfca.edu/~galles/visualization/TopoSortIndegree.html">https://www.cs.usfca.edu/~galles/visualization/TopoSortIndegree.html</a></p>
<h3 id="寻找强连通分量"><a href="#寻找强连通分量" class="headerlink" title="寻找强连通分量"></a><strong>寻找强连通分量</strong></h3><h2 id="单源最短路径问题（Single-Source-Shortest-Path-Problem）"><a href="#单源最短路径问题（Single-Source-Shortest-Path-Problem）" class="headerlink" title="单源最短路径问题（Single-Source Shortest Path Problem）"></a>单源最短路径问题（Single-Source Shortest Path Problem）</h2><h3 id="Dijikstra算法"><a href="#Dijikstra算法" class="headerlink" title="Dijikstra算法"></a>Dijikstra算法</h3><ul>
<li><p>Problem. Given a weighted graph G and two vertices s and t in G, find a shortest path from s to t in G</p>
<ul>
<li>问题: 给定一个加权图G和G中的两个顶点s和t，找出G中s到t的一条最短路径</li>
</ul>
</li>
<li><p>Dijkstra’s Idea: start from the vertex s, gradually grow a tree rooted at s, by adding the vertex that currently “looks” the best</p>
<ul>
<li>Dijkstra的想法:从顶点s开始，通过添加当前看起来最好的顶点，逐渐生长出以s为根的树</li>
</ul>
</li>
</ul>
<blockquote>
<p>Dijkstra算法：</p>
</blockquote>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2022-01-1115-52-29.png">  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Dijkstra(G, s, t)                                 complexity</span><br><span class="line">1. for (v=1; v≤n; v++) status[v] = unseen;           〇(n)</span><br><span class="line">2. status[s] = in-tree; dist[s] = 0;				 〇（1）</span><br><span class="line">3. for (each edge [s, w])							 〇（n）</span><br><span class="line">      status[w] = fringe; dad[w] = s;</span><br><span class="line">      dist[w] = wt(s, w);</span><br><span class="line">4. While (there are fringes)</span><br><span class="line">4.1   pick the fringe v of minimum dist[v];</span><br><span class="line">4.2   status[v] = in-tree;						   &lt;=n times</span><br><span class="line">4.3   for (each edge [v, w])						 〇（n）</span><br><span class="line">4.3.1   if (status[w] == unseen)					 〇（1）</span><br><span class="line">             status[w] = fringe; dad[w] = v; 		 〇（n）</span><br><span class="line">             dist[w] = dist[v] + wt(v, w);</span><br><span class="line">4.3.2   else if (status[w] == fringe) &amp; </span><br><span class="line">             (dist[w] &gt; dist[v] + wt(v, w))</span><br><span class="line">             dad[w] = v; </span><br><span class="line">             dist[w] = dist[v] + wt(v, w);</span><br><span class="line">5. The array dad[1..n] gives the path from s.  </span><br><span class="line">												Time= 〇(n^2)</span><br></pre></td></tr></table></figure>

<blockquote>
<p> 如果使用 min-heap 来存储 fringe</p>
</blockquote>
<ol>
<li>RetrieveMin(H): find the minimum, and restore the heap; </li>
<li>Insert(H, v): add a new fringe v and restore the heap;</li>
<li>Delete(H, v): delete v and restore the heap.</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">FixHeap(H, t, i)</span><br><span class="line">1. h = i; </span><br><span class="line">2. if (h&gt;0 &amp; H[h]&lt;H[⌊(h-1)/2⌋]) </span><br><span class="line">      while (h&gt;0 &amp; H[h]&lt;H[⌊(h-1)/2⌋]) </span><br><span class="line">         H[h] ↔ H[⌊(h-1)/2⌋];</span><br><span class="line">         h = ⌊(h-1)/2⌋</span><br><span class="line">    else</span><br><span class="line">      while (2h+1 ≤ t)</span><br><span class="line">         if (2h+1=t or H[2h+1]&lt;H[2h+2])</span><br><span class="line">            g = 2h+1;</span><br><span class="line">         else g = 2h+2;</span><br><span class="line">         if (H[h] &gt; H[g])</span><br><span class="line">            H[h] ↔ H[g];</span><br><span class="line">            h = g; </span><br><span class="line">         else break.                       time = 〇（log n）</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RerieveMin(H, t)</span><br><span class="line">\\ get the minimum &amp; delete</span><br><span class="line">\\ it from the heap H[0..t]</span><br><span class="line">1. min = H[0]; </span><br><span class="line">2. H[0] = H[t]; t = t – 1;</span><br><span class="line">3. FixHeap(H, t, 0);</span><br><span class="line">4. output(min).                            time = 〇（log n）</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Insert(H, t, v)</span><br><span class="line">\\ insert value v into</span><br><span class="line">\\ the heap H[0..t].</span><br><span class="line">1. t = t + 1;</span><br><span class="line">2. H[t] = v; </span><br><span class="line">3. FixHeap(H, t, t).				   	   time = 〇（log n）</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Delete(H, t, i)</span><br><span class="line">\\ delete H[i] from</span><br><span class="line">\\ the heap H[0..t].</span><br><span class="line">1. H[i] = H[t];</span><br><span class="line">2. t = t - 1; </span><br><span class="line">3. FixHeap(H, t, i).                       time = 〇（log n）</span><br></pre></td></tr></table></figure>

<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2022-01-116-20-00.png">   </p>
<p>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Dijkstra(G, s, t)</span><br><span class="line">1. for (v=1; v≤n; v++) status[v] = unseen;</span><br><span class="line">2. status[s] = in-tree; dist[s] = 0; H = Φ;</span><br><span class="line">3. for (each edge [s, w])</span><br><span class="line">      status[w] = fringe; dad[w] = s;</span><br><span class="line">      dist[w] = wt(s, w); Insert(H, w); </span><br><span class="line">4. While (there are fringes)</span><br><span class="line">4.1   v = RetrieveMin(H);</span><br><span class="line">4.2   status[v] = in-tree;</span><br><span class="line">4.3   for (each edge [v, w])</span><br><span class="line">4.3.1   if (status[w] == unseen)</span><br><span class="line">             status[w] = fringe; dad[w] = v; </span><br><span class="line">             dist[w] = dist[v] + wt(v, w); </span><br><span class="line">             Insert(H, w);</span><br><span class="line">4.3.2   else if (status[w] == fringe) &amp;    </span><br><span class="line">             Delete(H, w); dad[w] = v;</span><br><span class="line">             dist[w] = dist[v] + wt(v, w);</span><br><span class="line">             Insert(H, v);</span><br><span class="line">5. The array dad[1..n] gives the path from s.         </span><br><span class="line"></span><br><span class="line">Time= 〇（m log n）</span><br></pre></td></tr></table></figure>





<h3 id="Bellman-Ford算法"><a href="#Bellman-Ford算法" class="headerlink" title="Bellman-Ford算法"></a>Bellman-Ford算法</h3><p>如果图上出现了负的权值，那么使用dijikstra算法就会出错，这里介绍另外一个算法：</p>
<ul>
<li>Theorem. If there is no negative cycles in the graph G, then for a vertex v to which a shortest path from s consists of k edges, after at most k iterations of step 3, dist[v] becomes the length of the shortest path from s to v. <ul>
<li>如果图 G 中不存在负环，则对于从 s 到t的最短路径由 k 条边组成的顶点 v，经过步骤 3 的最多 k 次迭代，dist[v] 变为从 s 到t的最短路径的长度v.</li>
</ul>
</li>
</ul>
<ul>
<li>Therefore, n-1 iterations of step 3 will be sufficient to find a shortest path to every vertex from s. <ul>
<li>因此，第3步的n-1次迭代将足以找到从s到每个顶点的最短路径。</li>
</ul>
</li>
</ul>
<p>TIps：The problem is meaningful only for directed graphs with no negative cycles.</p>
<p>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Bellman-Ford(G, s, t)</span><br><span class="line">1. for (v=1; v≤n; v++) dist[v] = ∞;</span><br><span class="line">2. dist[s] = 0;</span><br><span class="line">3. loop n-1 times</span><br><span class="line">      for (each edge [v, w])</span><br><span class="line">         if (dist[w] &gt; dist[v] + wt(v, w))   </span><br><span class="line">                 dad[w] = v;</span><br><span class="line">                 dist[w] = dist[v] + wt(v, w);</span><br><span class="line">4. for (each edge [v, w])</span><br><span class="line">         if (dist[w] &gt; dist[v] + wt(v, w))   </span><br><span class="line">                 stop(“negative cycle”); </span><br><span class="line">5. The array dad[1..n] gives the path from s. </span><br></pre></td></tr></table></figure>





<h2 id="多源最短路径问题（All-Pairs-Shortest-Paths-Problem）"><a href="#多源最短路径问题（All-Pairs-Shortest-Paths-Problem）" class="headerlink" title="多源最短路径问题（All-Pairs Shortest Paths Problem）"></a>多源最短路径问题（All-Pairs Shortest Paths Problem）</h2><ul>
<li>Problem. Given a (possibly negatively) weighted graph G in an adjacency matrix M, find the shortest path from s to t for all vertex pairs (s, t). <ul>
<li>问题： 给定邻接矩阵M中的一个(可能是负的)加权图G，找出所有顶点对(s, t)，从s到t的最短路径。</li>
<li>即给定任意两个点，一个出发点，一个到达点，求这两个点的之间的最短路径，就是任意两点最短路径问题，多源最短路径</li>
</ul>
</li>
</ul>
<blockquote>
<p>Fyd</p>
</blockquote>
<h2 id="最小生成树Minimun-Spanning-Tree"><a href="#最小生成树Minimun-Spanning-Tree" class="headerlink" title="最小生成树Minimun Spanning Tree"></a>最小生成树Minimun Spanning Tree</h2><blockquote>
<p>参考视频：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Eb41177d1?from=search&amp;seid=9147041630400351432&amp;spm_id_from=333.337.0.0">https://www.bilibili.com/video/BV1Eb41177d1?from=search&amp;seid=9147041630400351432&amp;spm_id_from=333.337.0.0</a></p>
</blockquote>
<ul>
<li>Definition. Let G be a weighted graph. A spanning tree of G is a subgraph of G that is a tree and contains all vertices of G. The weight of a spanning tree T is the sum of weights of edges in T. <ul>
<li>定义。设G是一个加权图。G的生成树是G的一个子图，它是一棵树，包含G的所有顶点。生成树T的权值是T中各边的权值之和</li>
</ul>
</li>
<li>MST Problem. Given a weighted and undirected graph G, construct a minimum spanning tree of G, i.e., a spanning tree of G whose weight is the smallest over all spanning trees of G. <ul>
<li>MST的问题。给定一个加权无向图G，构造G的最小生成树，即G的所有生成树中权值最小的生成树。</li>
</ul>
</li>
</ul>
<blockquote>
<p>Dijkstra’s approach</p>
<ul>
<li>starting from any vertex s, grow a tree using the lightest edges</li>
</ul>
</blockquote>
<p><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2022-01-1115-52-29.png">  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Prim(G)</span><br><span class="line">1. for (v=1; v≤n; v++) status[v] = unseen;</span><br><span class="line">2. pick any vertex s; status[s] = in-tree; </span><br><span class="line">3. for (each edge [s, w])</span><br><span class="line">      status[w] = fringe; dad[w] = s;</span><br><span class="line">      WEIGHT[w] = wt(s, w);</span><br><span class="line">4. While (there are fringes)</span><br><span class="line">4.1   pick the fringe v of minimum WEIGHT[v];</span><br><span class="line">4.2   status[v] = in-tree;</span><br><span class="line">4.3   for (each edge [v, w])</span><br><span class="line">4.3.1   if (status[w] == unseen)</span><br><span class="line">             status[w] = fringe; dad[w] = v; </span><br><span class="line">             WEIGHT[w] = wt(v, w);</span><br><span class="line">4.3.2   else if (status[w] == fringe) &amp; </span><br><span class="line">             (WEIGHT[w] &gt; wt(v, w))</span><br><span class="line">             dad[w] = v; </span><br><span class="line">             WEIGHT[w] = wt(v, w);</span><br><span class="line">5. The array dad[1..n] gives the path from s.      </span><br></pre></td></tr></table></figure>



<blockquote>
<p>Kruskal’s approach</p>
<ul>
<li>Idea: repeatedly add the lightest edges to make a connected and acyclic graph that covers all vertices (i.e., a spanning tree).</li>
<li>重复添加最轻的边，生成一个覆盖所有顶点的连通无环图(即生成树)</li>
</ul>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Kruskal(G)</span><br><span class="line">1. sort the edges in nondecreasing order: </span><br><span class="line">      e1, e2, …, em;</span><br><span class="line">2. T = Φ;</span><br><span class="line">3. for (i = 1; i &lt;= m; i++) </span><br><span class="line">       let ei = [ui, vi]; </span><br><span class="line">       if (T + ei does not contain a cycle)</span><br><span class="line">           T = T + ei;</span><br><span class="line">4. return(T).</span><br></pre></td></tr></table></figure>



<blockquote>
<p>Questions:</p>
</blockquote>
<ol>
<li><p>How do we represent T so that we can test efficiently if T + ei contains a cycle?</p>
<p>我们如何表示T以便我们能有效地测试T + ei是否包含一个循环</p>
<p>Q1: Since T is acyclic, a component of T is a tree. Thus, adding an edge to the same component makes a cycle while adding an edge connecting two different components does not make a cycle. Thus, for the edge to be added, we only need to check if its two ends are in the same component.</p>
<p>因为T是无环的，所以T的一个分量就是树。因此，向同一个组件添加一条边构成一个循环，而向两个不同组件添加一条边则不构成一个循环。因此，对于要添加的边，我们只需要检查它的两端是否在同一个组件中。</p>
</li>
</ol>
<ul>
<li>We represent each component of T in a tree structure so that we can test if the two ends of an edge are in the component. 我们用树形结构表示T的每个分量，这样我们就可以测试一条边的两端是否在这个分量中</li>
</ul>
<h2 id="图匹配问题（Graph-Matching-（on-undirected-graph））"><a href="#图匹配问题（Graph-Matching-（on-undirected-graph））" class="headerlink" title="图匹配问题（Graph Matching （on undirected graph））"></a>图匹配问题（Graph Matching （on undirected graph））</h2><p>在无向图中匹配的定义：</p>
<ul>
<li>Definition. An edge set M in an undirected graph G is a matching in G if no two edges in M share a common end<ul>
<li>定义。如果无向图G中没有两条边共用一个端点，则无向图G中的边集M就是一个匹配</li>
<li><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2022-01-124-18-54.png">  </li>
</ul>
</li>
</ul>
<blockquote>
<p>The Maximum Matching Problem.</p>
<p>最大匹配问题</p>
<ul>
<li>Given an undirected graph G, construct a maximum matching in G.</li>
<li>给出一个无向图G，构造一个最大匹配。</li>
</ul>
</blockquote>
<ul>
<li><p>Definitions. M: a matching in G. A vertex is matched if it is an end of an edge in M, otherwise, unmatched.</p>
<ul>
<li>定义。如果一个顶点是M中某条边的末端，那么它就被匹配，否则就是不匹配。</li>
</ul>
</li>
<li><p>Definition. An augmenting path (w.r.t a matching M) starts and ends at unmatched vertices and goes alternatively with edges not in M and in M</p>
<ul>
<li>定义。一条增广路径(w.r.t，匹配M)开始和结束于不匹配的顶点，并交替地到达不在M和M中的边</li>
<li><img src="/2021/12/07/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/2022-01-14-19-39.png">  </li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>Theorem. A matching M is maximum if and only if there is no augmenting path w.r.t. M in G. <ul>
<li>定理。当且仅当G中没有增广路径w.r.t. M时，匹配的M是最大的。</li>
</ul>
</li>
<li>Proof. If M is max, then there is no augmenting path  otherwise, we would be able to construct a larger matching.<ul>
<li>证明。如果M是最大值，那么就没有增广路径，否则我们就可以构造一个更大的匹配。</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li><p>Finding an augmenting path:  start from an unmatched vertex, try to find an augmenting path ending at another unmatched vertex.</p>
<ul>
<li>寻找增广路径:从一个不匹配的顶点开始，尝试找到一个在另一个不匹配的顶点结束的增广路径。</li>
</ul>
</li>
<li><p>Pretty much similar to BFS but: at even levels, expand all possible ways, but at odd levels, expand on a single matching edge.</p>
<ul>
<li>与BFS非常相似，但是:在偶数层，扩展所有可能的方式，但在奇数层，扩展单个匹配边。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Augment(G, M) \\ Q is a queue</span><br><span class="line">1. for (each vertex v of G) level[v] = -1; </span><br><span class="line">2. for (each unmatched vertex v)</span><br><span class="line">       level[v] = 0; dad[v] = -1; Q ← v; </span><br><span class="line">3. while (Q ≠ Φ)</span><br><span class="line">      v ← Q; </span><br><span class="line">      if (level[v] is even)</span><br><span class="line">         for (each edge [v, w]) </span><br><span class="line">            if (level[w] == -1)</span><br><span class="line">               level[w] = level[v]+1; </span><br><span class="line">               dad[w] = v;  Q ← w; </span><br><span class="line">            else if (level[v] == level[w]) </span><br><span class="line">               return an augmenting path;</span><br><span class="line">      else \\ level[v] is odd</span><br><span class="line">         let [v, w] be the edge in M;</span><br><span class="line">         if (level[v] == level[w]) </span><br><span class="line">            return an augmenting path;</span><br><span class="line">         else level[w] = level[v]+1; </span><br><span class="line">                dad[w] = v;  Q ← w; </span><br><span class="line">4. Return (false)</span><br><span class="line"> </span><br><span class="line"> Time=〇（n+m）</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jiayi Liang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jiayi Liang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiayi Liang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
